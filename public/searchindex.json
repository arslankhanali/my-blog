[{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml ArgoCD deploying everything\n![oprah]({{ \u0026ldquo;my-blogs/static/argocd-app-of-apps/oprah.png\u0026rdquo; | relURL }})\n![oprah]({{ /argocd-app-of-apps/oprah.png | relURL }})\nAccess apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool—it\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ├── apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here │ ├── guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook │ │ ├── base │ │ └── overlays │ ├── kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ │ │ ├── base │ │ └── overlays │ └── podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize │ ├── base │ └── overlays ├── env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, │ ├── k3s │ │ ├── kustomization.yaml │ │ ├── platform │ │ └── workloads │ └── openshift │ ├── kustomization.yaml │ ├── platform │ └── workloads ├── ingress.yaml # Ingress to access ArgoCD dashboard ├── README.md ├── root-app.yaml # Root ARGOCD application └── values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ – Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ └── \u0026lt;app1\u0026gt;/ ├── base/ └── overlays/ ├── \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV └── \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ – Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n── env │ ├── \u0026lt;env1-name\u0026gt; │ │ ├── kustomization.yaml │ │ ├── platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ │ └── \u0026#39;argocd-application-for-app1\u0026#39;.yaml │ │ └── workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ ├── \u0026#39;argocd-application-for-app2\u0026#39;.yaml │ │ └── \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml – The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":0,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , — this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":1,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":2,"permalink":"/posts/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure 💡 The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n✅ Visual: Prompt with icons, colors, and context-aware sections\n✅ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n✅ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":3,"permalink":"/posts/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You’ve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":4,"permalink":"/posts/random/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes — ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80’s door anymore — it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy’s new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy’s map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph’s Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity — just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":5,"permalink":"/posts/random/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ✅ Works ❌ Fails with CSRF token error Safari ✅ Works ❌ Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":6,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode — K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":7,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":8,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM — after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ✅ Yes Setup bridge/qemu policies ✅ Yes VM create/operate via libvirt ❌ No Use virt-manager GUI ❌ No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":9,"permalink":"/posts/homelab/haos-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference — opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":10,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub 👉 Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":11,"permalink":"/posts/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\n🔧 Tech Walkthroughs — Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. 🌐 Home Lab \u0026amp; Automation — Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. 🛡️ Security \u0026amp; Best Practices — Focus security, supply chain integrity, and observability. 📦 Modern Ops — GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you’re an engineer, architect, or open source enthusiast — I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":12,"permalink":"/about/","summary":"Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"},{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml ArgoCD deploying everything\n![oprah]({{ \u0026ldquo;my-blogs/static/argocd-app-of-apps/oprah.png\u0026rdquo; | relURL }})\n![oprah]({{ /argocd-app-of-apps/oprah.png | relURL }})\nAccess apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool—it\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ├── apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here │ ├── guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook │ │ ├── base │ │ └── overlays │ ├── kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ │ │ ├── base │ │ └── overlays │ └── podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize │ ├── base │ └── overlays ├── env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, │ ├── k3s │ │ ├── kustomization.yaml │ │ ├── platform │ │ └── workloads │ └── openshift │ ├── kustomization.yaml │ ├── platform │ └── workloads ├── ingress.yaml # Ingress to access ArgoCD dashboard ├── README.md ├── root-app.yaml # Root ARGOCD application └── values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ – Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ └── \u0026lt;app1\u0026gt;/ ├── base/ └── overlays/ ├── \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV └── \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ – Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n── env │ ├── \u0026lt;env1-name\u0026gt; │ │ ├── kustomization.yaml │ │ ├── platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ │ └── \u0026#39;argocd-application-for-app1\u0026#39;.yaml │ │ └── workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ ├── \u0026#39;argocd-application-for-app2\u0026#39;.yaml │ │ └── \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml – The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":0,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , — this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":1,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":2,"permalink":"/posts/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure 💡 The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n✅ Visual: Prompt with icons, colors, and context-aware sections\n✅ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n✅ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":3,"permalink":"/posts/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You’ve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":4,"permalink":"/posts/random/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes — ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80’s door anymore — it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy’s new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy’s map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph’s Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity — just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":5,"permalink":"/posts/random/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ✅ Works ❌ Fails with CSRF token error Safari ✅ Works ❌ Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":6,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode — K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":7,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":8,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM — after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ✅ Yes Setup bridge/qemu policies ✅ Yes VM create/operate via libvirt ❌ No Use virt-manager GUI ❌ No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":9,"permalink":"/posts/homelab/haos-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference — opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":10,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub 👉 Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":11,"permalink":"/posts/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\n🔧 Tech Walkthroughs — Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. 🌐 Home Lab \u0026amp; Automation — Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. 🛡️ Security \u0026amp; Best Practices — Focus security, supply chain integrity, and observability. 📦 Modern Ops — GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you’re an engineer, architect, or open source enthusiast — I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":12,"permalink":"/about/","summary":"Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"},{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml ArgoCD deploying everything\n![oprah]({{ \u0026ldquo;my-blogs/static/argocd-app-of-apps/oprah.png\u0026rdquo; | relURL }})\n![oprah]({{ /argocd-app-of-apps/oprah.png | relURL }})\nAccess apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool—it\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ├── apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here │ ├── guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook │ │ ├── base │ │ └── overlays │ ├── kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ │ │ ├── base │ │ └── overlays │ └── podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize │ ├── base │ └── overlays ├── env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, │ ├── k3s │ │ ├── kustomization.yaml │ │ ├── platform │ │ └── workloads │ └── openshift │ ├── kustomization.yaml │ ├── platform │ └── workloads ├── ingress.yaml # Ingress to access ArgoCD dashboard ├── README.md ├── root-app.yaml # Root ARGOCD application └── values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ – Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ └── \u0026lt;app1\u0026gt;/ ├── base/ └── overlays/ ├── \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV └── \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ – Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n── env │ ├── \u0026lt;env1-name\u0026gt; │ │ ├── kustomization.yaml │ │ ├── platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ │ └── \u0026#39;argocd-application-for-app1\u0026#39;.yaml │ │ └── workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ ├── \u0026#39;argocd-application-for-app2\u0026#39;.yaml │ │ └── \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml – The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":0,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , — this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":1,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":2,"permalink":"/posts/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure 💡 The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n✅ Visual: Prompt with icons, colors, and context-aware sections\n✅ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n✅ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":3,"permalink":"/posts/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You’ve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":4,"permalink":"/posts/random/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes — ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80’s door anymore — it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy’s new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy’s map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph’s Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity — just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":5,"permalink":"/posts/random/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ✅ Works ❌ Fails with CSRF token error Safari ✅ Works ❌ Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":6,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode — K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":7,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":8,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM — after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ✅ Yes Setup bridge/qemu policies ✅ Yes VM create/operate via libvirt ❌ No Use virt-manager GUI ❌ No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":9,"permalink":"/posts/homelab/haos-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference — opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":10,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub 👉 Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":11,"permalink":"/posts/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\n🔧 Tech Walkthroughs — Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. 🌐 Home Lab \u0026amp; Automation — Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. 🛡️ Security \u0026amp; Best Practices — Focus security, supply chain integrity, and observability. 📦 Modern Ops — GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you’re an engineer, architect, or open source enthusiast — I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":12,"permalink":"/about/","summary":"Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"},{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml ArgoCD deploying everything\n![oprah]({{ \u0026ldquo;my-blogs/static/argocd-app-of-apps/oprah.png\u0026rdquo; | relURL }})\n![oprah]({{ /argocd-app-of-apps/oprah.png | relURL }})\nAccess apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool—it\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ├── apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here │ ├── guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook │ │ ├── base │ │ └── overlays │ ├── kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ │ │ ├── base │ │ └── overlays │ └── podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize │ ├── base │ └── overlays ├── env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, │ ├── k3s │ │ ├── kustomization.yaml │ │ ├── platform │ │ └── workloads │ └── openshift │ ├── kustomization.yaml │ ├── platform │ └── workloads ├── ingress.yaml # Ingress to access ArgoCD dashboard ├── README.md ├── root-app.yaml # Root ARGOCD application └── values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ – Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ └── \u0026lt;app1\u0026gt;/ ├── base/ └── overlays/ ├── \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV └── \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ – Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n── env │ ├── \u0026lt;env1-name\u0026gt; │ │ ├── kustomization.yaml │ │ ├── platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ │ └── \u0026#39;argocd-application-for-app1\u0026#39;.yaml │ │ └── workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ ├── \u0026#39;argocd-application-for-app2\u0026#39;.yaml │ │ └── \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml – The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":0,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , — this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":1,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":2,"permalink":"/posts/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure 💡 The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n✅ Visual: Prompt with icons, colors, and context-aware sections\n✅ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n✅ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":3,"permalink":"/posts/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You’ve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":4,"permalink":"/posts/random/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes — ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80’s door anymore — it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy’s new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy’s map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph’s Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity — just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":5,"permalink":"/posts/random/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ✅ Works ❌ Fails with CSRF token error Safari ✅ Works ❌ Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":6,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode — K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":7,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":8,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM — after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ✅ Yes Setup bridge/qemu policies ✅ Yes VM create/operate via libvirt ❌ No Use virt-manager GUI ❌ No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":9,"permalink":"/posts/homelab/haos-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference — opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":10,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub 👉 Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":11,"permalink":"/posts/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\n🔧 Tech Walkthroughs — Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. 🌐 Home Lab \u0026amp; Automation — Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. 🛡️ Security \u0026amp; Best Practices — Focus security, supply chain integrity, and observability. 📦 Modern Ops — GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you’re an engineer, architect, or open source enthusiast — I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":12,"permalink":"/about/","summary":"Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"},{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml ArgoCD deploying everything\n![oprah]({{ \u0026ldquo;my-blogs/static/argocd-app-of-apps/oprah.png\u0026rdquo; | relURL }})\n![oprah]({{ /argocd-app-of-apps/oprah.png | relURL }})\nAccess apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool—it\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ├── apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here │ ├── guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook │ │ ├── base │ │ └── overlays │ ├── kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ │ │ ├── base │ │ └── overlays │ └── podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize │ ├── base │ └── overlays ├── env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, │ ├── k3s │ │ ├── kustomization.yaml │ │ ├── platform │ │ └── workloads │ └── openshift │ ├── kustomization.yaml │ ├── platform │ └── workloads ├── ingress.yaml # Ingress to access ArgoCD dashboard ├── README.md ├── root-app.yaml # Root ARGOCD application └── values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ – Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ └── \u0026lt;app1\u0026gt;/ ├── base/ └── overlays/ ├── \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV └── \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ – Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n── env │ ├── \u0026lt;env1-name\u0026gt; │ │ ├── kustomization.yaml │ │ ├── platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ │ └── \u0026#39;argocd-application-for-app1\u0026#39;.yaml │ │ └── workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ ├── \u0026#39;argocd-application-for-app2\u0026#39;.yaml │ │ └── \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml – The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":0,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , — this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":1,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":2,"permalink":"/posts/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure 💡 The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n✅ Visual: Prompt with icons, colors, and context-aware sections\n✅ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n✅ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":3,"permalink":"/posts/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You’ve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":4,"permalink":"/posts/random/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes — ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80’s door anymore — it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy’s new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy’s map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph’s Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity — just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":5,"permalink":"/posts/random/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ✅ Works ❌ Fails with CSRF token error Safari ✅ Works ❌ Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":6,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode — K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":7,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":8,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM — after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ✅ Yes Setup bridge/qemu policies ✅ Yes VM create/operate via libvirt ❌ No Use virt-manager GUI ❌ No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":9,"permalink":"/posts/homelab/haos-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference — opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":10,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub 👉 Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":11,"permalink":"/posts/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\n🔧 Tech Walkthroughs — Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. 🌐 Home Lab \u0026amp; Automation — Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. 🛡️ Security \u0026amp; Best Practices — Focus security, supply chain integrity, and observability. 📦 Modern Ops — GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you’re an engineer, architect, or open source enthusiast — I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":12,"permalink":"/about/","summary":"Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"},{"content":"","date":"1 January, 0001","id":13,"permalink":"/posts/random/random/","summary":"","tags":"","title":""},{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml ArgoCD deploying everything\n![oprah]({{ \u0026ldquo;my-blogs/static/argocd-app-of-apps/oprah.png\u0026rdquo; | relURL }})\n![oprah]({{ /argocd-app-of-apps/oprah.png | relURL }})\nAccess apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool—it\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ├── apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here │ ├── guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook │ │ ├── base │ │ └── overlays │ ├── kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ │ │ ├── base │ │ └── overlays │ └── podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize │ ├── base │ └── overlays ├── env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, │ ├── k3s │ │ ├── kustomization.yaml │ │ ├── platform │ │ └── workloads │ └── openshift │ ├── kustomization.yaml │ ├── platform │ └── workloads ├── ingress.yaml # Ingress to access ArgoCD dashboard ├── README.md ├── root-app.yaml # Root ARGOCD application └── values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ – Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ └── \u0026lt;app1\u0026gt;/ ├── base/ └── overlays/ ├── \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV └── \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ – Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n── env │ ├── \u0026lt;env1-name\u0026gt; │ │ ├── kustomization.yaml │ │ ├── platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ │ └── \u0026#39;argocd-application-for-app1\u0026#39;.yaml │ │ └── workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ ├── \u0026#39;argocd-application-for-app2\u0026#39;.yaml │ │ └── \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml – The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":0,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , — this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":1,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":2,"permalink":"/posts/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure 💡 The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n✅ Visual: Prompt with icons, colors, and context-aware sections\n✅ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n✅ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":3,"permalink":"/posts/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You’ve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":4,"permalink":"/posts/random/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes — ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80’s door anymore — it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy’s new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy’s map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph’s Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity — just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":5,"permalink":"/posts/random/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ✅ Works ❌ Fails with CSRF token error Safari ✅ Works ❌ Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":6,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode — K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":7,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":8,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM — after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ✅ Yes Setup bridge/qemu policies ✅ Yes VM create/operate via libvirt ❌ No Use virt-manager GUI ❌ No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":9,"permalink":"/posts/homelab/haos-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference — opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":10,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub 👉 Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":11,"permalink":"/posts/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\n🔧 Tech Walkthroughs — Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. 🌐 Home Lab \u0026amp; Automation — Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. 🛡️ Security \u0026amp; Best Practices — Focus security, supply chain integrity, and observability. 📦 Modern Ops — GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you’re an engineer, architect, or open source enthusiast — I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":12,"permalink":"/about/","summary":"Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"},{"content":"\nArgoCD deploying everything\n","date":"1 January, 0001","id":13,"permalink":"/posts/random/random/","summary":"","tags":"","title":""},{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml Access apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool—it\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ├── apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here │ ├── guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook │ │ ├── base │ │ └── overlays │ ├── kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ │ │ ├── base │ │ └── overlays │ └── podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize │ ├── base │ └── overlays ├── env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, │ ├── k3s │ │ ├── kustomization.yaml │ │ ├── platform │ │ └── workloads │ └── openshift │ ├── kustomization.yaml │ ├── platform │ └── workloads ├── ingress.yaml # Ingress to access ArgoCD dashboard ├── README.md ├── root-app.yaml # Root ARGOCD application └── values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ – Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ └── \u0026lt;app1\u0026gt;/ ├── base/ └── overlays/ ├── \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV └── \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ – Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n── env │ ├── \u0026lt;env1-name\u0026gt; │ │ ├── kustomization.yaml │ │ ├── platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ │ └── \u0026#39;argocd-application-for-app1\u0026#39;.yaml │ │ └── workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` │ │ ├── \u0026#39;argocd-application-for-app2\u0026#39;.yaml │ │ └── \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml – The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":0,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity—and the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , — this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":1,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute — ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":2,"permalink":"/posts/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure 💡 The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n✅ Visual: Prompt with icons, colors, and context-aware sections\n✅ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n✅ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":3,"permalink":"/posts/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You’ve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":4,"permalink":"/posts/random/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes — ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80’s door anymore — it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy’s new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy’s map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph’s Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity — just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":5,"permalink":"/posts/random/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ✅ Works ❌ Fails with CSRF token error Safari ✅ Works ❌ Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":6,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode — K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":7,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":8,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM — after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ✅ Yes Setup bridge/qemu policies ✅ Yes VM create/operate via libvirt ❌ No Use virt-manager GUI ❌ No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":9,"permalink":"/posts/homelab/haos-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic — all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference — opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":10,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub 👉 Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":11,"permalink":"/posts/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\n🔧 Tech Walkthroughs — Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. 🌐 Home Lab \u0026amp; Automation — Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. 🛡️ Security \u0026amp; Best Practices — Focus security, supply chain integrity, and observability. 📦 Modern Ops — GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you’re an engineer, architect, or open source enthusiast — I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":12,"permalink":"/about/","summary":"Welcome to my blog — a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"},{"content":"\nArgoCD deploying everything\n","date":"1 January, 0001","id":13,"permalink":"/posts/random/random/","summary":"","tags":"","title":""}]