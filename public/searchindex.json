[{"content":"Part of series on Kubevirt Create a (homeassistant VM) containerdisk Setup Bridge Network for a Kubevirt VM 1. Prerequisites Before diving in, ensure you have:\nA K3s cluster with kubectl or oc access. Linux bridge configured on host (bridge0). jq installed (sudo dnf install jq or brew install jq). Basic knowledge of KubeVirt, CDI, and Kubernetes networking. HLD Home Network\n1.1 Router (192.168.50.1): Provides LAN IPs via DHCP.\n1.2 User Device: Laptop/Phone accessing VM or cluster services.\nKubernetes Host\n2.1 Physical NIC (enp3s0): Connects host to LAN.\n2.2 Linux Bridge (bridge0): Virtual switch connecting VMs to LAN.\n2.3 Flow: NIC ‚Üí Bridge ‚Üí VM ‚Üí LAN\nKubernetes Cluster\n3.1 Multus CNI: Manages VM network attachments.\n3.2 KubeVirt VM (Fedora): Attaches to bridge0 via homenet NAD.\nNetwork Flow\n4.1 VM gets LAN IP via DHCP.\n4.2 VM ‚Üî Linux Bridge ‚Üî Host NIC ‚Üî Router ‚Üî LAN\n4.3 VM can communicate directly with user devices.\nKey Points\n5.1 VM behaves like a normal LAN device.\n5.2 LAN devices can access VM directly.\n5.3 Multus + bridge allows bypassing pod network NAT.\ngraph TD %% Home Network subgraph \u0026#34;Home Network\u0026#34; Router[\u0026#34;Router / DHCP Server\u0026lt;br\u0026gt;IP: 192.168.50.1\u0026#34;] UserDevice[\u0026#34;Laptop / Phone\u0026#34;] end %% Kubernetes Host subgraph \u0026#34;Kubernetes Host\u0026#34; direction LR HostNIC[\u0026#34;Physical NIC: enp3s0\u0026#34;] LinuxBridge[\u0026#34;Linux Bridge: bridge0\u0026#34;] HostNIC --\u0026gt;|Port added to| LinuxBridge end %% Kubernetes Cluster subgraph \u0026#34;Kubernetes Cluster\u0026#34; Multus[\u0026#34;Multus CNI\u0026#34;] KubeVirtVM[\u0026#34;KubeVirt VM - Fedora\u0026#34;] end %% Network Flow Router --\u0026gt;|DHCP Lease / LAN Traffic| HostNIC LinuxBridge --\u0026gt;|Virtual Port| Multus Multus --\u0026gt;|Attaches VM to bridge - NAD homenet| KubeVirtVM KubeVirtVM --\u0026gt;|Traffic to/from LAN| LinuxBridge KubeVirtVM --\u0026gt;|Gets LAN IP| Router KubeVirtVM --\u0026gt;|Communicates with| UserDevice UserDevice --\u0026gt;|Connects to| KubeVirtVM %% Styles style Router fill:#fcf,stroke:#333,stroke-width:2px style UserDevice fill:#cff,stroke:#333,stroke-width:2px style HostNIC fill:#ffc,stroke:#333,stroke-width:2px style LinuxBridge fill:#bbf,stroke:#333,stroke-width:2px style Multus fill:#ff9,stroke:#333,stroke-width:2px style KubeVirtVM fill:#9f9,stroke:#333,stroke-width:2px 2. Why use a Bridge Network for KubeVirt VMs KubeVirt VMs normally connect to the cluster CNI (Flannel, Calico, etc), which is perfect for pod-to-pod communication but limits VMs that need:\nLAN visibility ‚Äì VM gets an IP on the same subnet as your home devices. Direct device access ‚Äì Talk to IoT devices, NAS, printers without NAT. Predictable IPs ‚Äì DHCP or static assignment for consistent network identity. Solution: Multus + bridge CNI. The VM attaches to bridge0 on the host, making it a full LAN citizen.\nProcess flow: graph TB A[\u0026#34;Create Linux Bridge on Host\u0026#34;] --\u0026gt; B[\u0026#34;Install KubeVirt \u0026amp; CDI\u0026#34;] B --\u0026gt; C[\u0026#34;Install Multus\u0026#34;] C --\u0026gt; D[\u0026#34;Create NetworkAttachmentDefinition\u0026#34;] D --\u0026gt; E[\u0026#34;Deploy VM with Network Attachment\u0026#34;] 3. Host Linux Bridge Setup (bridge0) with NetworkManager bridge0 is a virtual switch; the physical interface (enp3s0) connects it to your LAN. VMs attach to bridge0 via Multus.\nCLI Steps # 3.1 Create the bridge nmcli connection add type bridge con-name bridge0 ifname bridge0 stp no # 3.2 Add physical NIC to bridge nmcli connection add type ethernet con-name \u0026#34;Wired Connection\u0026#34; ifname enp3s0 master bridge0 # 3.3 Configure DHCP (or static if desired) nmcli connection modify bridge0 ipv4.method auto nmcli connection modify bridge0 ipv6.method auto # 3.4 Enable autoconnect nmcli connection modify bridge0 connection.autoconnect yes nmcli connection modify \u0026#34;Wired Connection\u0026#34; connection.autoconnect yes # 3.5 Bring up connections nmcli connection up bridge0 nmcli connection up \u0026#34;Wired Connection\u0026#34; # 3.6 Verify setup nmcli connection show nmcli device status ‚úÖ Result: bridge0 is live, VMs attached will get LAN IPs automatically.\nüí° Tip: Use nmcli on headless servers for fully automated bridge setup.\n4. Install KubeVirt Deploy the operator and KubeVirt CR to run VMs on Kubernetes.\nexport KUBEVIRT_VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name) echo $KUBEVIRT_VERSION kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-operator.yaml kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-cr.yaml kubectl get pods -n kubevirt ‚≠êÔ∏è Tip: Wait until all pods in kubevirt namespace are Running before continuing. 5. Install Containerized Data Importer (CDI) CDI handles VM image uploads and DataVolumes.\nexport CDI_VERSION=$(curl -s https://api.github.com/repos/kubevirt/containerized-data-importer/releases/latest | jq -r .tag_name) echo $CDI_VERSION kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml kubectl get pods -n cdi # kubectl -n cdi port-forward svc/cdi-uploadproxy 8443:443 6. Install Multus on K3s Multus enables multiple interfaces per VM/pod‚Äîessential for bridge networking.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: multus namespace: kube-system spec: repo: https://rke2-charts.rancher.io chart: rke2-multus targetNamespace: kube-system valuesContent: |- config: fullnameOverride: multus cni_conf: confDir: /var/lib/rancher/k3s/agent/etc/cni/net.d binDir: /var/lib/rancher/k3s/data/cni/ kubeconfig: /var/lib/rancher/k3s/agent/etc/cni/net.d/multus.d/multus.kubeconfig multusAutoconfigDir: /var/lib/rancher/k3s/agent/etc/cni/net.d manifests: dhcpDaemonSet: true EOF 7. Create DataVolume (Fedora 42) Create a persistent VM disk.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: annotations: cdi.kubevirt.io/storage.bind.immediate.requested: \u0026#34;\u0026#34; name: fedora-dv-42 spec: contentType: kubevirt source: http: url: \u0026#34;https://download.fedoraproject.org/pub/fedora/linux/releases/42/Cloud/x86_64/images/Fedora-Cloud-Base-Generic-42-1.1.x86_64.qcow2\u0026#34; storage: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi EOF 8. Network Attachment Definition Attach VM to bridge0. You can choose DHCP or Static IP.\nDHCP Static IP cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: homenet spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;bridge0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dhcp\u0026#34; } }\u0026#39; EOF cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: homenet spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;bridge0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;static\u0026#34;, \u0026#34;addresses\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;192.168.50.204/24\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;192.168.50.1\u0026#34; } ], \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;192.168.50.1\u0026#34; } ], \u0026#34;dns\u0026#34;: { \u0026#34;nameservers\u0026#34;: [\u0026#34;192.168.50.1\u0026#34;, \u0026#34;1.1.1.1\u0026#34;] } } }\u0026#39; EOF NAD diagram graph TD NAD_Choice{{\u0026#34;Network Attachment Definition\u0026lt;br\u0026gt;(homenet)\u0026#34;}} subgraph \u0026#34;Option 1: Dynamic IP\u0026#34; DHCP[\u0026#34;IPAM: DHCP\u0026#34;] DHCP_Config[(\u0026#34;ipam: { type: dhcp }\u0026#34;)] NAD_Choice --\u0026gt;|Selects| DHCP DHCP --\u0026gt;|Uses CNI Plugin| DHCP_Config end subgraph \u0026#34;Option 2: Static IP\u0026#34; Static[\u0026#34;IPAM: Static\u0026#34;] Static_Config[(\u0026#34;ipam: { type: static, addresses: [...] }\u0026#34;)] NAD_Choice --\u0026gt;|Selects| Static Static --\u0026gt;|Specifies configuration| Static_Config end style NAD_Choice fill:#fff,stroke:#333,stroke-width:2px 9. Create VirtualMachine Deploy Fedora VM with homenet interface.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: fedora-dv-bridge labels: kubevirt.io/os: linux spec: runStrategy: Always template: metadata: labels: kubevirt.io/domain: vm spec: domain: cpu: cores: 1 devices: disks: - name: disk0 disk: bus: virtio - name: cloudinitdisk cdrom: bus: sata readonly: true interfaces: - name: homenet bridge: {} model: virtio machine: type: q35 resources: requests: memory: 2048M networks: - name: homenet multus: networkName: homenet volumes: - name: disk0 persistentVolumeClaim: claimName: fedora-dv-42 - name: cloudinitdisk cloudInitNoCloud: userData: | #cloud-config hostname: fedora-dv-bridge ssh_pwauth: True password: fedora chpasswd: {expire: False} runcmd: - dnf install -y qemu-guest-agent cockpit - systemctl enable qemu-guest-agent - systemctl enable --now cockpit.socket - systemctl start qemu-guest-agent cockpit EOF üéâ Congratulations! Your KubeVirt VM now has a LAN IP, can SSH in, use Cockpit, and interact with home network devices seamlessly.\n","date":"23 August, 2025","id":0,"permalink":"/posts/kubevirt_bridge/","summary":"","tags":"kubernetes multus kubevirt networking","title":"Setup Bridge Network for a Kubevirt VM"},{"content":"1. Introduction: What is Home Assistant? Home Assistant is a powerful, open-source home automation platform that puts local control and privacy first. It can be run on various devices, from single-board computers like the Raspberry Pi to virtual machines and containers. It\u0026rsquo;s a great tool for anyone looking to automate their home without relying on big tech companies, giving you complete control over your smart devices.\n2. Installation Methods Home Assistant offers a few different ways to get started, but two of the most common are:\nHome Assistant Operating System (HaOS): This is the recommended and most popular installation method. HaOS is a lightweight, embedded operating system designed specifically to run Home Assistant and its ecosystem. It\u0026rsquo;s easy to install on a dedicated device like a Home Assistant Green, a Raspberry Pi, or a virtual machine. This method gives you access to the full Home Assistant experience, including the convenience of add-ons, which are pre-packaged applications that extend its functionality.\nHome Assistant Container: This method is for more advanced users who want to run Home Assistant within a container environment (like Docker or Podman). You\u0026rsquo;re responsible for managing the underlying operating system and the container yourself. While this offers flexibility, it comes with a trade-off: you don\u0026rsquo;t get access to the official add-ons.\n3. HaOS on KubeVirt: The Best of Both Worlds If you\u0026rsquo;re already running a Kubernetes homelab and prefer the flexibility of containerized applications but still want the convenience and features of the full HaOS experience, KubeVirt is the perfect solution. KubeVirt is a Kubernetes add-on that lets you run traditional virtual machines (VMs) alongside your container workloads. This means you can run the full HaOS as a VM right inside your Kubernetes cluster, giving you a powerful, unified platform for both your containers and your home automation.\n4. Install KubeVirt and CDI The first step is to get KubeVirt and its dependencies up and running in your cluster. We will also install the Containerized Data Importer (CDI), which is essential for importing the virtual machine disk image into Kubernetes.\nInstall Kubevirt \u0026amp; CDI Install Kubevirt # Install KuberVirt export KUBEVIRT_VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name) echo $KUBEVIRT_VERSION kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-operator.yaml kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-cr.yaml kubectl get pods -n kubevirt # Install Containerized Data Importer (CDI) export CDI_VERSION=$(curl -s https://api.github.com/repos/kubevirt/containerized-data-importer/releases/latest | jq -r .tag_name) echo $CDI_VERSION kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml kubectl get pods -n cdi # kubectl -n cdi port-forward svc/cdi-uploadproxy 8443:443 ‚≠êÔ∏è NOTE: Steps 5-7 are about creating your own HaOS containerdisks. If you are happy to use my image based on HomeAssistant 16.1 then go to step-8 directly.\nIn case you are interested. Here are the official containerdisk images for various OS\n5. Download the HaOS image # Download the HaOS qcow2 image wget https://github.com/home-assistant/operating-system/releases/download/16.1/haos_ova-16.1.qcow2.xz # unzip xz -dk haos_ova-16.1.qcow2.xz 6. Convert the image Convert the image into a foramt that is understood by Kubevirt\nsudo dnf install podman libguestfs-tools guestfs-tools -y # machine-id gave error # virt-sysprep -a haos_ova-16.1.qcow2 --operations machine-id,bash-history,logfiles,tmp-files,net-hostname,net-hwaddr Image_name=haos_ova-16.1.qcow2 # Make Golden Image by removing unique identifiers and temporary files from the image. virt-sysprep -a $Image_name --operations bash-history,logfiles,tmp-files,net-hostname,net-hwaddr # Conpress image qemu-img convert -c -O qcow2 $Image_name \u0026#34;$Image_name-containerimage.qcow2\u0026#34; 7. Build the image and push to your repo # Create your Containerfile tee Containerfile \u0026gt; /dev/null \u0026lt;\u0026lt;EOL FROM kubevirt/container-disk-v1alpha ADD $Image_name-containerimage.qcow2 /disk/ EOL # Login to your repo podman login quay.io -u arslankhanali -p \u0026lt;\u0026gt; # Make sure `homeassistant` repo exists and it is public # Buld the image podman build -t quay.io/arslankhanali/homeassistant:v1 . # Push the image podman push quay.io/arslankhanali/homeassistant:v1 8. Deploy the VM on Kubevirt I will deploy in the namespace vm\nkubectl create ns vm kubectl config set-context --current --namespace=vm cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: name: haos namespace: vm # \u0026lt;-- Change as per need spec: domain: resources: requests: memory: 2048Mi cpu: 1 limits: memory: 4096Mi cpu: 2 devices: disks: - name: containerdisk disk: bus: virtio rng: {} firmware: bootloader: efi: secureBoot: false # ‚úÖ disable SecureBoot to avoid SMM requirement terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: quay.io/arslankhanali/homeassistant:v1 # \u0026lt;-- Change as per need --- apiVersion: v1 kind: Service metadata: name: haos namespace: vm spec: type: NodePort selector: vmi: haos ports: - name: haos-ui port: 8123 protocol: TCP targetPort: 8123 nodePort: 30003 # \u0026lt;-- Change as per need - Will be random it not set EOF 9. Console and port forward virtctl console haos # Password is ususlly set in cloud init inside vmi yaml 10. Console and port forward virtctl port-forward vmi/haos 8123:8123 curl -kI http://localhost:8123/onboarding.html curl -kI http://192.168.50.200:30003 Delete oc delete -f vm-haos.yaml oc delete vmi haos oc delete svc haos References:\nhttps://github.com/ormergi/vm-image-builder/tree/main?tab=readme-ov-file ","date":"17 August, 2025","id":1,"permalink":"/posts/featured/haos-on-kubevirt/","summary":"Home Assistant is a powerful, open-source home automation platform that puts local control and privacy first. It can be run on various devices, from single-board computers like the Raspberry Pi to virtual machines and containers. It\u0026rsquo;s a great tool for anyone looking to automate their home without relying on big tech companies, giving you complete control over your smart devices.","tags":"kubernetes podman","title":"Home Assistant VM on Kubevirt"},{"content":"0. General logins I used with my K3s email. : neogeo@gmail.com username. : neo password 1 : geo password 2 : Admin@123 1. Login to K3s scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config sed -i \u0026#39;\u0026#39; \u0026#39;s|server: https://127\\.0\\.0\\.1:6443|server: https://192.168.50.200:6443|\u0026#39; ~/k3s-config export KUBECONFIG=~/k3s-config oc get all -A 2. Argo oc get secret -n argocd argocd-initial-admin-secret -o jsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d rfgyTgvrj3RzWee1 3. Headlamp oc get secret headlamp-admin-secret-token -n headlamp -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Bearer token:\neyJhbGciOiJSUzI1NiIsImtpZCI6Ik9QLUlRd0RDOEdaeXFyZ3g0dnVkT1V3RzNIN0oweS1SODVaVDhza0d4RkEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJoZWFkbGFtcCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJoZWFkbGFtcC1hZG1pbi1zZWNyZXQtdG9rZW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiaGVhZGxhbXAtYWRtaW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2YWIyMmQ2ZC1kZmVmLTQ5NzAtODI3MS1jNWU1NjA3NjYyMTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6aGVhZGxhbXA6aGVhZGxhbXAtYWRtaW4ifQ.eZXZsTH_foaS2VKJuwz8Bj8U337xX6v7KRJCb9ZcQ9TYhbd2CeW9Q9VPIEv6lAEJYbBf_C-RTxnLsszS_lNHPk0sOnTMt6v6bb7qHKP-2uoYrLcGV0zBg-b1QLN2-fcYJEXFE_D_qbkLrclHxzmG-m4xtf2CFJPz-z4PxvTGYdsP1QONTjcdXqdEOSeFIbFSpvtgQ2NRHMOmIV4ANc8jccb_AEhGiFLnSW_6G1TXkgNFXhnUpwR4o42nV3V1_9W-qLUoSwqy7Owh6lfl3SErxy8_IaHf8JMJykhlEDhZQGzXcCIc3tQPtoVsJFwYWfMWmflh16aUmJiUmKKfDrmnqA 4. AWX oc get secret -n awx-operator awx-demo-admin-password -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d wDhnfcMYlNzrGqDkZVUFmq03tQxcpWLt 5. Gitea neo Admin@123 6. Grafana oc get secret -n grafana grafana -o jsonpath=\u0026#39;{.data.admin-password}\u0026#39; | base64 -d T6zGt8FwF2cKUYa9o88uqCOYxE0DE6a2Gpc3qosm 6. Harbor oc get secrets -n harbor harbor-core -o jsonpath=\u0026#39;{.data.HARBOR_ADMIN_PASSWORD}\u0026#39; | base64 -d Admin@123 7. 8. 9. 10. ","date":"15 August, 2025","id":2,"permalink":"/hidden/k3s-logins/","summary":"Bearer token:","tags":"kubernetes podman","title":"K3s Logins"},{"content":"Notes REMEMBER to run kubectl and kustomize apply commands from the correct Namespace/Project! Helm will fuck up otherwise Resources Keep requests.cpu and requests.mem very low. So not a lot of resource is dedicated to the workload. No point giving 512Mi to a workload that is only using 100Mi. Do not put any limits.cpu and limits.mem. Pods crash saying OOM (out of memory) i.e. prometheus Only set it if you fear that a spike from this workload will overwhelm the node. Replicas Keep them to 1 ERROR: _non_namespaceable_\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;olm\u0026quot;}}] error: no objects passed to apply Remove namespace when resource has its own namespace e.g. olm Always use ClusterIP and expose via Ingress. LoadBalancer and NodePort will clash with host ports service: type: ClusterIP When you run the kustomize build or helm commands, it will download the chart in the directory. You can get the default values.yaml file in it. It is always a good idea to search for terms like enabled replicas resources ingress There should be only one storage class local-path is faster - so keep it default - also you might have to delete longhorn , it would mess up everything Use longhorn when a pod needs RWX storage # Check oc get storageclass # Remove default oc annotate storageclass longhorn storageclass.kubernetes.io/is-default-class- # Make default oc annotate storageclass local-path storageclass.kubernetes.io/is-default-class=true Cert-manager uses a webhook to validate resources like Certificate and Issuer before they are stored in etcd. Remove it in testing/dev env oc patch validatingwebhookconfiguration cert-manager-webhook --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/webhooks/0/failurePolicy\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;Ignore\u0026quot;}]' Homepage Annotations annotations: gethomepage.dev/enabled: \u0026#34;true\u0026#34; gethomepage.dev/name: \u0026lt;\u0026gt; gethomepage.dev/description: \u0026lt;\u0026gt; gethomepage.dev/group: Cluster Management | Developer Tools | Storage | Security | Monitoring | Server gethomepage.dev/icon: \u0026lt;\u0026gt;.png # \u0026lt;-- https://github.com/homarr-labs/dashboard-icons/tree/main/png # e.g. \u0026#34;app.kubernetes.io/name: longhorn\u0026#34; , This provides running status in homepage UI gethomepage.dev/pod-selector: |- app.kubernetes.io/name in ( longhorn ) gethomepage.dev/weight: \u0026#39;0\u0026#39; 1. Create ns=grafana oc create ns $ns oc project $ns # For Kustomize with Helm kustomize build ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ --enable-helm | oc apply -f - # For Kustomize oc apply -k ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ # For a simple Yaml manifest oc apply -f ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ingress.yaml 2. Verify ns=grafana oc get all -A oc get all curl -Ik --resolve $ns.node1:443:192.168.50.200 https://$ns.node1 # after /etc/hosts file is updated curl -Ik https://$ns.node1 # Node: See all requests and limits for cpu and mem oc describe nodes node1 2.1 Logs ns=grafana oc project $ns oc logs pods/ oc describe pods/ 2.2 Use --previous flag for why the pods/container crashed oc logs pods/prometheus-server-7ffd965767-jhdpc -c prometheus-server --previous\n3. Stop oc scale deployment --all --replicas=0 -n grafana oc scale deployment --all --replicas=1 -n grafana 4. Delete all ns=grafana oc project $ns kustomize build ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ --enable-helm | oc delete -f - oc delete --all all -n $ns oc delete --all pods -n $ns --force --grace-period=0 oc delete --all pvc -n $ns oc delete ns $ns 4.1. Delete pods Use --grace-period=0 --force\noc delete --all pods --force --grace-period=0 -n $ns 4.2. Delete: Kube proxy method e.g. Delete PVC\nCreate namespace for it again if deleted delete any webhooks start oc proxy in another terminal Run below: remember to change and oc proxy # Delete Namespace ns=cattle-system curl -X PATCH http://127.0.0.1:8001/api/v1/namespaces/$ns \\ -H \u0026#34;Content-Type: application/json-patch+json\u0026#34; \\ --data \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/finalizers\u0026#34;}]\u0026#39; 4.3. Delete: Remove Finalizer e.g. Delete namespace\nns=grafana oc get ns $ns -o json | jq \u0026#39;.spec.finalizers=[]\u0026#39; | oc replace --raw /api/v1/namespaces/$ns/finalize -f - 5. Exec in container in a pod oc exec -it \u0026lt;pod\u0026gt; -c \u0026lt;container\u0026gt; -- sh 6. Ingresss K3s is bound to 192.168.50.200 IP - and podman uses 192.168.50.100 IP for Traefik.\nkubectl logs -n kube-system deploy/traefik # Get Traefik version kubectl -n kube-system get deploy traefik -o jsonpath=\u0026#39;{.spec.template.spec.containers[0].image}\u0026#39; oc -n kube-system get svc traefik # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # traefik LoadBalancer 10.43.148.201 192.168.50.200 80:30854/TCP,443:31067/TCP 3d1h # No other svc will have EXTERNAL-IP. because they are only exposed via Ingress 6.1. DNS nslookup jellyseerr.ak Server: 192.168.50.100 Address: 192.168.50.100#53 Name: jellyseerr.ak Address: 192.168.50.100 nslookup jellyseerr.node1 Server: 192.168.50.100 Address: 192.168.50.100#53 Name: jellyseerr.node1 Address: 192.168.50.200 7. ERROR: Failed to allocate directory watch: Too many open files # Check Limits ssh node1 ulimit -n 1024 cat /proc/$(pgrep -f kubelet)/limits | grep \u0026#34;open files\u0026#34; Max open files 1048576 1048576 files cat /proc/sys/fs/inotify/max_user_watches cat /proc/sys/fs/inotify/max_user_instances 122145 128 # Increase limits: Temporarily sudo sysctl -w fs.inotify.max_user_watches=524288 sudo sysctl -w fs.inotify.max_user_instances=1024 # Restart all sudo systemctl restart k3s oc delete pod -n kubevirt -l kubevirt.io=virt-handler # Increase limits: Permanently (survives reboot) echo \u0026#34;fs.inotify.max_user_watches=524288\u0026#34; | sudo tee -a /etc/sysctl.conf echo \u0026#34;fs.inotify.max_user_instances=1024\u0026#34; | sudo tee -a /etc/sysctl.conf sudo sysctl -p 8. Check version of Helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm search repo prometheus-community/prometheus --versions helm search repo prometheus-community/prometheus --versions | grep \u0026#39;prometheus-community/prometheus \u0026#39; ","date":"15 August, 2025","id":3,"permalink":"/hidden/k3s/","summary":"oc logs pods/prometheus-server-7ffd965767-jhdpc -c prometheus-server --previous","tags":"kubernetes podman","title":"K3s"},{"content":"Introduction Welcome to another installment in our AI series! Today, we\u0026rsquo;re going to set up Open WebUI, a powerful, self-hosted web interface for interacting with various large language models (LLMs). This tool provides a beautiful user experience similar to ChatGPT but on your own terms. We\u0026rsquo;ll be using Podman to containerize Open WebUI, making it a breeze to manage.\nWhy Open WebUI? Open WebUI acts as a central hub for your LLMs. It can connect to local models running on Ollama or LM Studio, as well as remote services like MaaS (Model as a Service). This flexibility allows you to experiment with different models and backends all from a single, consistent interface.\nStep 1: Pulling the Open WebUI Image First, we need to pull the Open WebUI container image from its public registry. Podman makes this simple and efficient.\n# This command fetches the latest main version of the Open WebUI image from GitHub\u0026#39;s container registry. podman pull ghcr.io/open-webui/open-webui:main Step 2: Running the Container Now, let\u0026rsquo;s run the container with all the necessary configurations. We\u0026rsquo;ll map a port, set environment variables, and create a persistent volume for data.\npodman run -d \\ --name open-webui \\ -p 3001:8080 \\ -e OLLAMA_BASE_URL=http://192.168.50.50:11434 \\ # If you have Ollama -e OPENAI_API_KEY=dummykey \\ -v open-webui:/app/backend/data:z \\ --restart=always \\ ghcr.io/open-webui/open-webui:main Step 3: Accessing Open WebUI Once the container is running, you can access the web interface by navigating to your host machine\u0026rsquo;s IP address and the mapped port.\nhttp://192.168.50.200:3001\nInitial Setup The first time you visit the page, you\u0026rsquo;ll be prompted to create a user account. After creating your account, you can log in and access the admin settings to configure your LLM connections.\nStep 4: Configuring Connections In the Open WebUI admin panel, you can add various connections to different LLM services.\nAdd connections: Settings -\u0026gt; Admin Panel -\u0026gt; Conenctions http://192.168.50.200:3001/admin/settings/connections e.g. Notice URL format\nOLLAMA: http://192.168.50.50:11434 # \u0026lt; \u0026mdash; From Ollama CLI OPENAI: http://192.168.50.50:1234/v1 # \u0026lt; \u0026mdash; From LMStudio\n‚ö†Ô∏è WARNING: Remember to use http or https as appropriate for your connection. Always use the correct port and make sure your firewall is configured to allow traffic on these ports.\nYou now have a fully functional Open WebUI instance running on your server, giving you a powerful tool to manage and interact with all your LLMs in one place!\n","date":"8 August, 2025","id":4,"permalink":"/posts/ai/openwebui-on-podman/","summary":"Welcome to another installment in our AI series! Today, we\u0026rsquo;re going to set up Open WebUI, a powerful, self-hosted web interface for interacting with various large language models (LLMs). This tool provides a beautiful user experience similar to ChatGPT but on your own terms. We\u0026rsquo;ll be using Podman to containerize Open WebUI, making it a breeze to manage.","tags":"openwebui podman ai ollama llm","title":"Open WebUI with Podman"},{"content":"Introduction Today, we\u0026rsquo;re going to deploy Dify, an AI application development platform, using Podman and podman-compose. Dify provides a powerful, visual way to build and manage AI applications, and deploying it on your local fedora server gives you full control.\nPrerequisites Before we start, make sure you have Podman and podman-compose installed. If you haven\u0026rsquo;t, you can refer to my previous blog post on how to set up Podman.\nüü¢ NOTE: This guide assumes you are running on a RHEL-based system with SELinux enabled, which is a common setup for fedora environments. The steps for SELinux are crucial for a smooth deployment.\nStep 1: Clone the Dify Repository First, we need to clone the Dify repository from GitHub. This will give us the necessary configuration files to deploy the application.\ngit clone [https://github.com/langgenius/dify.git](https://github.com/langgenius/dify.git) cd ~/dify/docker cp .env.example .env After cloning, we move into the docker directory and copy the example environment file to .env. This file contains the configuration variables for our Dify deployment.\nStep 2: Configure SELinux For systems with SELinux enabled, you\u0026rsquo;ll need to adjust the security context of the Dify volumes. This step prevents permission errors when Podman tries to access the files. By setting the context correctly, we avoid having to use the :z flag in our compose file.\nchcon -Rt container_file_t /home/neo/dify/docker/ ls -Z Step 3: Modify the docker-compose.yaml We need to make a couple of small but important changes to the docker-compose.yaml file to ensure Dify runs smoothly with Podman.\nvim docker-compose.yaml Change 1: Add Default Networks Add a default network to the end of the file. This ensures all services that don\u0026rsquo;t have an explicit network defined can communicate.\n# Go to end of file `shift+g` networks: ssrf_proxy_network: driver: bridge internal: true milvus: driver: bridge opensearch-net: driver: bridge internal: true default: driver: bridge Change 2: Ports Adjust the NGINX ports to avoid conflicts on your host. We\u0026rsquo;ll change the host ports from 80 and 443 to 980 and 9443 respectively. Remember to also change the corresponding variables in the .env file you copied earlier.\n# Search `/NGINX_PORT` ports: - \u0026#39;980:${NGINX_PORT:-80}\u0026#39; - \u0026#39;9443:${NGINX_SSL_PORT:-443}\u0026#39; Might also have to change nginx port in .env file 980 and 9443\nStep 4: Deploy and Verify Now we\u0026rsquo;re ready to deploy Dify using podman-compose.\nStart the containers\npodman-compose up -d podman pod logs pod_docker -f Step 5: Open Ports sudo firewall-cmd --add-port=980/tcp --permanent sudo firewall-cmd --reload Step 6: Test and Access podman pod logs pod_docker -f curl http://localhost:980/apps curl http://192.168.50.200:980/apps That\u0026rsquo;s it! You now have Dify running on your server. You can start building AI applications and experimenting with the platform.\n","date":"8 August, 2025","id":5,"permalink":"/posts/ai/dify-on-podman/","summary":"Today, we\u0026rsquo;re going to deploy Dify, an AI application development platform, using Podman and podman-compose. Dify provides a powerful, visual way to build and manage AI applications, and deploying it on your local fedora server gives you full control.","tags":"dify podman ai langgenius docker-compose","title":"Dify: A Podman Deployment Guide"},{"content":"Part of series on Arr-stack:\nArr-Stack Installation Arr-Stack Configuration Introduction The \u0026ldquo;arr stack\u0026rdquo; is a popular collection of applications used for managing media libraries. By running these applications as rootless containers with Podman, you can enhance security by preventing the containers from having root privileges on the host machine.\nThis guide will walk you through setting up the arr stack, including Jellyfin, Jellyseerr, Prowlarr, Sonarr, Radarr, and Transmission as rootless containers. We will cover directory preparation, mounting an external hard drive, and configuring the services using a podman-compose.yaml file.\nWhat is the \u0026ldquo;arr stack\u0026rdquo;? üóÉÔ∏è ‚ö†Ô∏è Check legal obligations where you live The arr stack is a suite of applications that work together to automate the process of finding, downloading, and organizing movies, TV shows, and other media.\nApplication Purpose Jellyfin A free software media system that lets you control the management and streaming of your media. Jellyseerr A request management and media discovery tool for Jellyfin and other \u0026lsquo;arr\u0026rsquo; apps. Prowlarr An indexer manager for the other \u0026lsquo;arr\u0026rsquo; apps, providing a centralized way to manage your indexers (sources for media). Sonarr Manages and automates the downloading of TV shows. Radarr Manages and automates the downloading of movies. Transmission A lightweight BitTorrent client used for downloading the media files. Prerequisites Before you start, make sure you have Podman and podman-compose installed. For Podman, you can often find it in your distribution\u0026rsquo;s package manager. For podman-compose, it\u0026rsquo;s a Python script that you can install with pip.\n# Example for a Fedora-based system sudo dnf install podman podman-docker # install podman-compose pip install podman-compose Setup Directories and External Drive First, we need to create the necessary directory structure for our applications and media files. This setup ensures that all your data is organized and easily accessible to the containers.\n# Set up a base directory for the arr stack dir=~/arr-stack mkdir -p $dir cd $dir # Create subdirectories for media and application downloads mkdir -p $dir/tvshows mkdir -p $dir/movies mkdir -p $dir/books mkdir -p $dir/transmission/downloads/complete/radarr mkdir -p $dir/transmission/downloads/complete/tv-sonarr mkdir -p $dir/transmission/downloads/incomplete/ mkdir -p $dir/transmission/watch mkdir -p $dir/prowler Mount an External Hard Drive If you have a large media library, it\u0026rsquo;s best to store it on an external hard drive. This section shows you how to mount the drive manually or automatically.\nManual Mount To manually mount your external drive, first find its device name and then mount it with the correct permissions. The uid=1000 and gid=1000 options ensure the drive is owned by your user, which is crucial for rootless containers. The context option is important for SELinux.\n# First, create a mount point mkdir -p $dir/external-drive # List block devices to find your external drive (e.g., /dev/sda1) lsblk # NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS # sda 8:0 0 1.8T 0 disk # ‚îî‚îÄsda1 8:1 0 1.8T 0 part ~/arr-stack/external-drive # Manually mount the drive. Replace /dev/sda1 with your device. sudo mount -t exfat -o users,noexec,uid=1000,gid=1000,context=\u0026#34;system_u:object_r:svirt_sandbox_file_t:s0\u0026#34; /dev/sda1 ~/arr-stack/external-drive Automatic Mount with /etc/fstab To have your drive automatically mount at startup, you need to add an entry to /etc/fstab.\nFind the UUID: The UUID is a unique identifier for your drive. # Find the UUID of your external drive sudo blkid /dev/sda1 Add to /etc/fstab: Append a new line to /etc/fstab using tee -a to avoid overwriting the file. # Path to your UUID and path echo \u0026#39;UUID=ECA3-DE06 /home/neo/arr-stack/external-drive exfat rw,users,noexec,nofail,async,auto,uid=1000,gid=1000,umask=0022,context=system_u:object_r:svirt_sandbox_file_t:s0 0 0\u0026#39; | sudo tee -a /etc/fstab Reload and Mount: Apply the new fstab entry. systemctl daemon-reload sudo mount -a ls -la arr-stack Check permission: Make sure your user (non-root) owns the folder ls -la ~/arr-stack You can verify the mount by running ls -la ~/arr-stack/external-drive.\nNote: The umount command is used to unmount the drive.\nsudo umount ~/arr-stack/external-drive The podman-compose.yaml File This file defines all the services in your arr stack. Each service is a container with specific configurations like image, port mappings, and volume mounts. A crucial aspect here is the volume mapping (- /path/on/host:/path/in/container:z). The :z option is essential for Podman to handle SELinux permissions correctly in a rootless environment.\nLet\u0026rsquo;s briefly explain some key configurations:\nimage: Specifies the container image to use. container_name: Sets a friendly name for the container. environment: Defines environment variables, such as PUID and PGID for user and group IDs, and TZ for the timezone. volumes: Connects host directories to container directories. This is how the applications can access your media files. ports: Maps container ports to host ports, allowing you to access the web UIs. labels: Used for services like Traefik to automatically configure reverse proxying and SSL/TLS. Some services, like Sonarr, Radarr, and Transmission, are run with PUID=0 and PGID=0 inside the container. This is a common practice for these specific images to avoid permission issues when writing to the /downloads directory, which is shared among them. The rootless Podman environment handles this by mapping the container\u0026rsquo;s root user (UID 0) to your host user (UID 1000).\nYou can ignore traefik section\ntee ~/arr-stack/podman-compose.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#34;3\u0026#34; services: # https://docs.linuxserver.io/images/docker-jellyfin/ jellyfin: image: docker.io/jellyfin/jellyfin:latest container_name: jellyfin environment: - TZ=Australia/Sydney volumes: - jellyfin-config:/config:z - jellyfin-cache:/cache:z - ~/arr-stack/external-drive:/media_external-drive:z - ~/arr-stack/movies:/media_movies:z - ~/arr-stack/tvshows:/media_tvshows:z - ~/arr-stack/prowler:/media_prowler:z - ~/arr-stack/transmission:/media_transmission:z ports: - 8096:8096 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.rule=Host(`jellyfin.ak`)\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.jellyfin.loadbalancer.server.port=8096\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.tls=true\u0026#34; # https://docs.jellyseerr.dev/getting-started/docker?docker-methods=docker-compose jellyseerr: image: docker.io/fallenbagel/jellyseerr:latest container_name: jellyseerr environment: - PUID=1000 - PGID=1000 - LOG_LEVEL=debug - TZ=Australia/Sydney ports: - 5055:5055 volumes: - jellyseerr-config:/app/config:z restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.rule=Host(`jellyseerr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.jellyseerr.loadbalancer.server.port=5055\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-prowlarr/#application-setup prowlarr: image: lscr.io/linuxserver/prowlarr:latest container_name: prowlarr environment: - PUID=1000 - PGID=1000 - TZ=Australia/Sydney volumes: - prowlarr-config:/config:z - ~/arr-stack/prowler/downloads:/downloads:z ports: - 9696:9696 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.rule=Host(`prowlarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.prowlarr.loadbalancer.server.port=9696\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-sonarr/ sonarr: image: lscr.io/linuxserver/sonarr:latest container_name: sonarr environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - sonarr-config:/config:z - ~/arr-stack/tvshows:/tvshows:z - ~/arr-stack/transmission/downloads:/downloads:z ports: - 8989:8989 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.sonarr.rule=Host(`sonarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.sonarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.sonarr.loadbalancer.server.port=8989\u0026#34; - \u0026#34;traefik.http.routers.sonarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-radarr/ radarr: image: lscr.io/linuxserver/radarr:latest container_name: radarr environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - radarr-config:/config:z - ~/arr-stack/movies:/movies:z - ~/arr-stack/transmission/downloads:/downloads:z ports: - 7878:7878 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.radarr.rule=Host(`radarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.radarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.radarr.loadbalancer.server.port=7878\u0026#34; - \u0026#34;traefik.http.routers.radarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-transmission/ transmission: image: lscr.io/linuxserver/transmission:latest container_name: transmission environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - transmission-config:/config:z - ~/arr-stack/transmission/downloads:/downloads:z - ~/arr-stack/transmission/watch:/watch:z ports: - 9091:9091 - 51413:51413 - 51413:51413/udp restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.transmission.rule=Host(`transmission.ak`)\u0026#34; - \u0026#34;traefik.http.routers.transmission.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.transmission.loadbalancer.server.port=9091\u0026#34; - \u0026#34;traefik.http.routers.transmission.tls=true\u0026#34; # It always run as UID=1111(flare_bypasser) # https://github.com/yoori/flare-bypasser flare-bypasser: image: ghcr.io/yoori/flare-bypasser:latest container_name: flare-bypasser environment: - PUID=1000 - PGID=1000 - TZ=Australia/Sydney volumes: - flare-bypasser-config:/config:z ports: - 8191:8080 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.rule=Host(`flare-bypasser.ak`)\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.flare-bypasser.loadbalancer.server.port=8191\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.tls=true\u0026#34; volumes: jellyfin-config: jellyfin-cache: sonarr-config: radarr-config: jellyseerr-config: prowlarr-config: transmission-config: flare-bypasser-config: EOL Deploying the Stack Once the podman-compose.yaml file is created, you can deploy the entire stack with a single command.\n# Make sure you\u0026#39;re in the ~/arr-stack directory cd ~/arr-stack # Deploy the services in detached mode podman-compose -f podman-compose.yaml up -d --force-recreate The --force-recreate flag ensures that if any configuration changes have been made, the containers will be rebuilt from scratch.\nTo check if the containers are running and to verify user permissions, you can use the following commands:\npodman ps # To check the user inside a container podman exec -it sonarr whoami This will show you the user that the container is running as, which should be abc (uid 1000) for most services, or root (uid 0) for the PUID=0 services, which is correctly mapped to your user.\nService Address Jellyfin http://192.168.50.200:8096 Jellyserr http://192.168.50.200:5055 Prowlarr http://192.168.50.200:9696 Sonarr http://192.168.50.200:8989 Radarr http://192.168.50.200:7878 Transmission http://192.168.50.200:9091 flare-bypasser http://192.168.50.200:8191 Enable Auto start # Remove old service files - That automatically start containers containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload # Create new service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload Update all cd ~/arr-stack echo \u0026#34;Pulling all images used by running containers...\u0026#34; for img in $(podman ps --format \u0026#34;{{.Image}}\u0026#34; | sort -u); do echo \u0026#34;Pulling: $img\u0026#34; podman pull \u0026#34;$img\u0026#34; done # Remove old service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload # Re-generate and enable new service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for name in \u0026#34;${containers[@]}\u0026#34;; do # This command generates a new systemd service file for the container podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files # This moves the newly generated service file to the correct systemd user directory mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ # Enable and start the new service systemctl --user enable \u0026#34;container-$name.service\u0026#34; systemctl --user start \u0026#34;container-$name.service\u0026#34; done # Reload the systemd manager configuration systemctl --user daemon-reload Remove all cd ~/arr-stack podman-compose down -v podman image prune containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload sudo umount ~/arr-stack/external-drive sudo nano /etc/fstab systemctl daemon-reload rm -rf ~/arr-stack ","date":"8 August, 2025","id":6,"permalink":"/hidden/podman-arr-stack/","summary":"Part of series on Arr-stack:","tags":"arr-stack podman homelab","title":"Arr stack - Installation"},{"content":"1. Introduction Podman is a daemonless container engine for running OCI containers.\nIt can run as root (rootful) or as a non-root user (rootless), each with different privileges and security implications.\nFeature Rootful Podman (Run as root) Rootless Podman (Run as non-root) Privileges Full root privileges on host system (sudo podman ...). Limited to user‚Äôs privileges (podman ...). Security Compromised container could gain root on host. Limited damage ‚Äî container root maps to non-root UID on host. Networking Can bind to privileged ports (\u0026lt;1024) directly. Uses slirp4netns, cannot bind \u0026lt;1024 without extra config. User ID Mapping Container root = Host root. Container root maps to non-root UID. 2. Running Rootless Do not use sudo to start containers.\nEnable lingering for your user:\nFor a rootless container to keep running after you\u0026rsquo;ve logged out, the Podman process itself needs to be managed by a system that persists. By enabling lingering, you allow the user\u0026rsquo;s systemd instance to continue running, which in turn can manage and keep Podman-related services alive.\nsudo loginctl enable-linger \u0026#34;$USER\u0026#34; Enable Podman socket for your user:\nThis command sets up and starts the Podman API socket for the current user.\nsystemctl --user enable --now podman.socket This creates a Podman socket at:\n/run/user/\u0026lt;UID\u0026gt;/podman/podman.sock (e.g., /run/user/1000/podman/podman.sock)\n3. Volume Mounting Default volume paths:\n# Rootful /var/lib/containers/storage/volumes/ # Rootless $HOME/.local/share/containers/storage/volumes/ 4. User Mapping Details Keeep - PUID=0 \u0026amp; - PGID=0 for containers in compose file. They will run as 0 inside container but 1000 on the host. i.e. rootless on host.\nüü¢ Make sure to run PODMAN without sudo Check UID mapping:\ncat /proc/$(pgrep -u \u0026#34;$USER\u0026#34; podman | head -n 1)/uid_map Example output:\nContainer UID Host UID Range Notes 0 1000 1 Container UID 0 is mapped to UID 1000 on host 1 524288 65536 Container UID 1 is mapped to UID 524288 on host 1000 525287 65536 Container UID 1000 is mapped to UID 525287 on host (524,288 + 999 = 525,287) 5. Auto restart: Systemd Integration (Rootless) Rootless service files live in:\n$HOME/.config/systemd/user\nGenerate and enable container services:\nmkdir -p ~/.config/systemd/user/ containers=($(podman ps --format \u0026#39;{{.Names}}\u0026#39;)) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; \\ --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload 6. Update All Containers for name in $(podman ps --format \u0026#39;{{.Names}}\u0026#39;); do image=$(podman inspect --format \u0026#39;{{.ImageName}}\u0026#39; \u0026#34;$name\u0026#34;) podman pull \u0026#34;$image\u0026#34; systemctl --user restart \u0026#34;container-$name.service\u0026#34; echo \u0026#34;$name updated and restarted.\u0026#34; done 7. Stop All Containers for name in $(podman ps --format \u0026#39;{{.Names}}\u0026#39;); do echo \u0026#34;Stopping $name...\u0026#34; podman stop \u0026#34;$name\u0026#34; done 8. List Container IPs for net in $(podman network ls --format \u0026#39;{{.Name}}\u0026#39;); do echo \u0026#34;Network: $net\u0026#34; podman network inspect $net | jq -r \u0026#39; .[0].containers | to_entries[] | \u0026#34;\\(.value.name) \\(.value.interfaces.eth0.subnets[0].ipnet // \u0026#34;\u0026#34;)\u0026#34; \u0026#39; | while read -r name ipcidr; do ip=${ipcidr%%/*} [ -z \u0026#34;$ip\u0026#34; ] \u0026amp;\u0026amp; ip=\u0026#34;(none)\u0026#34; echo -e \u0026#34;$name\\t$ip\u0026#34; done echo \u0026#34;\u0026#34; done 9. Closing Note Rootless Podman is more secure and works seamlessly with systemd for automated container management.\n","date":"8 August, 2025","id":7,"permalink":"/hidden/podman-101/","summary":"","tags":"podman","title":"Podman 101 - Always Run Rootless!"},{"content":"Introduction Setting up a reverse proxy is a crucial step in managing a homelab. It allows you to expose multiple services on your network, all under a single domain, with proper SSL/TLS encryption. Traefik is an excellent, modern, and easy-to-configure reverse proxy that integrates seamlessly with container orchestrators like Podman.\nThis guide walks you through setting up Traefik on Podman, complete with a self-signed wildcard certificate for local development. This setup is perfect for homelab environments where you need secure, accessible services without the hassle of public DNS records and paid certificates.\n1. Prerequisites and System Configuration Before we deploy Traefik, we need to configure our system to allow it to run properly. We\u0026rsquo;ll enable access to privileged ports and configure the Podman socket.\nSystem Setup First, let\u0026rsquo;s allow non-root users to bind to privileged ports (like 80 and 443).\nüö® DANGER: If you are running Pi-Hole as well then you already have minimum port set as 53. In that case do not run below cmd. It will break Pi-hole # Check lowest port. If port \u0026lt; 80 set then no need to run below cmds # cat /etc/sysctl.conf echo \u0026#39;net.ipv4.ip_unprivileged_port_start=80\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p Next, we\u0026rsquo;ll enable the Podman socket for our user. This is essential for Traefik to be able to detect and configure services running in other containers.\nsystemctl --user enable --now podman.socket Finally, we\u0026rsquo;ll open up the necessary ports on the firewall to ensure Traefik can receive traffic.\nsudo firewall-cmd --add-service={http,https} --permanent sudo firewall-cmd --reload 2. Directory Structure and Certificates To keep our configuration organized, we\u0026rsquo;ll create a dedicated directory for Traefik and generate a self-signed certificate. This certificate will be used for all our local services.\nDirectory Structure We\u0026rsquo;ll create a base directory for Traefik and two subdirectories: certs for our SSL certificates and dynamic for dynamic Traefik configurations.\nBASE_DIR=\u0026#34;~/traefik\u0026#34; mkdir -p \u0026#34;$BASE_DIR/certs\u0026#34; mkdir -p \u0026#34;$BASE_DIR/dynamic\u0026#34; cd \u0026#34;$BASE_DIR\u0026#34; Self-Signed Certificate Generation This script will generate a wildcard self-signed certificate for *.ak. This means any subdomain like homeassistant.ak or traefik.ak will be trusted by Traefik.\n# FOR DNS:*.ak domain=\u0026#34;.ak\u0026#34; if [ ! -f certs/local.crt ] \u0026amp;\u0026amp; [ ! -f certs/local.key ]; then echo \u0026#34;Generating wildcard self-signed certificate for *.$domain...\u0026#34; openssl req -x509 -newkey rsa:4096 -nodes -days 825 -sha256 \\ -keyout certs/local.key -out certs/local.crt \\ -subj \u0026#34;/C=PK/ST=Punjab/L=Lahore/O=HomeLab/OU=Development/CN=$domain\u0026#34; \\ -addext \u0026#34;subjectAltName=DNS:$domain,DNS:*.$domain\u0026#34; \\ -addext \u0026#34;basicConstraints=CA:FALSE\u0026#34; \\ -addext \u0026#34;keyUsage=keyEncipherment,dataEncipherment,digitalSignature\u0026#34; \\ -addext \u0026#34;extendedKeyUsage=serverAuth\u0026#34; \\ -addext \u0026#34;authorityKeyIdentifier=keyid,issuer\u0026#34; echo \u0026#34;‚úÖ Certificate generated:\u0026#34; openssl x509 -in certs/local.crt -noout -text | grep -A1 \u0026#34;Subject Alternative Name\u0026#34; else echo \u0026#34;‚ö†Ô∏è Local certificate already exists, skipping generation.\u0026#34; fi ‚ö†Ô∏è WARNING: For browsers to trust this certificate, you\u0026rsquo;ll need to manually import it into your system\u0026rsquo;s trust store.\n3. Traefik Configuration Files Traefik uses two types of configuration files: static and dynamic. The static configuration defines the core settings, while the dynamic configuration sets up routers, services, and middleware.\nStatic Configuration (traefik.yml) This file defines Traefik\u0026rsquo;s entry points (ports 80 and 443), enables the API dashboard, and configures the docker provider to monitor Podman containers. It also enables a file provider for dynamic configurations.\ntee ~/traefik/traefik.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: https scheme: https https: address: \u0026#34;:443\u0026#34; api: dashboard: true insecure: true providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: true file: directory: /etc/traefik/dynamic watch: true EOL Dynamic TLS Configuration (dynamic.yml) This file tells Traefik where to find the self-signed certificates we generated earlier.\ntee ~/traefik/dynamic/dynamic.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; tls: certificates: - certFile: /certs/local.crt keyFile: /certs/local.key EOL Dynamic Routers and Services To expose your services (like Home Assistant), you\u0026rsquo;ll create a separate dynamic configuration file. This file defines a router to match incoming traffic and a service to forward that traffic to the correct backend.\ntee ~/traefik/dynamic/homeassistant.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; http: routers: homeassistant: rule: \u0026#34;Host(`homeassistant.ak`)\u0026#34; service: homeassistant entryPoints: - https tls: {} middlewares: [] priority: 10 services: homeassistant: loadBalancer: servers: - url: \u0026#34;http://192.168.50.202:8123\u0026#34; EOL 4. Deploying with Podman Compose We\u0026rsquo;ll use a podman-compose.yml file to define and run our Traefik container.\npodman-compose.yml This file sets up the Traefik container, maps ports, and mounts the necessary configuration directories and the Podman socket. The labels on the traefik service configure Traefik\u0026rsquo;s own dashboard.\ntee /home/neo/traefik/podman-compose.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#34;3.3\u0026#34; services: traefik: image: docker.io/library/traefik:latest container_name: traefik security_opt: - label=type:container_runtime_t ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; - \u0026#34;8080:8080\u0026#34; volumes: - /run/user/1000/podman/podman.sock:/var/run/docker.sock:z - ./certs:/certs:ro,Z - ./traefik.yml:/etc/traefik/traefik.yml:ro,Z - ./dynamic:/etc/traefik/dynamic:ro,Z labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.traefik.rule=Host(`traefik.ak`)\u0026#34; - \u0026#34;traefik.http.routers.traefik.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.traefik.loadbalancer.server.port=8080\u0026#34; - \u0026#34;traefik.http.routers.traefik.tls=true\u0026#34; EOL Starting Traefik Now, you can start Traefik with the podman compose up command.\npodman compose -f ~/traefik/podman-compose.yml up -d --force-recreate üü¢ NOTE: You can access the Traefik dashboard at http://192.168.50.200:8080\nAuto start containers=(traefik) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload This setup gives you a powerful and flexible reverse proxy for all your homelab services, secured with local certificates. You can easily add new services by simply creating a new configuration file in the dynamic directory. üöÄ\n","date":"8 August, 2025","id":8,"permalink":"/hidden/podman-traefik/","summary":"Setting up a reverse proxy is a crucial step in managing a homelab. It allows you to expose multiple services on your network, all under a single domain, with proper SSL/TLS encryption. Traefik is an excellent, modern, and easy-to-configure reverse proxy that integrates seamlessly with container orchestrators like Podman.","tags":"kubernetes podman traefik","title":"Traefik on Podman with Local Certificates"},{"content":"What Pi-hole Does Pi-hole is a network-level ad blocker that acts as a DNS sinkhole. Here\u0026rsquo;s how to run it securely on your Fedora-based homelab using Podman and Traefik, with a clean and idempotent setup.\nStep-by-Step Setup 1. Prepare System # Create working directory mkdir ~/pihole \u0026amp;\u0026amp; cd ~/pihole 2. Allow Binding to Port 53 (as non-root) echo \u0026#39;net.ipv4.ip_unprivileged_port_start=53\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p # Reloads kernel parameters from the config /etc/sysctl.conf 3. Create podman-compose.yml tee ~/pihole/podman-compose.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#39;3\u0026#39; services: pihole: container_name: pihole image: docker.io/pihole/pihole:latest restart: unless-stopped ports: - \u0026#34;192.168.50.100:53:53/tcp\u0026#34; # \u0026lt;--- Change - \u0026#34;192.168.50.100:53:53/udp\u0026#34; # \u0026lt;--- Change - \u0026#34;192.168.50.100:8099:80/tcp\u0026#34; # \u0026lt;--- Change environment: TZ: \u0026#34;Australia/Sydney\u0026#34; FTLCONF_webserver_api_password: \u0026#34;pihole\u0026#34; # \u0026lt;--- Change volumes: - pihole-etc:/etc/pihole - pihole-dnsmasq:/etc/dnsmasq.d cap_add: - NET_ADMIN dns: - 9.9.9.9 - 45.90.30.0 labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.pihole.rule=Host(`pihole.node1`)\u0026#34; # \u0026lt;--- Change - \u0026#34;traefik.http.routers.pihole.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.pihole.tls=true\u0026#34; - \u0026#34;traefik.http.services.pihole.loadbalancer.server.port=80\u0026#34; volumes: pihole-etc: pihole-dnsmasq: EOL 4. Deploy Pi-hole # Ensure clean state podman compose -f ~/pihole/podman-compose.yml down # Deploy podman compose -f ~/pihole/podman-compose.yml up -d --force-recreate Auto start # Allow the user\u0026#39;s user services (like systemd --user) to run even when not logged in sudo loginctl enable-linger \u0026#34;$USER\u0026#34; # Enable and start the Podman API socket for the current user, required for systemd + Podman integration systemctl --user enable --now podman.socket # Ensure the user systemd service directory exists mkdir -p ~/.config/systemd/user/ # Generate a systemd service unit file for the container containers=(pihole) # \u0026lt;--- Change to your container name for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload 5. Post-deploy Actions # Check HTTP response curl -kI http://192.168.50.100:8099/admin/login # Set or reset password sudo podman exec -it pihole pihole setpassword 6. Open Firewall for DNS sudo firewall-cmd --permanent --add-service=dns sudo firewall-cmd --reload 7. Access Service Address Pi-hole UI https://pihole.node1 (via Traefik) Direct IP http://192.168.50.100:8099/admin 8. DNS: Enter your local DNS entries in pihole So you don\u0026rsquo;t have to add them manually to each devices /etc/hosts file\npodman exec pihole sed -i \u0026#39;s/^ etc_dnsmasq_d *= *.*/ etc_dnsmasq_d=true/\u0026#39; /etc/pihole/pihole.toml # Create custom_hosts.conf in /etc/dnsmasq.d/ with specific entries # Add your hostnames below DOMAIN_SUFFIX=\u0026#34;.node1\u0026#34; IP=\u0026#34;192.168.50.100\u0026#34; HOSTS=( node1 homeassistant cockpit traefik pihole jellyfin jellyseerr prowlarr sonarr radarr transmission flare-bypasser readarr calibre-web calibre wazuh homepage kavita nextcloud hello test test1 test2 argocd k3s ) # address=/node1.localhost/192.168.50.100 for host in \u0026#34;${HOSTS[@]}\u0026#34;; do echo \u0026#34;address=/${host}${DOMAIN_SUFFIX}/${IP}\u0026#34; done | podman exec -i pihole tee /etc/dnsmasq.d/custom_hosts.conf \u0026gt; /dev/null Restart pihole pod\n# Verify the contents of custom_hosts.conf podman exec -it pihole cat /etc/dnsmasq.d/custom_hosts.conf podman exec -it pihole cat /etc/pihole/pihole.toml | grep etc_dnsmasq_d # Restart the pihole container to apply changes podman restart pihole Undo All\n# undo podman exec -it pihole rm /etc/dnsmasq.d/custom_hosts.conf podman exec pihole sed -i \u0026#39;s/^ etc_dnsmasq_d *= *.*/ etc_dnsmasq_d=false/\u0026#39; /etc/pihole/pihole.toml 9. Router Change Primary DNS in router to 192.168.50.100\n10. Summary You now have a self-hosted, containerized Pi-hole setup running under Podman, fronted by Traefik, and configured for secure DNS resolution across your homelab.\n11. Delete podman stop pihole podman rm pihole podman volume rm pihole-etc pihole-dnsmasq rm -rf ~/pihole podman rmi docker.io/pihole/pihole sudo sed -i \u0026#39;/net.ipv4.ip_unprivileged_port_start=53/d\u0026#39; /etc/sysctl.conf sudo sysctl -p sudo firewall-cmd --permanent --remove-service=dns sudo firewall-cmd --reload ","date":"7 August, 2025","id":9,"permalink":"/hidden/podman-pihole/","summary":"Pi-hole is a network-level ad blocker that acts as a DNS sinkhole. Here\u0026rsquo;s how to run it securely on your Fedora-based homelab using Podman and Traefik, with a clean and idempotent setup.","tags":"pihole install fedora","title":"Pihole - Installation on Podman"},{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity‚Äîand the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml Access apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool‚Äîit\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ‚îú‚îÄ‚îÄ apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here ‚îÇ ‚îú‚îÄ‚îÄ guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îÇ ‚îú‚îÄ‚îÄ kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îÇ ‚îî‚îÄ‚îÄ podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îú‚îÄ‚îÄ env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, ‚îÇ ‚îú‚îÄ‚îÄ k3s ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ platform ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ workloads ‚îÇ ‚îî‚îÄ‚îÄ openshift ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îú‚îÄ‚îÄ platform ‚îÇ ‚îî‚îÄ‚îÄ workloads ‚îú‚îÄ‚îÄ ingress.yaml # Ingress to access ArgoCD dashboard ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ root-app.yaml # Root ARGOCD application ‚îî‚îÄ‚îÄ values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ ‚Äì Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ ‚îî‚îÄ‚îÄ \u0026lt;app1\u0026gt;/ ‚îú‚îÄ‚îÄ base/ ‚îî‚îÄ‚îÄ overlays/ ‚îú‚îÄ‚îÄ \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV ‚îî‚îÄ‚îÄ \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ ‚Äì Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n‚îÄ‚îÄ env ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;env1-name\u0026gt; ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ \u0026#39;argocd-application-for-app1\u0026#39;.yaml ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ \u0026#39;argocd-application-for-app2\u0026#39;.yaml ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml ‚Äì The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":10,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity‚Äîand the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute ‚Äî ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , ‚Äî this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":11,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute ‚Äî ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":12,"permalink":"/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure üí° The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n‚úÖ Visual: Prompt with icons, colors, and context-aware sections\n‚úÖ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n‚úÖ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":13,"permalink":"/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You‚Äôve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":14,"permalink":"/posts/networks/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes ‚Äî ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80‚Äôs door anymore ‚Äî it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy‚Äôs new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy‚Äôs map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph‚Äôs Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity ‚Äî just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":15,"permalink":"/posts/networks/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ‚úÖ Works ‚ùå Fails with CSRF token error Safari ‚úÖ Works ‚ùå Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":16,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode ‚Äî K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":17,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":18,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Introduction The \u0026ldquo;Arr stack\u0026rdquo; is a popular set of applications for automating your media library. This blog post will walk you through configuring a basic Arr stack using Podman on your homelab. We\u0026rsquo;ll cover Jellyfin for media streaming, Jellyserr for integrating services, Prowlarr for managing indexers, and Sonarr and Radarr for automating TV and movie downloads. This setup provides a simple yet powerful way to manage your media. üöÄ\nConfiguring the Arr Stack Basic flow about how each service talks to each other\nJellyfin Add media and scan Enable trickplay image extraction Save trickplay images next to media Trickplay Image Interval = 300000 Only generate images from key frames Jellyserr Settings -\u0026gt; Jellyfin. Get API from Jellyfin (Advanced -\u0026gt; API Keys) - See pic js1 Settings -\u0026gt; Services -\u0026gt; Radarr. Get API from Radarr (Settings -\u0026gt; General) Settings -\u0026gt; Services -\u0026gt; Sonarr. Get API from Sonarr (Settings -\u0026gt; General) Make sure flare-bypasser container is up Prowler Add indexers -\u0026gt; YTS etc Settings -\u0026gt; Indexer -\u0026gt; FlareSolverr -\u0026gt; Setup flare-bypasser Settings -\u0026gt; Apps -\u0026gt; Applications -\u0026gt; Radarr Settings -\u0026gt; Apps -\u0026gt; Applications -\u0026gt; Sonarr Sonarr Select location for TvShows Settings -\u0026gt; Indexer -\u0026gt; Verify indexers (from Prowler) Settings -\u0026gt; Download Clients -\u0026gt; Transmission Radarr Select location for Movies Settings -\u0026gt; Indexer -\u0026gt; Verify indexers (from Prowler) Settings -\u0026gt; Download Clients -\u0026gt; Transmission Service Local IP Access Ingress Jellyfin http://192.168.50.200:8096 https://jellyfin.node1 Jellyseerr http://192.168.50.200:5055 https://jellyseerr.node1 Prowlarr http://192.168.50.200:9696 https://prowlarr.node1 Sonarr http://192.168.50.200:8989 https://sonarr.node1 Radarr http://192.168.50.200:7878 https://radarr.node1 Transmission http://192.168.50.200:9091 https://transmission.node1 flare-bypasser http://192.168.50.200:8191 https://flare-bypasser.node1 Jellyfin libraries - Add Movies Jellyfin libraries - Add Shows jellyseerr - Give Jellyfin URL jellyseerr - Sync libraries jellyseerr - Sync with radarr\nGet key from radarr configure radarr in jellyseerr jellyseerr - Sync with sonarr\nGet key from sonarr configure sonarr in jellyseerr jellyseerr - Import users from jellyfin Prowlarr - configure flaresolverr Prowlarr - add radarr Prowlarr - add sonarr Prowlarr - tags sonarr - just get HEVC Summary Congratulations! üéâ You\u0026rsquo;ve now configured a powerful and automated media stack. With this setup, you can add movies and TV shows from your phone or desktop, and the Arr stack will automatically find and download them, ready for you to watch on Jellyfin. This is a great way to streamline your homelab and take control of your media.\n","date":"8 August, 2025","id":19,"permalink":"/hidden/podman-arr-stack-configuration/","summary":"The \u0026ldquo;Arr stack\u0026rdquo; is a popular set of applications for automating your media library. This blog post will walk you through configuring a basic Arr stack using Podman on your homelab. We\u0026rsquo;ll cover Jellyfin for media streaming, Jellyserr for integrating services, Prowlarr for managing indexers, and Sonarr and Radarr for automating TV and movie downloads. This setup provides a simple yet powerful way to manage your media. üöÄ","tags":"arr-stack podman homelab","title":"Arr Stack - Configuration"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic ‚Äî all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM ‚Äî after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ‚úÖ Yes Setup bridge/qemu policies ‚úÖ Yes VM create/operate via libvirt ‚ùå No Use virt-manager GUI ‚ùå No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":20,"permalink":"/homelab/homeassistant-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic ‚Äî all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference ‚Äî opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":21,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub üëâ Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":22,"permalink":"/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog ‚Äî a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\nüîß Tech Walkthroughs ‚Äî Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. üåê Home Lab \u0026amp; Automation ‚Äî Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. üõ°Ô∏è Security \u0026amp; Best Practices ‚Äî Focus security, supply chain integrity, and observability. üì¶ Modern Ops ‚Äî GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you‚Äôre an engineer, architect, or open source enthusiast ‚Äî I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":23,"permalink":"/about/","summary":"Welcome to my blog ‚Äî a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"},{"content":"Introduction MKubevirt VM connect to the cluster CNI (Flannel, Calico, etc), which is perfect for pod-to-pod communication but limits VMs that need full LAN access. We will address that by attaching it to the bridge network on host.\nPart of series on Kubevirt Create a (homeassistant VM) containerdisk Setup Bridge Network for a Kubevirt VM 1. Prerequisites Before diving in, ensure you have:\nA K3s cluster with kubectl or oc access. Linux bridge configured on host (bridge0). jq installed (sudo dnf install jq or brew install jq). Basic knowledge of KubeVirt, CDI, and Kubernetes networking. HLD Home Network\n1.1 Router (192.168.50.1): Provides LAN IPs via DHCP.\n1.2 User Device: Laptop/Phone accessing VM or cluster services.\nKubernetes Host\n2.1 Physical NIC (enp3s0): Connects host to LAN.\n2.2 Linux Bridge (bridge0): Virtual switch connecting VMs to LAN.\n2.3 Flow: NIC ‚Üí Bridge ‚Üí VM ‚Üí LAN\nKubernetes Cluster\n3.1 Multus CNI: Manages VM network attachments.\n3.2 KubeVirt VM (Fedora): Attaches to bridge0 via homenet NAD.\nNetwork Flow\n4.1 VM gets LAN IP via DHCP.\n4.2 VM ‚Üî Linux Bridge ‚Üî Host NIC ‚Üî Router ‚Üî LAN\n4.3 VM can communicate directly with user devices.\nKey Points\n5.1 VM behaves like a normal LAN device.\n5.2 LAN devices can access VM directly.\n5.3 Multus + bridge allows bypassing pod network NAT.\ngraph TD %% Home Network subgraph \u0026#34;Home Network\u0026#34; Router[\u0026#34;Router / DHCP Server\u0026lt;br\u0026gt;IP: 192.168.50.1\u0026#34;] UserDevice[\u0026#34;Laptop / Phone\u0026#34;] end %% Kubernetes Host subgraph \u0026#34;Kubernetes Host\u0026#34; direction LR HostNIC[\u0026#34;Physical NIC: enp3s0\u0026#34;] LinuxBridge[\u0026#34;Linux Bridge: bridge0\u0026#34;] HostNIC --\u0026gt;|Port added to| LinuxBridge end %% Kubernetes Cluster subgraph \u0026#34;Kubernetes Cluster\u0026#34; Multus[\u0026#34;Multus CNI\u0026#34;] KubeVirtVM[\u0026#34;KubeVirt VM - Fedora\u0026#34;] end %% Network Flow Router --\u0026gt;|DHCP Lease / LAN Traffic| HostNIC LinuxBridge --\u0026gt;|Virtual Port| Multus Multus --\u0026gt;|Attaches VM to bridge - NAD homenet| KubeVirtVM KubeVirtVM --\u0026gt;|Traffic to/from LAN| LinuxBridge KubeVirtVM --\u0026gt;|Gets LAN IP| Router KubeVirtVM --\u0026gt;|Communicates with| UserDevice UserDevice --\u0026gt;|Connects to| KubeVirtVM %% Styles style Router fill:#fcf,stroke:#333,stroke-width:2px style UserDevice fill:#cff,stroke:#333,stroke-width:2px style HostNIC fill:#ffc,stroke:#333,stroke-width:2px style LinuxBridge fill:#bbf,stroke:#333,stroke-width:2px style Multus fill:#ff9,stroke:#333,stroke-width:2px style KubeVirtVM fill:#9f9,stroke:#333,stroke-width:2px 2. Why use a Bridge Network for KubeVirt VMs KubeVirt VMs normally connect to the cluster CNI (Flannel, Calico, etc), which is perfect for pod-to-pod communication but limits VMs that need:\nLAN visibility ‚Äì VM gets an IP on the same subnet as your home devices. Direct device access ‚Äì Talk to IoT devices, NAS, printers without NAT. Predictable IPs ‚Äì DHCP or static assignment for consistent network identity. Solution: Multus + bridge CNI. The VM attaches to bridge0 on the host, making it a full LAN citizen.\nProcess flow: graph TB A[\u0026#34;Create Linux Bridge on Host\u0026#34;] --\u0026gt; B[\u0026#34;Install KubeVirt \u0026amp; CDI\u0026#34;] B --\u0026gt; C[\u0026#34;Install Multus\u0026#34;] C --\u0026gt; D[\u0026#34;Create NetworkAttachmentDefinition\u0026#34;] D --\u0026gt; E[\u0026#34;Deploy VM with Network Attachment\u0026#34;] 3. Host Linux Bridge Setup (bridge0) with NetworkManager bridge0 is a virtual switch; the physical interface (enp3s0) connects it to your LAN. VMs attach to bridge0 via Multus.\nCLI Steps # 3.1 Create the bridge nmcli connection add type bridge con-name bridge0 ifname bridge0 stp no # 3.2 Add physical NIC to bridge nmcli connection add type ethernet con-name \u0026#34;Wired Connection\u0026#34; ifname enp3s0 master bridge0 # 3.3 Configure DHCP (or static if desired) nmcli connection modify bridge0 ipv4.method auto nmcli connection modify bridge0 ipv6.method auto # 3.4 Enable autoconnect nmcli connection modify bridge0 connection.autoconnect yes nmcli connection modify \u0026#34;Wired Connection\u0026#34; connection.autoconnect yes # 3.5 Bring up connections nmcli connection up bridge0 nmcli connection up \u0026#34;Wired Connection\u0026#34; # 3.6 Verify setup nmcli connection show nmcli device status ‚úÖ Result: bridge0 is live, VMs attached will get LAN IPs automatically.\nüí° Tip: Use nmcli on headless servers for fully automated bridge setup.\n4. Install KubeVirt Deploy the operator and KubeVirt CR to run VMs on Kubernetes.\nexport KUBEVIRT_VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name) echo $KUBEVIRT_VERSION kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-operator.yaml kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-cr.yaml kubectl get pods -n kubevirt ‚≠êÔ∏è Tip: Wait until all pods in kubevirt namespace are Running before continuing. 5. Install Containerized Data Importer (CDI) CDI handles VM image uploads and DataVolumes.\nexport CDI_VERSION=$(curl -s https://api.github.com/repos/kubevirt/containerized-data-importer/releases/latest | jq -r .tag_name) echo $CDI_VERSION kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml kubectl get pods -n cdi # kubectl -n cdi port-forward svc/cdi-uploadproxy 8443:443 6. Install Multus on K3s Multus enables multiple interfaces per VM/pod‚Äîessential for bridge networking.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: multus namespace: kube-system spec: repo: https://rke2-charts.rancher.io chart: rke2-multus targetNamespace: kube-system valuesContent: |- config: fullnameOverride: multus cni_conf: confDir: /var/lib/rancher/k3s/agent/etc/cni/net.d binDir: /var/lib/rancher/k3s/data/cni/ kubeconfig: /var/lib/rancher/k3s/agent/etc/cni/net.d/multus.d/multus.kubeconfig multusAutoconfigDir: /var/lib/rancher/k3s/agent/etc/cni/net.d manifests: dhcpDaemonSet: true EOF 7. Create DataVolume (Fedora 42) Create a persistent VM disk.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: annotations: cdi.kubevirt.io/storage.bind.immediate.requested: \u0026#34;\u0026#34; name: fedora-dv-42 spec: contentType: kubevirt source: http: url: \u0026#34;https://download.fedoraproject.org/pub/fedora/linux/releases/42/Cloud/x86_64/images/Fedora-Cloud-Base-Generic-42-1.1.x86_64.qcow2\u0026#34; storage: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi EOF 8. Network Attachment Definition Attach VM to bridge0. You can choose DHCP or Static IP.\nDHCP Static IP cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: homenet spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;bridge0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dhcp\u0026#34; } }\u0026#39; EOF cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: homenet spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;bridge0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;static\u0026#34;, \u0026#34;addresses\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;192.168.50.204/24\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;192.168.50.1\u0026#34; } ], \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;192.168.50.1\u0026#34; } ], \u0026#34;dns\u0026#34;: { \u0026#34;nameservers\u0026#34;: [\u0026#34;192.168.50.1\u0026#34;, \u0026#34;1.1.1.1\u0026#34;] } } }\u0026#39; EOF NAD diagram graph TD NAD_Choice{{\u0026#34;Network Attachment Definition\u0026lt;br\u0026gt;(homenet)\u0026#34;}} subgraph \u0026#34;Option 1: Dynamic IP\u0026#34; DHCP[\u0026#34;IPAM: DHCP\u0026#34;] DHCP_Config[(\u0026#34;ipam: { type: dhcp }\u0026#34;)] NAD_Choice --\u0026gt;|Selects| DHCP DHCP --\u0026gt;|Uses CNI Plugin| DHCP_Config end subgraph \u0026#34;Option 2: Static IP\u0026#34; Static[\u0026#34;IPAM: Static\u0026#34;] Static_Config[(\u0026#34;ipam: { type: static, addresses: [...] }\u0026#34;)] NAD_Choice --\u0026gt;|Selects| Static Static --\u0026gt;|Specifies configuration| Static_Config end style NAD_Choice fill:#fff,stroke:#333,stroke-width:2px 9. Create VirtualMachine Deploy Fedora VM with homenet interface.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: fedora-dv-bridge labels: kubevirt.io/os: linux spec: runStrategy: Always template: metadata: labels: kubevirt.io/domain: vm spec: domain: cpu: cores: 1 devices: disks: - name: disk0 disk: bus: virtio - name: cloudinitdisk cdrom: bus: sata readonly: true interfaces: - name: homenet bridge: {} model: virtio machine: type: q35 resources: requests: memory: 2048M networks: - name: homenet multus: networkName: homenet volumes: - name: disk0 persistentVolumeClaim: claimName: fedora-dv-42 - name: cloudinitdisk cloudInitNoCloud: userData: | #cloud-config hostname: fedora-dv-bridge ssh_pwauth: True password: fedora chpasswd: {expire: False} runcmd: - dnf install -y qemu-guest-agent cockpit - systemctl enable qemu-guest-agent - systemctl enable --now cockpit.socket - systemctl start qemu-guest-agent cockpit EOF üéâ Congratulations! Your KubeVirt VM now has a LAN IP, can SSH in, use Cockpit, and interact with home network devices seamlessly.\n","date":"23 August, 2025","id":0,"permalink":"/posts/kubevirt_bridge/","summary":"MKubevirt VM connect to the cluster CNI (Flannel, Calico, etc), which is perfect for pod-to-pod communication but limits VMs that need full LAN access. We will address that by attaching it to the bridge network on host.","tags":"kubernetes multus kubevirt networking","title":"Setup Bridge Network for a Kubevirt VM"},{"content":"1. Introduction: What is Home Assistant? Home Assistant is a powerful, open-source home automation platform that puts local control and privacy first. It can be run on various devices, from single-board computers like the Raspberry Pi to virtual machines and containers. It\u0026rsquo;s a great tool for anyone looking to automate their home without relying on big tech companies, giving you complete control over your smart devices.\n2. Installation Methods Home Assistant offers a few different ways to get started, but two of the most common are:\nHome Assistant Operating System (HaOS): This is the recommended and most popular installation method. HaOS is a lightweight, embedded operating system designed specifically to run Home Assistant and its ecosystem. It\u0026rsquo;s easy to install on a dedicated device like a Home Assistant Green, a Raspberry Pi, or a virtual machine. This method gives you access to the full Home Assistant experience, including the convenience of add-ons, which are pre-packaged applications that extend its functionality.\nHome Assistant Container: This method is for more advanced users who want to run Home Assistant within a container environment (like Docker or Podman). You\u0026rsquo;re responsible for managing the underlying operating system and the container yourself. While this offers flexibility, it comes with a trade-off: you don\u0026rsquo;t get access to the official add-ons.\n3. HaOS on KubeVirt: The Best of Both Worlds If you\u0026rsquo;re already running a Kubernetes homelab and prefer the flexibility of containerized applications but still want the convenience and features of the full HaOS experience, KubeVirt is the perfect solution. KubeVirt is a Kubernetes add-on that lets you run traditional virtual machines (VMs) alongside your container workloads. This means you can run the full HaOS as a VM right inside your Kubernetes cluster, giving you a powerful, unified platform for both your containers and your home automation.\n4. Install KubeVirt and CDI The first step is to get KubeVirt and its dependencies up and running in your cluster. We will also install the Containerized Data Importer (CDI), which is essential for importing the virtual machine disk image into Kubernetes.\nInstall Kubevirt \u0026amp; CDI Install Kubevirt # Install KuberVirt export KUBEVIRT_VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name) echo $KUBEVIRT_VERSION kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-operator.yaml kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-cr.yaml kubectl get pods -n kubevirt # Install Containerized Data Importer (CDI) export CDI_VERSION=$(curl -s https://api.github.com/repos/kubevirt/containerized-data-importer/releases/latest | jq -r .tag_name) echo $CDI_VERSION kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml kubectl get pods -n cdi # kubectl -n cdi port-forward svc/cdi-uploadproxy 8443:443 ‚≠êÔ∏è NOTE: Steps 5-7 are about creating your own HaOS containerdisks. If you are happy to use my image based on HomeAssistant 16.1 then go to step-8 directly.\nIn case you are interested. Here are the official containerdisk images for various OS\n5. Download the HaOS image # Download the HaOS qcow2 image wget https://github.com/home-assistant/operating-system/releases/download/16.1/haos_ova-16.1.qcow2.xz # unzip xz -dk haos_ova-16.1.qcow2.xz 6. Convert the image Convert the image into a foramt that is understood by Kubevirt\nsudo dnf install podman libguestfs-tools guestfs-tools -y # machine-id gave error # virt-sysprep -a haos_ova-16.1.qcow2 --operations machine-id,bash-history,logfiles,tmp-files,net-hostname,net-hwaddr Image_name=haos_ova-16.1.qcow2 # Make Golden Image by removing unique identifiers and temporary files from the image. virt-sysprep -a $Image_name --operations bash-history,logfiles,tmp-files,net-hostname,net-hwaddr # Conpress image qemu-img convert -c -O qcow2 $Image_name \u0026#34;$Image_name-containerimage.qcow2\u0026#34; 7. Build the image and push to your repo # Create your Containerfile tee Containerfile \u0026gt; /dev/null \u0026lt;\u0026lt;EOL FROM kubevirt/container-disk-v1alpha ADD $Image_name-containerimage.qcow2 /disk/ EOL # Login to your repo podman login quay.io -u arslankhanali -p \u0026lt;\u0026gt; # Make sure `homeassistant` repo exists and it is public # Buld the image podman build -t quay.io/arslankhanali/homeassistant:v1 . # Push the image podman push quay.io/arslankhanali/homeassistant:v1 8. Deploy the VM on Kubevirt I will deploy in the namespace vm\nkubectl create ns vm kubectl config set-context --current --namespace=vm cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: name: haos namespace: vm # \u0026lt;-- Change as per need spec: domain: resources: requests: memory: 2048Mi cpu: 1 limits: memory: 4096Mi cpu: 2 devices: disks: - name: containerdisk disk: bus: virtio rng: {} firmware: bootloader: efi: secureBoot: false # ‚úÖ disable SecureBoot to avoid SMM requirement terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: quay.io/arslankhanali/homeassistant:v1 # \u0026lt;-- Change as per need --- apiVersion: v1 kind: Service metadata: name: haos namespace: vm spec: type: NodePort selector: vmi: haos ports: - name: haos-ui port: 8123 protocol: TCP targetPort: 8123 nodePort: 30003 # \u0026lt;-- Change as per need - Will be random it not set EOF 9. Console and port forward virtctl console haos # Password is ususlly set in cloud init inside vmi yaml 10. Console and port forward virtctl port-forward vmi/haos 8123:8123 curl -kI http://localhost:8123/onboarding.html curl -kI http://192.168.50.200:30003 Delete oc delete -f vm-haos.yaml oc delete vmi haos oc delete svc haos References:\nhttps://github.com/ormergi/vm-image-builder/tree/main?tab=readme-ov-file ","date":"17 August, 2025","id":1,"permalink":"/posts/featured/haos-on-kubevirt/","summary":"Home Assistant is a powerful, open-source home automation platform that puts local control and privacy first. It can be run on various devices, from single-board computers like the Raspberry Pi to virtual machines and containers. It\u0026rsquo;s a great tool for anyone looking to automate their home without relying on big tech companies, giving you complete control over your smart devices.","tags":"kubernetes podman","title":"Home Assistant VM on Kubevirt"},{"content":"0. General logins I used with my K3s email. : neogeo@gmail.com username. : neo password 1 : geo password 2 : Admin@123 1. Login to K3s scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config sed -i \u0026#39;\u0026#39; \u0026#39;s|server: https://127\\.0\\.0\\.1:6443|server: https://192.168.50.200:6443|\u0026#39; ~/k3s-config export KUBECONFIG=~/k3s-config oc get all -A 2. Argo oc get secret -n argocd argocd-initial-admin-secret -o jsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d rfgyTgvrj3RzWee1 3. Headlamp oc get secret headlamp-admin-secret-token -n headlamp -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Bearer token:\neyJhbGciOiJSUzI1NiIsImtpZCI6Ik9QLUlRd0RDOEdaeXFyZ3g0dnVkT1V3RzNIN0oweS1SODVaVDhza0d4RkEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJoZWFkbGFtcCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJoZWFkbGFtcC1hZG1pbi1zZWNyZXQtdG9rZW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiaGVhZGxhbXAtYWRtaW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2YWIyMmQ2ZC1kZmVmLTQ5NzAtODI3MS1jNWU1NjA3NjYyMTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6aGVhZGxhbXA6aGVhZGxhbXAtYWRtaW4ifQ.eZXZsTH_foaS2VKJuwz8Bj8U337xX6v7KRJCb9ZcQ9TYhbd2CeW9Q9VPIEv6lAEJYbBf_C-RTxnLsszS_lNHPk0sOnTMt6v6bb7qHKP-2uoYrLcGV0zBg-b1QLN2-fcYJEXFE_D_qbkLrclHxzmG-m4xtf2CFJPz-z4PxvTGYdsP1QONTjcdXqdEOSeFIbFSpvtgQ2NRHMOmIV4ANc8jccb_AEhGiFLnSW_6G1TXkgNFXhnUpwR4o42nV3V1_9W-qLUoSwqy7Owh6lfl3SErxy8_IaHf8JMJykhlEDhZQGzXcCIc3tQPtoVsJFwYWfMWmflh16aUmJiUmKKfDrmnqA 4. AWX oc get secret -n awx-operator awx-demo-admin-password -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d wDhnfcMYlNzrGqDkZVUFmq03tQxcpWLt 5. Gitea neo Admin@123 6. Grafana oc get secret -n grafana grafana -o jsonpath=\u0026#39;{.data.admin-password}\u0026#39; | base64 -d T6zGt8FwF2cKUYa9o88uqCOYxE0DE6a2Gpc3qosm 6. Harbor oc get secrets -n harbor harbor-core -o jsonpath=\u0026#39;{.data.HARBOR_ADMIN_PASSWORD}\u0026#39; | base64 -d Admin@123 7. 8. 9. 10. ","date":"15 August, 2025","id":2,"permalink":"/hidden/k3s-logins/","summary":"Bearer token:","tags":"kubernetes podman","title":"K3s Logins"},{"content":"Notes REMEMBER to run kubectl and kustomize apply commands from the correct Namespace/Project! Helm will fuck up otherwise Resources Keep requests.cpu and requests.mem very low. So not a lot of resource is dedicated to the workload. No point giving 512Mi to a workload that is only using 100Mi. Do not put any limits.cpu and limits.mem. Pods crash saying OOM (out of memory) i.e. prometheus Only set it if you fear that a spike from this workload will overwhelm the node. Replicas Keep them to 1 ERROR: _non_namespaceable_\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;olm\u0026quot;}}] error: no objects passed to apply Remove namespace when resource has its own namespace e.g. olm Always use ClusterIP and expose via Ingress. LoadBalancer and NodePort will clash with host ports service: type: ClusterIP When you run the kustomize build or helm commands, it will download the chart in the directory. You can get the default values.yaml file in it. It is always a good idea to search for terms like enabled replicas resources ingress There should be only one storage class local-path is faster - so keep it default - also you might have to delete longhorn , it would mess up everything Use longhorn when a pod needs RWX storage # Check oc get storageclass # Remove default oc annotate storageclass longhorn storageclass.kubernetes.io/is-default-class- # Make default oc annotate storageclass local-path storageclass.kubernetes.io/is-default-class=true Cert-manager uses a webhook to validate resources like Certificate and Issuer before they are stored in etcd. Remove it in testing/dev env oc patch validatingwebhookconfiguration cert-manager-webhook --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/webhooks/0/failurePolicy\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;Ignore\u0026quot;}]' Homepage Annotations annotations: gethomepage.dev/enabled: \u0026#34;true\u0026#34; gethomepage.dev/name: \u0026lt;\u0026gt; gethomepage.dev/description: \u0026lt;\u0026gt; gethomepage.dev/group: Cluster Management | Developer Tools | Storage | Security | Monitoring | Server gethomepage.dev/icon: \u0026lt;\u0026gt;.png # \u0026lt;-- https://github.com/homarr-labs/dashboard-icons/tree/main/png # e.g. \u0026#34;app.kubernetes.io/name: longhorn\u0026#34; , This provides running status in homepage UI gethomepage.dev/pod-selector: |- app.kubernetes.io/name in ( longhorn ) gethomepage.dev/weight: \u0026#39;0\u0026#39; 1. Create ns=grafana oc create ns $ns oc project $ns # For Kustomize with Helm kustomize build ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ --enable-helm | oc apply -f - # For Kustomize oc apply -k ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ # For a simple Yaml manifest oc apply -f ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ingress.yaml 2. Verify ns=grafana oc get all -A oc get all curl -Ik --resolve $ns.node1:443:192.168.50.200 https://$ns.node1 # after /etc/hosts file is updated curl -Ik https://$ns.node1 # Node: See all requests and limits for cpu and mem oc describe nodes node1 2.1 Logs ns=grafana oc project $ns oc logs pods/ oc describe pods/ 2.2 Use --previous flag for why the pods/container crashed oc logs pods/prometheus-server-7ffd965767-jhdpc -c prometheus-server --previous\n3. Stop oc scale deployment --all --replicas=0 -n grafana oc scale deployment --all --replicas=1 -n grafana 4. Delete all ns=grafana oc project $ns kustomize build ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ --enable-helm | oc delete -f - oc delete --all all -n $ns oc delete --all pods -n $ns --force --grace-period=0 oc delete --all pvc -n $ns oc delete ns $ns 4.1. Delete pods Use --grace-period=0 --force\noc delete --all pods --force --grace-period=0 -n $ns 4.2. Delete: Kube proxy method e.g. Delete PVC\nCreate namespace for it again if deleted delete any webhooks start oc proxy in another terminal Run below: remember to change and oc proxy # Delete Namespace ns=cattle-system curl -X PATCH http://127.0.0.1:8001/api/v1/namespaces/$ns \\ -H \u0026#34;Content-Type: application/json-patch+json\u0026#34; \\ --data \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/finalizers\u0026#34;}]\u0026#39; 4.3. Delete: Remove Finalizer e.g. Delete namespace\nns=grafana oc get ns $ns -o json | jq \u0026#39;.spec.finalizers=[]\u0026#39; | oc replace --raw /api/v1/namespaces/$ns/finalize -f - 5. Exec in container in a pod oc exec -it \u0026lt;pod\u0026gt; -c \u0026lt;container\u0026gt; -- sh 6. Ingresss K3s is bound to 192.168.50.200 IP - and podman uses 192.168.50.100 IP for Traefik.\nkubectl logs -n kube-system deploy/traefik # Get Traefik version kubectl -n kube-system get deploy traefik -o jsonpath=\u0026#39;{.spec.template.spec.containers[0].image}\u0026#39; oc -n kube-system get svc traefik # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # traefik LoadBalancer 10.43.148.201 192.168.50.200 80:30854/TCP,443:31067/TCP 3d1h # No other svc will have EXTERNAL-IP. because they are only exposed via Ingress 6.1. DNS nslookup jellyseerr.ak Server: 192.168.50.100 Address: 192.168.50.100#53 Name: jellyseerr.ak Address: 192.168.50.100 nslookup jellyseerr.node1 Server: 192.168.50.100 Address: 192.168.50.100#53 Name: jellyseerr.node1 Address: 192.168.50.200 7. ERROR: Failed to allocate directory watch: Too many open files # Check Limits ssh node1 ulimit -n 1024 cat /proc/$(pgrep -f kubelet)/limits | grep \u0026#34;open files\u0026#34; Max open files 1048576 1048576 files cat /proc/sys/fs/inotify/max_user_watches cat /proc/sys/fs/inotify/max_user_instances 122145 128 # Increase limits: Temporarily sudo sysctl -w fs.inotify.max_user_watches=524288 sudo sysctl -w fs.inotify.max_user_instances=1024 # Restart all sudo systemctl restart k3s oc delete pod -n kubevirt -l kubevirt.io=virt-handler # Increase limits: Permanently (survives reboot) echo \u0026#34;fs.inotify.max_user_watches=524288\u0026#34; | sudo tee -a /etc/sysctl.conf echo \u0026#34;fs.inotify.max_user_instances=1024\u0026#34; | sudo tee -a /etc/sysctl.conf sudo sysctl -p 8. Check version of Helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm search repo prometheus-community/prometheus --versions helm search repo prometheus-community/prometheus --versions | grep \u0026#39;prometheus-community/prometheus \u0026#39; ","date":"15 August, 2025","id":3,"permalink":"/hidden/k3s/","summary":"oc logs pods/prometheus-server-7ffd965767-jhdpc -c prometheus-server --previous","tags":"kubernetes podman","title":"K3s"},{"content":"Introduction Welcome to another installment in our AI series! Today, we\u0026rsquo;re going to set up Open WebUI, a powerful, self-hosted web interface for interacting with various large language models (LLMs). This tool provides a beautiful user experience similar to ChatGPT but on your own terms. We\u0026rsquo;ll be using Podman to containerize Open WebUI, making it a breeze to manage.\nWhy Open WebUI? Open WebUI acts as a central hub for your LLMs. It can connect to local models running on Ollama or LM Studio, as well as remote services like MaaS (Model as a Service). This flexibility allows you to experiment with different models and backends all from a single, consistent interface.\nStep 1: Pulling the Open WebUI Image First, we need to pull the Open WebUI container image from its public registry. Podman makes this simple and efficient.\n# This command fetches the latest main version of the Open WebUI image from GitHub\u0026#39;s container registry. podman pull ghcr.io/open-webui/open-webui:main Step 2: Running the Container Now, let\u0026rsquo;s run the container with all the necessary configurations. We\u0026rsquo;ll map a port, set environment variables, and create a persistent volume for data.\npodman run -d \\ --name open-webui \\ -p 3001:8080 \\ -e OLLAMA_BASE_URL=http://192.168.50.50:11434 \\ # If you have Ollama -e OPENAI_API_KEY=dummykey \\ -v open-webui:/app/backend/data:z \\ --restart=always \\ ghcr.io/open-webui/open-webui:main Step 3: Accessing Open WebUI Once the container is running, you can access the web interface by navigating to your host machine\u0026rsquo;s IP address and the mapped port.\nhttp://192.168.50.200:3001\nInitial Setup The first time you visit the page, you\u0026rsquo;ll be prompted to create a user account. After creating your account, you can log in and access the admin settings to configure your LLM connections.\nStep 4: Configuring Connections In the Open WebUI admin panel, you can add various connections to different LLM services.\nAdd connections: Settings -\u0026gt; Admin Panel -\u0026gt; Conenctions http://192.168.50.200:3001/admin/settings/connections e.g. Notice URL format\nOLLAMA: http://192.168.50.50:11434 # \u0026lt; \u0026mdash; From Ollama CLI OPENAI: http://192.168.50.50:1234/v1 # \u0026lt; \u0026mdash; From LMStudio\n‚ö†Ô∏è WARNING: Remember to use http or https as appropriate for your connection. Always use the correct port and make sure your firewall is configured to allow traffic on these ports.\nYou now have a fully functional Open WebUI instance running on your server, giving you a powerful tool to manage and interact with all your LLMs in one place!\n","date":"8 August, 2025","id":4,"permalink":"/posts/ai/openwebui-on-podman/","summary":"Welcome to another installment in our AI series! Today, we\u0026rsquo;re going to set up Open WebUI, a powerful, self-hosted web interface for interacting with various large language models (LLMs). This tool provides a beautiful user experience similar to ChatGPT but on your own terms. We\u0026rsquo;ll be using Podman to containerize Open WebUI, making it a breeze to manage.","tags":"openwebui podman ai ollama llm","title":"Open WebUI with Podman"},{"content":"Introduction Today, we\u0026rsquo;re going to deploy Dify, an AI application development platform, using Podman and podman-compose. Dify provides a powerful, visual way to build and manage AI applications, and deploying it on your local fedora server gives you full control.\nPrerequisites Before we start, make sure you have Podman and podman-compose installed. If you haven\u0026rsquo;t, you can refer to my previous blog post on how to set up Podman.\nüü¢ NOTE: This guide assumes you are running on a RHEL-based system with SELinux enabled, which is a common setup for fedora environments. The steps for SELinux are crucial for a smooth deployment.\nStep 1: Clone the Dify Repository First, we need to clone the Dify repository from GitHub. This will give us the necessary configuration files to deploy the application.\ngit clone [https://github.com/langgenius/dify.git](https://github.com/langgenius/dify.git) cd ~/dify/docker cp .env.example .env After cloning, we move into the docker directory and copy the example environment file to .env. This file contains the configuration variables for our Dify deployment.\nStep 2: Configure SELinux For systems with SELinux enabled, you\u0026rsquo;ll need to adjust the security context of the Dify volumes. This step prevents permission errors when Podman tries to access the files. By setting the context correctly, we avoid having to use the :z flag in our compose file.\nchcon -Rt container_file_t /home/neo/dify/docker/ ls -Z Step 3: Modify the docker-compose.yaml We need to make a couple of small but important changes to the docker-compose.yaml file to ensure Dify runs smoothly with Podman.\nvim docker-compose.yaml Change 1: Add Default Networks Add a default network to the end of the file. This ensures all services that don\u0026rsquo;t have an explicit network defined can communicate.\n# Go to end of file `shift+g` networks: ssrf_proxy_network: driver: bridge internal: true milvus: driver: bridge opensearch-net: driver: bridge internal: true default: driver: bridge Change 2: Ports Adjust the NGINX ports to avoid conflicts on your host. We\u0026rsquo;ll change the host ports from 80 and 443 to 980 and 9443 respectively. Remember to also change the corresponding variables in the .env file you copied earlier.\n# Search `/NGINX_PORT` ports: - \u0026#39;980:${NGINX_PORT:-80}\u0026#39; - \u0026#39;9443:${NGINX_SSL_PORT:-443}\u0026#39; Might also have to change nginx port in .env file 980 and 9443\nStep 4: Deploy and Verify Now we\u0026rsquo;re ready to deploy Dify using podman-compose.\nStart the containers\npodman-compose up -d podman pod logs pod_docker -f Step 5: Open Ports sudo firewall-cmd --add-port=980/tcp --permanent sudo firewall-cmd --reload Step 6: Test and Access podman pod logs pod_docker -f curl http://localhost:980/apps curl http://192.168.50.200:980/apps That\u0026rsquo;s it! You now have Dify running on your server. You can start building AI applications and experimenting with the platform.\n","date":"8 August, 2025","id":5,"permalink":"/posts/ai/dify-on-podman/","summary":"Today, we\u0026rsquo;re going to deploy Dify, an AI application development platform, using Podman and podman-compose. Dify provides a powerful, visual way to build and manage AI applications, and deploying it on your local fedora server gives you full control.","tags":"dify podman ai langgenius docker-compose","title":"Dify: A Podman Deployment Guide"},{"content":"Part of series on Arr-stack:\nArr-Stack Installation Arr-Stack Configuration Introduction The \u0026ldquo;arr stack\u0026rdquo; is a popular collection of applications used for managing media libraries. By running these applications as rootless containers with Podman, you can enhance security by preventing the containers from having root privileges on the host machine.\nThis guide will walk you through setting up the arr stack, including Jellyfin, Jellyseerr, Prowlarr, Sonarr, Radarr, and Transmission as rootless containers. We will cover directory preparation, mounting an external hard drive, and configuring the services using a podman-compose.yaml file.\nWhat is the \u0026ldquo;arr stack\u0026rdquo;? üóÉÔ∏è ‚ö†Ô∏è Check legal obligations where you live The arr stack is a suite of applications that work together to automate the process of finding, downloading, and organizing movies, TV shows, and other media.\nApplication Purpose Jellyfin A free software media system that lets you control the management and streaming of your media. Jellyseerr A request management and media discovery tool for Jellyfin and other \u0026lsquo;arr\u0026rsquo; apps. Prowlarr An indexer manager for the other \u0026lsquo;arr\u0026rsquo; apps, providing a centralized way to manage your indexers (sources for media). Sonarr Manages and automates the downloading of TV shows. Radarr Manages and automates the downloading of movies. Transmission A lightweight BitTorrent client used for downloading the media files. Prerequisites Before you start, make sure you have Podman and podman-compose installed. For Podman, you can often find it in your distribution\u0026rsquo;s package manager. For podman-compose, it\u0026rsquo;s a Python script that you can install with pip.\n# Example for a Fedora-based system sudo dnf install podman podman-docker # install podman-compose pip install podman-compose Setup Directories and External Drive First, we need to create the necessary directory structure for our applications and media files. This setup ensures that all your data is organized and easily accessible to the containers.\n# Set up a base directory for the arr stack dir=~/arr-stack mkdir -p $dir cd $dir # Create subdirectories for media and application downloads mkdir -p $dir/tvshows mkdir -p $dir/movies mkdir -p $dir/books mkdir -p $dir/transmission/downloads/complete/radarr mkdir -p $dir/transmission/downloads/complete/tv-sonarr mkdir -p $dir/transmission/downloads/incomplete/ mkdir -p $dir/transmission/watch mkdir -p $dir/prowler Mount an External Hard Drive If you have a large media library, it\u0026rsquo;s best to store it on an external hard drive. This section shows you how to mount the drive manually or automatically.\nManual Mount To manually mount your external drive, first find its device name and then mount it with the correct permissions. The uid=1000 and gid=1000 options ensure the drive is owned by your user, which is crucial for rootless containers. The context option is important for SELinux.\n# First, create a mount point mkdir -p $dir/external-drive # List block devices to find your external drive (e.g., /dev/sda1) lsblk # NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS # sda 8:0 0 1.8T 0 disk # ‚îî‚îÄsda1 8:1 0 1.8T 0 part ~/arr-stack/external-drive # Manually mount the drive. Replace /dev/sda1 with your device. sudo mount -t exfat -o users,noexec,uid=1000,gid=1000,context=\u0026#34;system_u:object_r:svirt_sandbox_file_t:s0\u0026#34; /dev/sda1 ~/arr-stack/external-drive Automatic Mount with /etc/fstab To have your drive automatically mount at startup, you need to add an entry to /etc/fstab.\nFind the UUID: The UUID is a unique identifier for your drive. # Find the UUID of your external drive sudo blkid /dev/sda1 Add to /etc/fstab: Append a new line to /etc/fstab using tee -a to avoid overwriting the file. # Path to your UUID and path echo \u0026#39;UUID=ECA3-DE06 /home/neo/arr-stack/external-drive exfat rw,users,noexec,nofail,async,auto,uid=1000,gid=1000,umask=0022,context=system_u:object_r:svirt_sandbox_file_t:s0 0 0\u0026#39; | sudo tee -a /etc/fstab Reload and Mount: Apply the new fstab entry. systemctl daemon-reload sudo mount -a ls -la arr-stack Check permission: Make sure your user (non-root) owns the folder ls -la ~/arr-stack You can verify the mount by running ls -la ~/arr-stack/external-drive.\nNote: The umount command is used to unmount the drive.\nsudo umount ~/arr-stack/external-drive The podman-compose.yaml File This file defines all the services in your arr stack. Each service is a container with specific configurations like image, port mappings, and volume mounts. A crucial aspect here is the volume mapping (- /path/on/host:/path/in/container:z). The :z option is essential for Podman to handle SELinux permissions correctly in a rootless environment.\nLet\u0026rsquo;s briefly explain some key configurations:\nimage: Specifies the container image to use. container_name: Sets a friendly name for the container. environment: Defines environment variables, such as PUID and PGID for user and group IDs, and TZ for the timezone. volumes: Connects host directories to container directories. This is how the applications can access your media files. ports: Maps container ports to host ports, allowing you to access the web UIs. labels: Used for services like Traefik to automatically configure reverse proxying and SSL/TLS. Some services, like Sonarr, Radarr, and Transmission, are run with PUID=0 and PGID=0 inside the container. This is a common practice for these specific images to avoid permission issues when writing to the /downloads directory, which is shared among them. The rootless Podman environment handles this by mapping the container\u0026rsquo;s root user (UID 0) to your host user (UID 1000).\nYou can ignore traefik section\ntee ~/arr-stack/podman-compose.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#34;3\u0026#34; services: # https://docs.linuxserver.io/images/docker-jellyfin/ jellyfin: image: docker.io/jellyfin/jellyfin:latest container_name: jellyfin environment: - TZ=Australia/Sydney volumes: - jellyfin-config:/config:z - jellyfin-cache:/cache:z - ~/arr-stack/external-drive:/media_external-drive:z - ~/arr-stack/movies:/media_movies:z - ~/arr-stack/tvshows:/media_tvshows:z - ~/arr-stack/prowler:/media_prowler:z - ~/arr-stack/transmission:/media_transmission:z ports: - 8096:8096 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.rule=Host(`jellyfin.ak`)\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.jellyfin.loadbalancer.server.port=8096\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.tls=true\u0026#34; # https://docs.jellyseerr.dev/getting-started/docker?docker-methods=docker-compose jellyseerr: image: docker.io/fallenbagel/jellyseerr:latest container_name: jellyseerr environment: - PUID=1000 - PGID=1000 - LOG_LEVEL=debug - TZ=Australia/Sydney ports: - 5055:5055 volumes: - jellyseerr-config:/app/config:z restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.rule=Host(`jellyseerr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.jellyseerr.loadbalancer.server.port=5055\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-prowlarr/#application-setup prowlarr: image: lscr.io/linuxserver/prowlarr:latest container_name: prowlarr environment: - PUID=1000 - PGID=1000 - TZ=Australia/Sydney volumes: - prowlarr-config:/config:z - ~/arr-stack/prowler/downloads:/downloads:z ports: - 9696:9696 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.rule=Host(`prowlarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.prowlarr.loadbalancer.server.port=9696\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-sonarr/ sonarr: image: lscr.io/linuxserver/sonarr:latest container_name: sonarr environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - sonarr-config:/config:z - ~/arr-stack/tvshows:/tvshows:z - ~/arr-stack/transmission/downloads:/downloads:z ports: - 8989:8989 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.sonarr.rule=Host(`sonarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.sonarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.sonarr.loadbalancer.server.port=8989\u0026#34; - \u0026#34;traefik.http.routers.sonarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-radarr/ radarr: image: lscr.io/linuxserver/radarr:latest container_name: radarr environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - radarr-config:/config:z - ~/arr-stack/movies:/movies:z - ~/arr-stack/transmission/downloads:/downloads:z ports: - 7878:7878 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.radarr.rule=Host(`radarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.radarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.radarr.loadbalancer.server.port=7878\u0026#34; - \u0026#34;traefik.http.routers.radarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-transmission/ transmission: image: lscr.io/linuxserver/transmission:latest container_name: transmission environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - transmission-config:/config:z - ~/arr-stack/transmission/downloads:/downloads:z - ~/arr-stack/transmission/watch:/watch:z ports: - 9091:9091 - 51413:51413 - 51413:51413/udp restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.transmission.rule=Host(`transmission.ak`)\u0026#34; - \u0026#34;traefik.http.routers.transmission.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.transmission.loadbalancer.server.port=9091\u0026#34; - \u0026#34;traefik.http.routers.transmission.tls=true\u0026#34; # It always run as UID=1111(flare_bypasser) # https://github.com/yoori/flare-bypasser flare-bypasser: image: ghcr.io/yoori/flare-bypasser:latest container_name: flare-bypasser environment: - PUID=1000 - PGID=1000 - TZ=Australia/Sydney volumes: - flare-bypasser-config:/config:z ports: - 8191:8080 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.rule=Host(`flare-bypasser.ak`)\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.flare-bypasser.loadbalancer.server.port=8191\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.tls=true\u0026#34; volumes: jellyfin-config: jellyfin-cache: sonarr-config: radarr-config: jellyseerr-config: prowlarr-config: transmission-config: flare-bypasser-config: EOL Deploying the Stack Once the podman-compose.yaml file is created, you can deploy the entire stack with a single command.\n# Make sure you\u0026#39;re in the ~/arr-stack directory cd ~/arr-stack # Deploy the services in detached mode podman-compose -f podman-compose.yaml up -d --force-recreate The --force-recreate flag ensures that if any configuration changes have been made, the containers will be rebuilt from scratch.\nTo check if the containers are running and to verify user permissions, you can use the following commands:\npodman ps # To check the user inside a container podman exec -it sonarr whoami This will show you the user that the container is running as, which should be abc (uid 1000) for most services, or root (uid 0) for the PUID=0 services, which is correctly mapped to your user.\nService Address Jellyfin http://192.168.50.200:8096 Jellyserr http://192.168.50.200:5055 Prowlarr http://192.168.50.200:9696 Sonarr http://192.168.50.200:8989 Radarr http://192.168.50.200:7878 Transmission http://192.168.50.200:9091 flare-bypasser http://192.168.50.200:8191 Enable Auto start # Remove old service files - That automatically start containers containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload # Create new service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload Update all cd ~/arr-stack echo \u0026#34;Pulling all images used by running containers...\u0026#34; for img in $(podman ps --format \u0026#34;{{.Image}}\u0026#34; | sort -u); do echo \u0026#34;Pulling: $img\u0026#34; podman pull \u0026#34;$img\u0026#34; done # Remove old service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload # Re-generate and enable new service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for name in \u0026#34;${containers[@]}\u0026#34;; do # This command generates a new systemd service file for the container podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files # This moves the newly generated service file to the correct systemd user directory mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ # Enable and start the new service systemctl --user enable \u0026#34;container-$name.service\u0026#34; systemctl --user start \u0026#34;container-$name.service\u0026#34; done # Reload the systemd manager configuration systemctl --user daemon-reload Remove all cd ~/arr-stack podman-compose down -v podman image prune containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload sudo umount ~/arr-stack/external-drive sudo nano /etc/fstab systemctl daemon-reload rm -rf ~/arr-stack ","date":"8 August, 2025","id":6,"permalink":"/hidden/podman-arr-stack/","summary":"Part of series on Arr-stack:","tags":"arr-stack podman homelab","title":"Arr stack - Installation"},{"content":"1. Introduction Podman is a daemonless container engine for running OCI containers.\nIt can run as root (rootful) or as a non-root user (rootless), each with different privileges and security implications.\nFeature Rootful Podman (Run as root) Rootless Podman (Run as non-root) Privileges Full root privileges on host system (sudo podman ...). Limited to user‚Äôs privileges (podman ...). Security Compromised container could gain root on host. Limited damage ‚Äî container root maps to non-root UID on host. Networking Can bind to privileged ports (\u0026lt;1024) directly. Uses slirp4netns, cannot bind \u0026lt;1024 without extra config. User ID Mapping Container root = Host root. Container root maps to non-root UID. 2. Running Rootless Do not use sudo to start containers.\nEnable lingering for your user:\nFor a rootless container to keep running after you\u0026rsquo;ve logged out, the Podman process itself needs to be managed by a system that persists. By enabling lingering, you allow the user\u0026rsquo;s systemd instance to continue running, which in turn can manage and keep Podman-related services alive.\nsudo loginctl enable-linger \u0026#34;$USER\u0026#34; Enable Podman socket for your user:\nThis command sets up and starts the Podman API socket for the current user.\nsystemctl --user enable --now podman.socket This creates a Podman socket at:\n/run/user/\u0026lt;UID\u0026gt;/podman/podman.sock (e.g., /run/user/1000/podman/podman.sock)\n3. Volume Mounting Default volume paths:\n# Rootful /var/lib/containers/storage/volumes/ # Rootless $HOME/.local/share/containers/storage/volumes/ 4. User Mapping Details Keeep - PUID=0 \u0026amp; - PGID=0 for containers in compose file. They will run as 0 inside container but 1000 on the host. i.e. rootless on host.\nüü¢ Make sure to run PODMAN without sudo Check UID mapping:\ncat /proc/$(pgrep -u \u0026#34;$USER\u0026#34; podman | head -n 1)/uid_map Example output:\nContainer UID Host UID Range Notes 0 1000 1 Container UID 0 is mapped to UID 1000 on host 1 524288 65536 Container UID 1 is mapped to UID 524288 on host 1000 525287 65536 Container UID 1000 is mapped to UID 525287 on host (524,288 + 999 = 525,287) 5. Auto restart: Systemd Integration (Rootless) Rootless service files live in:\n$HOME/.config/systemd/user\nGenerate and enable container services:\nmkdir -p ~/.config/systemd/user/ containers=($(podman ps --format \u0026#39;{{.Names}}\u0026#39;)) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; \\ --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload 6. Update All Containers for name in $(podman ps --format \u0026#39;{{.Names}}\u0026#39;); do image=$(podman inspect --format \u0026#39;{{.ImageName}}\u0026#39; \u0026#34;$name\u0026#34;) podman pull \u0026#34;$image\u0026#34; systemctl --user restart \u0026#34;container-$name.service\u0026#34; echo \u0026#34;$name updated and restarted.\u0026#34; done 7. Stop All Containers for name in $(podman ps --format \u0026#39;{{.Names}}\u0026#39;); do echo \u0026#34;Stopping $name...\u0026#34; podman stop \u0026#34;$name\u0026#34; done 8. List Container IPs for net in $(podman network ls --format \u0026#39;{{.Name}}\u0026#39;); do echo \u0026#34;Network: $net\u0026#34; podman network inspect $net | jq -r \u0026#39; .[0].containers | to_entries[] | \u0026#34;\\(.value.name) \\(.value.interfaces.eth0.subnets[0].ipnet // \u0026#34;\u0026#34;)\u0026#34; \u0026#39; | while read -r name ipcidr; do ip=${ipcidr%%/*} [ -z \u0026#34;$ip\u0026#34; ] \u0026amp;\u0026amp; ip=\u0026#34;(none)\u0026#34; echo -e \u0026#34;$name\\t$ip\u0026#34; done echo \u0026#34;\u0026#34; done 9. Closing Note Rootless Podman is more secure and works seamlessly with systemd for automated container management.\n","date":"8 August, 2025","id":7,"permalink":"/hidden/podman-101/","summary":"","tags":"podman","title":"Podman 101 - Always Run Rootless!"},{"content":"Introduction Setting up a reverse proxy is a crucial step in managing a homelab. It allows you to expose multiple services on your network, all under a single domain, with proper SSL/TLS encryption. Traefik is an excellent, modern, and easy-to-configure reverse proxy that integrates seamlessly with container orchestrators like Podman.\nThis guide walks you through setting up Traefik on Podman, complete with a self-signed wildcard certificate for local development. This setup is perfect for homelab environments where you need secure, accessible services without the hassle of public DNS records and paid certificates.\n1. Prerequisites and System Configuration Before we deploy Traefik, we need to configure our system to allow it to run properly. We\u0026rsquo;ll enable access to privileged ports and configure the Podman socket.\nSystem Setup First, let\u0026rsquo;s allow non-root users to bind to privileged ports (like 80 and 443).\nüö® DANGER: If you are running Pi-Hole as well then you already have minimum port set as 53. In that case do not run below cmd. It will break Pi-hole # Check lowest port. If port \u0026lt; 80 set then no need to run below cmds # cat /etc/sysctl.conf echo \u0026#39;net.ipv4.ip_unprivileged_port_start=80\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p Next, we\u0026rsquo;ll enable the Podman socket for our user. This is essential for Traefik to be able to detect and configure services running in other containers.\nsystemctl --user enable --now podman.socket Finally, we\u0026rsquo;ll open up the necessary ports on the firewall to ensure Traefik can receive traffic.\nsudo firewall-cmd --add-service={http,https} --permanent sudo firewall-cmd --reload 2. Directory Structure and Certificates To keep our configuration organized, we\u0026rsquo;ll create a dedicated directory for Traefik and generate a self-signed certificate. This certificate will be used for all our local services.\nDirectory Structure We\u0026rsquo;ll create a base directory for Traefik and two subdirectories: certs for our SSL certificates and dynamic for dynamic Traefik configurations.\nBASE_DIR=\u0026#34;~/traefik\u0026#34; mkdir -p \u0026#34;$BASE_DIR/certs\u0026#34; mkdir -p \u0026#34;$BASE_DIR/dynamic\u0026#34; cd \u0026#34;$BASE_DIR\u0026#34; Self-Signed Certificate Generation This script will generate a wildcard self-signed certificate for *.ak. This means any subdomain like homeassistant.ak or traefik.ak will be trusted by Traefik.\n# FOR DNS:*.ak domain=\u0026#34;.ak\u0026#34; if [ ! -f certs/local.crt ] \u0026amp;\u0026amp; [ ! -f certs/local.key ]; then echo \u0026#34;Generating wildcard self-signed certificate for *.$domain...\u0026#34; openssl req -x509 -newkey rsa:4096 -nodes -days 825 -sha256 \\ -keyout certs/local.key -out certs/local.crt \\ -subj \u0026#34;/C=PK/ST=Punjab/L=Lahore/O=HomeLab/OU=Development/CN=$domain\u0026#34; \\ -addext \u0026#34;subjectAltName=DNS:$domain,DNS:*.$domain\u0026#34; \\ -addext \u0026#34;basicConstraints=CA:FALSE\u0026#34; \\ -addext \u0026#34;keyUsage=keyEncipherment,dataEncipherment,digitalSignature\u0026#34; \\ -addext \u0026#34;extendedKeyUsage=serverAuth\u0026#34; \\ -addext \u0026#34;authorityKeyIdentifier=keyid,issuer\u0026#34; echo \u0026#34;‚úÖ Certificate generated:\u0026#34; openssl x509 -in certs/local.crt -noout -text | grep -A1 \u0026#34;Subject Alternative Name\u0026#34; else echo \u0026#34;‚ö†Ô∏è Local certificate already exists, skipping generation.\u0026#34; fi ‚ö†Ô∏è WARNING: For browsers to trust this certificate, you\u0026rsquo;ll need to manually import it into your system\u0026rsquo;s trust store.\n3. Traefik Configuration Files Traefik uses two types of configuration files: static and dynamic. The static configuration defines the core settings, while the dynamic configuration sets up routers, services, and middleware.\nStatic Configuration (traefik.yml) This file defines Traefik\u0026rsquo;s entry points (ports 80 and 443), enables the API dashboard, and configures the docker provider to monitor Podman containers. It also enables a file provider for dynamic configurations.\ntee ~/traefik/traefik.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: https scheme: https https: address: \u0026#34;:443\u0026#34; api: dashboard: true insecure: true providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: true file: directory: /etc/traefik/dynamic watch: true EOL Dynamic TLS Configuration (dynamic.yml) This file tells Traefik where to find the self-signed certificates we generated earlier.\ntee ~/traefik/dynamic/dynamic.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; tls: certificates: - certFile: /certs/local.crt keyFile: /certs/local.key EOL Dynamic Routers and Services To expose your services (like Home Assistant), you\u0026rsquo;ll create a separate dynamic configuration file. This file defines a router to match incoming traffic and a service to forward that traffic to the correct backend.\ntee ~/traefik/dynamic/homeassistant.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; http: routers: homeassistant: rule: \u0026#34;Host(`homeassistant.ak`)\u0026#34; service: homeassistant entryPoints: - https tls: {} middlewares: [] priority: 10 services: homeassistant: loadBalancer: servers: - url: \u0026#34;http://192.168.50.202:8123\u0026#34; EOL 4. Deploying with Podman Compose We\u0026rsquo;ll use a podman-compose.yml file to define and run our Traefik container.\npodman-compose.yml This file sets up the Traefik container, maps ports, and mounts the necessary configuration directories and the Podman socket. The labels on the traefik service configure Traefik\u0026rsquo;s own dashboard.\ntee /home/neo/traefik/podman-compose.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#34;3.3\u0026#34; services: traefik: image: docker.io/library/traefik:latest container_name: traefik security_opt: - label=type:container_runtime_t ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; - \u0026#34;8080:8080\u0026#34; volumes: - /run/user/1000/podman/podman.sock:/var/run/docker.sock:z - ./certs:/certs:ro,Z - ./traefik.yml:/etc/traefik/traefik.yml:ro,Z - ./dynamic:/etc/traefik/dynamic:ro,Z labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.traefik.rule=Host(`traefik.ak`)\u0026#34; - \u0026#34;traefik.http.routers.traefik.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.traefik.loadbalancer.server.port=8080\u0026#34; - \u0026#34;traefik.http.routers.traefik.tls=true\u0026#34; EOL Starting Traefik Now, you can start Traefik with the podman compose up command.\npodman compose -f ~/traefik/podman-compose.yml up -d --force-recreate üü¢ NOTE: You can access the Traefik dashboard at http://192.168.50.200:8080\nAuto start containers=(traefik) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload This setup gives you a powerful and flexible reverse proxy for all your homelab services, secured with local certificates. You can easily add new services by simply creating a new configuration file in the dynamic directory. üöÄ\n","date":"8 August, 2025","id":8,"permalink":"/hidden/podman-traefik/","summary":"Setting up a reverse proxy is a crucial step in managing a homelab. It allows you to expose multiple services on your network, all under a single domain, with proper SSL/TLS encryption. Traefik is an excellent, modern, and easy-to-configure reverse proxy that integrates seamlessly with container orchestrators like Podman.","tags":"kubernetes podman traefik","title":"Traefik on Podman with Local Certificates"},{"content":"What Pi-hole Does Pi-hole is a network-level ad blocker that acts as a DNS sinkhole. Here\u0026rsquo;s how to run it securely on your Fedora-based homelab using Podman and Traefik, with a clean and idempotent setup.\nStep-by-Step Setup 1. Prepare System # Create working directory mkdir ~/pihole \u0026amp;\u0026amp; cd ~/pihole 2. Allow Binding to Port 53 (as non-root) echo \u0026#39;net.ipv4.ip_unprivileged_port_start=53\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p # Reloads kernel parameters from the config /etc/sysctl.conf 3. Create podman-compose.yml tee ~/pihole/podman-compose.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#39;3\u0026#39; services: pihole: container_name: pihole image: docker.io/pihole/pihole:latest restart: unless-stopped ports: - \u0026#34;192.168.50.100:53:53/tcp\u0026#34; # \u0026lt;--- Change - \u0026#34;192.168.50.100:53:53/udp\u0026#34; # \u0026lt;--- Change - \u0026#34;192.168.50.100:8099:80/tcp\u0026#34; # \u0026lt;--- Change environment: TZ: \u0026#34;Australia/Sydney\u0026#34; FTLCONF_webserver_api_password: \u0026#34;pihole\u0026#34; # \u0026lt;--- Change volumes: - pihole-etc:/etc/pihole - pihole-dnsmasq:/etc/dnsmasq.d cap_add: - NET_ADMIN dns: - 9.9.9.9 - 45.90.30.0 labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.pihole.rule=Host(`pihole.node1`)\u0026#34; # \u0026lt;--- Change - \u0026#34;traefik.http.routers.pihole.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.pihole.tls=true\u0026#34; - \u0026#34;traefik.http.services.pihole.loadbalancer.server.port=80\u0026#34; volumes: pihole-etc: pihole-dnsmasq: EOL 4. Deploy Pi-hole # Ensure clean state podman compose -f ~/pihole/podman-compose.yml down # Deploy podman compose -f ~/pihole/podman-compose.yml up -d --force-recreate Auto start # Allow the user\u0026#39;s user services (like systemd --user) to run even when not logged in sudo loginctl enable-linger \u0026#34;$USER\u0026#34; # Enable and start the Podman API socket for the current user, required for systemd + Podman integration systemctl --user enable --now podman.socket # Ensure the user systemd service directory exists mkdir -p ~/.config/systemd/user/ # Generate a systemd service unit file for the container containers=(pihole) # \u0026lt;--- Change to your container name for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload 5. Post-deploy Actions # Check HTTP response curl -kI http://192.168.50.100:8099/admin/login # Set or reset password sudo podman exec -it pihole pihole setpassword 6. Open Firewall for DNS sudo firewall-cmd --permanent --add-service=dns sudo firewall-cmd --reload 7. Access Service Address Pi-hole UI https://pihole.node1 (via Traefik) Direct IP http://192.168.50.100:8099/admin 8. DNS: Enter your local DNS entries in pihole So you don\u0026rsquo;t have to add them manually to each devices /etc/hosts file\npodman exec pihole sed -i \u0026#39;s/^ etc_dnsmasq_d *= *.*/ etc_dnsmasq_d=true/\u0026#39; /etc/pihole/pihole.toml # Create custom_hosts.conf in /etc/dnsmasq.d/ with specific entries # Add your hostnames below DOMAIN_SUFFIX=\u0026#34;.node1\u0026#34; IP=\u0026#34;192.168.50.100\u0026#34; HOSTS=( node1 homeassistant cockpit traefik pihole jellyfin jellyseerr prowlarr sonarr radarr transmission flare-bypasser readarr calibre-web calibre wazuh homepage kavita nextcloud hello test test1 test2 argocd k3s ) # address=/node1.localhost/192.168.50.100 for host in \u0026#34;${HOSTS[@]}\u0026#34;; do echo \u0026#34;address=/${host}${DOMAIN_SUFFIX}/${IP}\u0026#34; done | podman exec -i pihole tee /etc/dnsmasq.d/custom_hosts.conf \u0026gt; /dev/null Restart pihole pod\n# Verify the contents of custom_hosts.conf podman exec -it pihole cat /etc/dnsmasq.d/custom_hosts.conf podman exec -it pihole cat /etc/pihole/pihole.toml | grep etc_dnsmasq_d # Restart the pihole container to apply changes podman restart pihole Undo All\n# undo podman exec -it pihole rm /etc/dnsmasq.d/custom_hosts.conf podman exec pihole sed -i \u0026#39;s/^ etc_dnsmasq_d *= *.*/ etc_dnsmasq_d=false/\u0026#39; /etc/pihole/pihole.toml 9. Router Change Primary DNS in router to 192.168.50.100\n10. Summary You now have a self-hosted, containerized Pi-hole setup running under Podman, fronted by Traefik, and configured for secure DNS resolution across your homelab.\n11. Delete podman stop pihole podman rm pihole podman volume rm pihole-etc pihole-dnsmasq rm -rf ~/pihole podman rmi docker.io/pihole/pihole sudo sed -i \u0026#39;/net.ipv4.ip_unprivileged_port_start=53/d\u0026#39; /etc/sysctl.conf sudo sysctl -p sudo firewall-cmd --permanent --remove-service=dns sudo firewall-cmd --reload ","date":"7 August, 2025","id":9,"permalink":"/hidden/podman-pihole/","summary":"Pi-hole is a network-level ad blocker that acts as a DNS sinkhole. Here\u0026rsquo;s how to run it securely on your Fedora-based homelab using Podman and Traefik, with a clean and idempotent setup.","tags":"pihole install fedora","title":"Pihole - Installation on Podman"},{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity‚Äîand the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml Access apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool‚Äîit\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ‚îú‚îÄ‚îÄ apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here ‚îÇ ‚îú‚îÄ‚îÄ guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îÇ ‚îú‚îÄ‚îÄ kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îÇ ‚îî‚îÄ‚îÄ podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îú‚îÄ‚îÄ env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, ‚îÇ ‚îú‚îÄ‚îÄ k3s ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ platform ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ workloads ‚îÇ ‚îî‚îÄ‚îÄ openshift ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îú‚îÄ‚îÄ platform ‚îÇ ‚îî‚îÄ‚îÄ workloads ‚îú‚îÄ‚îÄ ingress.yaml # Ingress to access ArgoCD dashboard ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ root-app.yaml # Root ARGOCD application ‚îî‚îÄ‚îÄ values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ ‚Äì Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ ‚îî‚îÄ‚îÄ \u0026lt;app1\u0026gt;/ ‚îú‚îÄ‚îÄ base/ ‚îî‚îÄ‚îÄ overlays/ ‚îú‚îÄ‚îÄ \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV ‚îî‚îÄ‚îÄ \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ ‚Äì Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n‚îÄ‚îÄ env ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;env1-name\u0026gt; ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ \u0026#39;argocd-application-for-app1\u0026#39;.yaml ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ \u0026#39;argocd-application-for-app2\u0026#39;.yaml ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml ‚Äì The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":10,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity‚Äîand the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute ‚Äî ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , ‚Äî this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":11,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute ‚Äî ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":12,"permalink":"/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure üí° The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n‚úÖ Visual: Prompt with icons, colors, and context-aware sections\n‚úÖ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n‚úÖ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":13,"permalink":"/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You‚Äôve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":14,"permalink":"/posts/networks/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes ‚Äî ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80‚Äôs door anymore ‚Äî it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy‚Äôs new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy‚Äôs map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph‚Äôs Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity ‚Äî just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":15,"permalink":"/posts/networks/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ‚úÖ Works ‚ùå Fails with CSRF token error Safari ‚úÖ Works ‚ùå Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":16,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode ‚Äî K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":17,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":18,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Introduction The \u0026ldquo;Arr stack\u0026rdquo; is a popular set of applications for automating your media library. This blog post will walk you through configuring a basic Arr stack using Podman on your homelab. We\u0026rsquo;ll cover Jellyfin for media streaming, Jellyserr for integrating services, Prowlarr for managing indexers, and Sonarr and Radarr for automating TV and movie downloads. This setup provides a simple yet powerful way to manage your media. üöÄ\nConfiguring the Arr Stack Basic flow about how each service talks to each other\nJellyfin Add media and scan Enable trickplay image extraction Save trickplay images next to media Trickplay Image Interval = 300000 Only generate images from key frames Jellyserr Settings -\u0026gt; Jellyfin. Get API from Jellyfin (Advanced -\u0026gt; API Keys) - See pic js1 Settings -\u0026gt; Services -\u0026gt; Radarr. Get API from Radarr (Settings -\u0026gt; General) Settings -\u0026gt; Services -\u0026gt; Sonarr. Get API from Sonarr (Settings -\u0026gt; General) Make sure flare-bypasser container is up Prowler Add indexers -\u0026gt; YTS etc Settings -\u0026gt; Indexer -\u0026gt; FlareSolverr -\u0026gt; Setup flare-bypasser Settings -\u0026gt; Apps -\u0026gt; Applications -\u0026gt; Radarr Settings -\u0026gt; Apps -\u0026gt; Applications -\u0026gt; Sonarr Sonarr Select location for TvShows Settings -\u0026gt; Indexer -\u0026gt; Verify indexers (from Prowler) Settings -\u0026gt; Download Clients -\u0026gt; Transmission Radarr Select location for Movies Settings -\u0026gt; Indexer -\u0026gt; Verify indexers (from Prowler) Settings -\u0026gt; Download Clients -\u0026gt; Transmission Service Local IP Access Ingress Jellyfin http://192.168.50.200:8096 https://jellyfin.node1 Jellyseerr http://192.168.50.200:5055 https://jellyseerr.node1 Prowlarr http://192.168.50.200:9696 https://prowlarr.node1 Sonarr http://192.168.50.200:8989 https://sonarr.node1 Radarr http://192.168.50.200:7878 https://radarr.node1 Transmission http://192.168.50.200:9091 https://transmission.node1 flare-bypasser http://192.168.50.200:8191 https://flare-bypasser.node1 Jellyfin libraries - Add Movies Jellyfin libraries - Add Shows jellyseerr - Give Jellyfin URL jellyseerr - Sync libraries jellyseerr - Sync with radarr\nGet key from radarr configure radarr in jellyseerr jellyseerr - Sync with sonarr\nGet key from sonarr configure sonarr in jellyseerr jellyseerr - Import users from jellyfin Prowlarr - configure flaresolverr Prowlarr - add radarr Prowlarr - add sonarr Prowlarr - tags sonarr - just get HEVC Summary Congratulations! üéâ You\u0026rsquo;ve now configured a powerful and automated media stack. With this setup, you can add movies and TV shows from your phone or desktop, and the Arr stack will automatically find and download them, ready for you to watch on Jellyfin. This is a great way to streamline your homelab and take control of your media.\n","date":"8 August, 2025","id":19,"permalink":"/hidden/podman-arr-stack-configuration/","summary":"The \u0026ldquo;Arr stack\u0026rdquo; is a popular set of applications for automating your media library. This blog post will walk you through configuring a basic Arr stack using Podman on your homelab. We\u0026rsquo;ll cover Jellyfin for media streaming, Jellyserr for integrating services, Prowlarr for managing indexers, and Sonarr and Radarr for automating TV and movie downloads. This setup provides a simple yet powerful way to manage your media. üöÄ","tags":"arr-stack podman homelab","title":"Arr Stack - Configuration"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic ‚Äî all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM ‚Äî after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ‚úÖ Yes Setup bridge/qemu policies ‚úÖ Yes VM create/operate via libvirt ‚ùå No Use virt-manager GUI ‚ùå No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":20,"permalink":"/homelab/homeassistant-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic ‚Äî all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference ‚Äî opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":21,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub üëâ Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":22,"permalink":"/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog ‚Äî a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\nüîß Tech Walkthroughs ‚Äî Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. üåê Home Lab \u0026amp; Automation ‚Äî Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. üõ°Ô∏è Security \u0026amp; Best Practices ‚Äî Focus security, supply chain integrity, and observability. üì¶ Modern Ops ‚Äî GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you‚Äôre an engineer, architect, or open source enthusiast ‚Äî I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":23,"permalink":"/about/","summary":"Welcome to my blog ‚Äî a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"},{"content":"Introduction Kubevirt VM connect to the cluster CNI (Flannel, Calico, etc), which is perfect for pod-to-pod communication but limits VMs that need full LAN access. We will address that by attaching it to the bridge network on host.\nPart of series on Kubevirt Create a (homeassistant VM) containerdisk Setup Bridge Network for a Kubevirt VM 1. Prerequisites Before diving in, ensure you have:\nA K3s cluster with kubectl or oc access. Linux bridge configured on host (bridge0). jq installed (sudo dnf install jq or brew install jq). Basic knowledge of KubeVirt, CDI, and Kubernetes networking. HLD Home Network\n1.1 Router (192.168.50.1): Provides LAN IPs via DHCP.\n1.2 User Device: Laptop/Phone accessing VM or cluster services.\nKubernetes Host\n2.1 Physical NIC (enp3s0): Connects host to LAN.\n2.2 Linux Bridge (bridge0): Virtual switch connecting VMs to LAN.\n2.3 Flow: NIC ‚Üí Bridge ‚Üí VM ‚Üí LAN\nKubernetes Cluster\n3.1 Multus CNI: Manages VM network attachments.\n3.2 KubeVirt VM (Fedora): Attaches to bridge0 via homenet NAD.\nNetwork Flow\n4.1 VM gets LAN IP via DHCP.\n4.2 VM ‚Üî Linux Bridge ‚Üî Host NIC ‚Üî Router ‚Üî LAN\n4.3 VM can communicate directly with user devices.\nKey Points\n5.1 VM behaves like a normal LAN device.\n5.2 LAN devices can access VM directly.\n5.3 Multus + bridge allows bypassing pod network NAT.\ngraph TD %% Home Network subgraph \u0026#34;Home Network\u0026#34; Router[\u0026#34;Router / DHCP Server\u0026lt;br\u0026gt;IP: 192.168.50.1\u0026#34;] UserDevice[\u0026#34;Laptop / Phone\u0026#34;] end %% Kubernetes Host subgraph \u0026#34;Kubernetes Host\u0026#34; direction LR HostNIC[\u0026#34;Physical NIC: enp3s0\u0026#34;] LinuxBridge[\u0026#34;Linux Bridge: bridge0\u0026#34;] HostNIC --\u0026gt;|Port added to| LinuxBridge end %% Kubernetes Cluster subgraph \u0026#34;Kubernetes Cluster\u0026#34; Multus[\u0026#34;Multus CNI\u0026#34;] KubeVirtVM[\u0026#34;KubeVirt VM - Fedora\u0026#34;] end %% Network Flow Router --\u0026gt;|DHCP Lease / LAN Traffic| HostNIC LinuxBridge --\u0026gt;|Virtual Port| Multus Multus --\u0026gt;|Attaches VM to bridge - NAD homenet| KubeVirtVM KubeVirtVM --\u0026gt;|Traffic to/from LAN| LinuxBridge KubeVirtVM --\u0026gt;|Gets LAN IP| Router KubeVirtVM --\u0026gt;|Communicates with| UserDevice UserDevice --\u0026gt;|Connects to| KubeVirtVM %% Styles style Router fill:#fcf,stroke:#333,stroke-width:2px style UserDevice fill:#cff,stroke:#333,stroke-width:2px style HostNIC fill:#ffc,stroke:#333,stroke-width:2px style LinuxBridge fill:#bbf,stroke:#333,stroke-width:2px style Multus fill:#ff9,stroke:#333,stroke-width:2px style KubeVirtVM fill:#9f9,stroke:#333,stroke-width:2px 2. Why use a Bridge Network for KubeVirt VMs KubeVirt VMs normally connect to the cluster CNI (Flannel, Calico, etc), which is perfect for pod-to-pod communication but limits VMs that need:\nLAN visibility ‚Äì VM gets an IP on the same subnet as your home devices. Direct device access ‚Äì Talk to IoT devices, NAS, printers without NAT. Predictable IPs ‚Äì DHCP or static assignment for consistent network identity. Solution: Multus + bridge CNI. The VM attaches to bridge0 on the host, making it a full LAN citizen.\nProcess flow: graph TB A[\u0026#34;Create Linux Bridge on Host\u0026#34;] --\u0026gt; B[\u0026#34;Install KubeVirt \u0026amp; CDI\u0026#34;] B --\u0026gt; C[\u0026#34;Install Multus\u0026#34;] C --\u0026gt; D[\u0026#34;Create NetworkAttachmentDefinition\u0026#34;] D --\u0026gt; E[\u0026#34;Deploy VM with Network Attachment\u0026#34;] 3. Host Linux Bridge Setup (bridge0) with NetworkManager bridge0 is a virtual switch; the physical interface (enp3s0) connects it to your LAN. VMs attach to bridge0 via Multus.\nCLI Steps # 3.1 Create the bridge nmcli connection add type bridge con-name bridge0 ifname bridge0 stp no # 3.2 Add physical NIC to bridge nmcli connection add type ethernet con-name \u0026#34;Wired Connection\u0026#34; ifname enp3s0 master bridge0 # 3.3 Configure DHCP (or static if desired) nmcli connection modify bridge0 ipv4.method auto nmcli connection modify bridge0 ipv6.method auto # 3.4 Enable autoconnect nmcli connection modify bridge0 connection.autoconnect yes nmcli connection modify \u0026#34;Wired Connection\u0026#34; connection.autoconnect yes # 3.5 Bring up connections nmcli connection up bridge0 nmcli connection up \u0026#34;Wired Connection\u0026#34; # 3.6 Verify setup nmcli connection show nmcli device status ‚úÖ Result: bridge0 is live, VMs attached will get LAN IPs automatically.\nüí° Tip: Use nmcli on headless servers for fully automated bridge setup.\n4. Install KubeVirt Deploy the operator and KubeVirt CR to run VMs on Kubernetes.\nexport KUBEVIRT_VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name) echo $KUBEVIRT_VERSION kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-operator.yaml kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-cr.yaml kubectl get pods -n kubevirt ‚≠êÔ∏è Tip: Wait until all pods in kubevirt namespace are Running before continuing. 5. Install Containerized Data Importer (CDI) CDI handles VM image uploads and DataVolumes.\nexport CDI_VERSION=$(curl -s https://api.github.com/repos/kubevirt/containerized-data-importer/releases/latest | jq -r .tag_name) echo $CDI_VERSION kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml kubectl get pods -n cdi # kubectl -n cdi port-forward svc/cdi-uploadproxy 8443:443 6. Install Multus on K3s Multus enables multiple interfaces per VM/pod‚Äîessential for bridge networking.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: multus namespace: kube-system spec: repo: https://rke2-charts.rancher.io chart: rke2-multus targetNamespace: kube-system valuesContent: |- config: fullnameOverride: multus cni_conf: confDir: /var/lib/rancher/k3s/agent/etc/cni/net.d binDir: /var/lib/rancher/k3s/data/cni/ kubeconfig: /var/lib/rancher/k3s/agent/etc/cni/net.d/multus.d/multus.kubeconfig multusAutoconfigDir: /var/lib/rancher/k3s/agent/etc/cni/net.d manifests: dhcpDaemonSet: true EOF 7. Create DataVolume (Fedora 42) Create a persistent VM disk.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: annotations: cdi.kubevirt.io/storage.bind.immediate.requested: \u0026#34;\u0026#34; name: fedora-dv-42 spec: contentType: kubevirt source: http: url: \u0026#34;https://download.fedoraproject.org/pub/fedora/linux/releases/42/Cloud/x86_64/images/Fedora-Cloud-Base-Generic-42-1.1.x86_64.qcow2\u0026#34; storage: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi EOF 8. Network Attachment Definition Attach VM to bridge0. You can choose DHCP or Static IP.\nDHCP Static IP cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: homenet spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;bridge0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dhcp\u0026#34; } }\u0026#39; EOF cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: homenet spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;bridge0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;static\u0026#34;, \u0026#34;addresses\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;192.168.50.204/24\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;192.168.50.1\u0026#34; } ], \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;192.168.50.1\u0026#34; } ], \u0026#34;dns\u0026#34;: { \u0026#34;nameservers\u0026#34;: [\u0026#34;192.168.50.1\u0026#34;, \u0026#34;1.1.1.1\u0026#34;] } } }\u0026#39; EOF NAD diagram graph TD NAD_Choice{{\u0026#34;Network Attachment Definition\u0026lt;br\u0026gt;(homenet)\u0026#34;}} subgraph \u0026#34;Option 1: Dynamic IP\u0026#34; DHCP[\u0026#34;IPAM: DHCP\u0026#34;] DHCP_Config[(\u0026#34;ipam: { type: dhcp }\u0026#34;)] NAD_Choice --\u0026gt;|Selects| DHCP DHCP --\u0026gt;|Uses CNI Plugin| DHCP_Config end subgraph \u0026#34;Option 2: Static IP\u0026#34; Static[\u0026#34;IPAM: Static\u0026#34;] Static_Config[(\u0026#34;ipam: { type: static, addresses: [...] }\u0026#34;)] NAD_Choice --\u0026gt;|Selects| Static Static --\u0026gt;|Specifies configuration| Static_Config end style NAD_Choice fill:#fff,stroke:#333,stroke-width:2px 9. Create VirtualMachine Deploy Fedora VM with homenet interface.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: fedora-dv-bridge labels: kubevirt.io/os: linux spec: runStrategy: Always template: metadata: labels: kubevirt.io/domain: vm spec: domain: cpu: cores: 1 devices: disks: - name: disk0 disk: bus: virtio - name: cloudinitdisk cdrom: bus: sata readonly: true interfaces: - name: homenet bridge: {} model: virtio machine: type: q35 resources: requests: memory: 2048M networks: - name: homenet multus: networkName: homenet volumes: - name: disk0 persistentVolumeClaim: claimName: fedora-dv-42 - name: cloudinitdisk cloudInitNoCloud: userData: | #cloud-config hostname: fedora-dv-bridge ssh_pwauth: True password: fedora chpasswd: {expire: False} runcmd: - dnf install -y qemu-guest-agent cockpit - systemctl enable qemu-guest-agent - systemctl enable --now cockpit.socket - systemctl start qemu-guest-agent cockpit EOF üéâ Congratulations! Your KubeVirt VM now has a LAN IP, can SSH in, use Cockpit, and interact with home network devices seamlessly.\n","date":"23 August, 2025","id":0,"permalink":"/posts/kubevirt_bridge/","summary":"Kubevirt VM connect to the cluster CNI (Flannel, Calico, etc), which is perfect for pod-to-pod communication but limits VMs that need full LAN access. We will address that by attaching it to the bridge network on host.","tags":"kubernetes multus kubevirt networking","title":"Setup Bridge Network for a Kubevirt VM"},{"content":"1. Introduction: What is Home Assistant? Home Assistant is a powerful, open-source home automation platform that puts local control and privacy first. It can be run on various devices, from single-board computers like the Raspberry Pi to virtual machines and containers. It\u0026rsquo;s a great tool for anyone looking to automate their home without relying on big tech companies, giving you complete control over your smart devices.\n2. Installation Methods Home Assistant offers a few different ways to get started, but two of the most common are:\nHome Assistant Operating System (HaOS): This is the recommended and most popular installation method. HaOS is a lightweight, embedded operating system designed specifically to run Home Assistant and its ecosystem. It\u0026rsquo;s easy to install on a dedicated device like a Home Assistant Green, a Raspberry Pi, or a virtual machine. This method gives you access to the full Home Assistant experience, including the convenience of add-ons, which are pre-packaged applications that extend its functionality.\nHome Assistant Container: This method is for more advanced users who want to run Home Assistant within a container environment (like Docker or Podman). You\u0026rsquo;re responsible for managing the underlying operating system and the container yourself. While this offers flexibility, it comes with a trade-off: you don\u0026rsquo;t get access to the official add-ons.\n3. HaOS on KubeVirt: The Best of Both Worlds If you\u0026rsquo;re already running a Kubernetes homelab and prefer the flexibility of containerized applications but still want the convenience and features of the full HaOS experience, KubeVirt is the perfect solution. KubeVirt is a Kubernetes add-on that lets you run traditional virtual machines (VMs) alongside your container workloads. This means you can run the full HaOS as a VM right inside your Kubernetes cluster, giving you a powerful, unified platform for both your containers and your home automation.\n4. Install KubeVirt and CDI The first step is to get KubeVirt and its dependencies up and running in your cluster. We will also install the Containerized Data Importer (CDI), which is essential for importing the virtual machine disk image into Kubernetes.\nInstall Kubevirt \u0026amp; CDI Install Kubevirt # Install KuberVirt export KUBEVIRT_VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name) echo $KUBEVIRT_VERSION kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-operator.yaml kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-cr.yaml kubectl get pods -n kubevirt # Install Containerized Data Importer (CDI) export CDI_VERSION=$(curl -s https://api.github.com/repos/kubevirt/containerized-data-importer/releases/latest | jq -r .tag_name) echo $CDI_VERSION kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml kubectl get pods -n cdi # kubectl -n cdi port-forward svc/cdi-uploadproxy 8443:443 ‚≠êÔ∏è NOTE: Steps 5-7 are about creating your own HaOS containerdisks. If you are happy to use my image based on HomeAssistant 16.1 then go to step-8 directly.\nIn case you are interested. Here are the official containerdisk images for various OS\n5. Download the HaOS image # Download the HaOS qcow2 image wget https://github.com/home-assistant/operating-system/releases/download/16.1/haos_ova-16.1.qcow2.xz # unzip xz -dk haos_ova-16.1.qcow2.xz 6. Convert the image Convert the image into a foramt that is understood by Kubevirt\nsudo dnf install podman libguestfs-tools guestfs-tools -y # machine-id gave error # virt-sysprep -a haos_ova-16.1.qcow2 --operations machine-id,bash-history,logfiles,tmp-files,net-hostname,net-hwaddr Image_name=haos_ova-16.1.qcow2 # Make Golden Image by removing unique identifiers and temporary files from the image. virt-sysprep -a $Image_name --operations bash-history,logfiles,tmp-files,net-hostname,net-hwaddr # Conpress image qemu-img convert -c -O qcow2 $Image_name \u0026#34;$Image_name-containerimage.qcow2\u0026#34; 7. Build the image and push to your repo # Create your Containerfile tee Containerfile \u0026gt; /dev/null \u0026lt;\u0026lt;EOL FROM kubevirt/container-disk-v1alpha ADD $Image_name-containerimage.qcow2 /disk/ EOL # Login to your repo podman login quay.io -u arslankhanali -p \u0026lt;\u0026gt; # Make sure `homeassistant` repo exists and it is public # Buld the image podman build -t quay.io/arslankhanali/homeassistant:v1 . # Push the image podman push quay.io/arslankhanali/homeassistant:v1 8. Deploy the VM on Kubevirt I will deploy in the namespace vm\nkubectl create ns vm kubectl config set-context --current --namespace=vm cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: name: haos namespace: vm # \u0026lt;-- Change as per need spec: domain: resources: requests: memory: 2048Mi cpu: 1 limits: memory: 4096Mi cpu: 2 devices: disks: - name: containerdisk disk: bus: virtio rng: {} firmware: bootloader: efi: secureBoot: false # ‚úÖ disable SecureBoot to avoid SMM requirement terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: quay.io/arslankhanali/homeassistant:v1 # \u0026lt;-- Change as per need --- apiVersion: v1 kind: Service metadata: name: haos namespace: vm spec: type: NodePort selector: vmi: haos ports: - name: haos-ui port: 8123 protocol: TCP targetPort: 8123 nodePort: 30003 # \u0026lt;-- Change as per need - Will be random it not set EOF 9. Console and port forward virtctl console haos # Password is ususlly set in cloud init inside vmi yaml 10. Console and port forward virtctl port-forward vmi/haos 8123:8123 curl -kI http://localhost:8123/onboarding.html curl -kI http://192.168.50.200:30003 Delete oc delete -f vm-haos.yaml oc delete vmi haos oc delete svc haos References:\nhttps://github.com/ormergi/vm-image-builder/tree/main?tab=readme-ov-file ","date":"17 August, 2025","id":1,"permalink":"/posts/featured/haos-on-kubevirt/","summary":"Home Assistant is a powerful, open-source home automation platform that puts local control and privacy first. It can be run on various devices, from single-board computers like the Raspberry Pi to virtual machines and containers. It\u0026rsquo;s a great tool for anyone looking to automate their home without relying on big tech companies, giving you complete control over your smart devices.","tags":"kubernetes podman","title":"Home Assistant VM on Kubevirt"},{"content":"0. General logins I used with my K3s email. : neogeo@gmail.com username. : neo password 1 : geo password 2 : Admin@123 1. Login to K3s scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config sed -i \u0026#39;\u0026#39; \u0026#39;s|server: https://127\\.0\\.0\\.1:6443|server: https://192.168.50.200:6443|\u0026#39; ~/k3s-config export KUBECONFIG=~/k3s-config oc get all -A 2. Argo oc get secret -n argocd argocd-initial-admin-secret -o jsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d rfgyTgvrj3RzWee1 3. Headlamp oc get secret headlamp-admin-secret-token -n headlamp -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Bearer token:\neyJhbGciOiJSUzI1NiIsImtpZCI6Ik9QLUlRd0RDOEdaeXFyZ3g0dnVkT1V3RzNIN0oweS1SODVaVDhza0d4RkEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJoZWFkbGFtcCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJoZWFkbGFtcC1hZG1pbi1zZWNyZXQtdG9rZW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiaGVhZGxhbXAtYWRtaW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2YWIyMmQ2ZC1kZmVmLTQ5NzAtODI3MS1jNWU1NjA3NjYyMTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6aGVhZGxhbXA6aGVhZGxhbXAtYWRtaW4ifQ.eZXZsTH_foaS2VKJuwz8Bj8U337xX6v7KRJCb9ZcQ9TYhbd2CeW9Q9VPIEv6lAEJYbBf_C-RTxnLsszS_lNHPk0sOnTMt6v6bb7qHKP-2uoYrLcGV0zBg-b1QLN2-fcYJEXFE_D_qbkLrclHxzmG-m4xtf2CFJPz-z4PxvTGYdsP1QONTjcdXqdEOSeFIbFSpvtgQ2NRHMOmIV4ANc8jccb_AEhGiFLnSW_6G1TXkgNFXhnUpwR4o42nV3V1_9W-qLUoSwqy7Owh6lfl3SErxy8_IaHf8JMJykhlEDhZQGzXcCIc3tQPtoVsJFwYWfMWmflh16aUmJiUmKKfDrmnqA 4. AWX oc get secret -n awx-operator awx-demo-admin-password -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d wDhnfcMYlNzrGqDkZVUFmq03tQxcpWLt 5. Gitea neo Admin@123 6. Grafana oc get secret -n grafana grafana -o jsonpath=\u0026#39;{.data.admin-password}\u0026#39; | base64 -d T6zGt8FwF2cKUYa9o88uqCOYxE0DE6a2Gpc3qosm 6. Harbor oc get secrets -n harbor harbor-core -o jsonpath=\u0026#39;{.data.HARBOR_ADMIN_PASSWORD}\u0026#39; | base64 -d Admin@123 7. 8. 9. 10. ","date":"15 August, 2025","id":2,"permalink":"/hidden/k3s-logins/","summary":"Bearer token:","tags":"kubernetes podman","title":"K3s Logins"},{"content":"Notes REMEMBER to run kubectl and kustomize apply commands from the correct Namespace/Project! Helm will fuck up otherwise Resources Keep requests.cpu and requests.mem very low. So not a lot of resource is dedicated to the workload. No point giving 512Mi to a workload that is only using 100Mi. Do not put any limits.cpu and limits.mem. Pods crash saying OOM (out of memory) i.e. prometheus Only set it if you fear that a spike from this workload will overwhelm the node. Replicas Keep them to 1 ERROR: _non_namespaceable_\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;olm\u0026quot;}}] error: no objects passed to apply Remove namespace when resource has its own namespace e.g. olm Always use ClusterIP and expose via Ingress. LoadBalancer and NodePort will clash with host ports service: type: ClusterIP When you run the kustomize build or helm commands, it will download the chart in the directory. You can get the default values.yaml file in it. It is always a good idea to search for terms like enabled replicas resources ingress There should be only one storage class local-path is faster - so keep it default - also you might have to delete longhorn , it would mess up everything Use longhorn when a pod needs RWX storage # Check oc get storageclass # Remove default oc annotate storageclass longhorn storageclass.kubernetes.io/is-default-class- # Make default oc annotate storageclass local-path storageclass.kubernetes.io/is-default-class=true Cert-manager uses a webhook to validate resources like Certificate and Issuer before they are stored in etcd. Remove it in testing/dev env oc patch validatingwebhookconfiguration cert-manager-webhook --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/webhooks/0/failurePolicy\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;Ignore\u0026quot;}]' Homepage Annotations annotations: gethomepage.dev/enabled: \u0026#34;true\u0026#34; gethomepage.dev/name: \u0026lt;\u0026gt; gethomepage.dev/description: \u0026lt;\u0026gt; gethomepage.dev/group: Cluster Management | Developer Tools | Storage | Security | Monitoring | Server gethomepage.dev/icon: \u0026lt;\u0026gt;.png # \u0026lt;-- https://github.com/homarr-labs/dashboard-icons/tree/main/png # e.g. \u0026#34;app.kubernetes.io/name: longhorn\u0026#34; , This provides running status in homepage UI gethomepage.dev/pod-selector: |- app.kubernetes.io/name in ( longhorn ) gethomepage.dev/weight: \u0026#39;0\u0026#39; 1. Create ns=grafana oc create ns $ns oc project $ns # For Kustomize with Helm kustomize build ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ --enable-helm | oc apply -f - # For Kustomize oc apply -k ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ # For a simple Yaml manifest oc apply -f ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ingress.yaml 2. Verify ns=grafana oc get all -A oc get all curl -Ik --resolve $ns.node1:443:192.168.50.200 https://$ns.node1 # after /etc/hosts file is updated curl -Ik https://$ns.node1 # Node: See all requests and limits for cpu and mem oc describe nodes node1 2.1 Logs ns=grafana oc project $ns oc logs pods/ oc describe pods/ 2.2 Use --previous flag for why the pods/container crashed oc logs pods/prometheus-server-7ffd965767-jhdpc -c prometheus-server --previous\n3. Stop oc scale deployment --all --replicas=0 -n grafana oc scale deployment --all --replicas=1 -n grafana 4. Delete all ns=grafana oc project $ns kustomize build ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ --enable-helm | oc delete -f - oc delete --all all -n $ns oc delete --all pods -n $ns --force --grace-period=0 oc delete --all pvc -n $ns oc delete ns $ns 4.1. Delete pods Use --grace-period=0 --force\noc delete --all pods --force --grace-period=0 -n $ns 4.2. Delete: Kube proxy method e.g. Delete PVC\nCreate namespace for it again if deleted delete any webhooks start oc proxy in another terminal Run below: remember to change and oc proxy # Delete Namespace ns=cattle-system curl -X PATCH http://127.0.0.1:8001/api/v1/namespaces/$ns \\ -H \u0026#34;Content-Type: application/json-patch+json\u0026#34; \\ --data \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/finalizers\u0026#34;}]\u0026#39; 4.3. Delete: Remove Finalizer e.g. Delete namespace\nns=grafana oc get ns $ns -o json | jq \u0026#39;.spec.finalizers=[]\u0026#39; | oc replace --raw /api/v1/namespaces/$ns/finalize -f - 5. Exec in container in a pod oc exec -it \u0026lt;pod\u0026gt; -c \u0026lt;container\u0026gt; -- sh 6. Ingresss K3s is bound to 192.168.50.200 IP - and podman uses 192.168.50.100 IP for Traefik.\nkubectl logs -n kube-system deploy/traefik # Get Traefik version kubectl -n kube-system get deploy traefik -o jsonpath=\u0026#39;{.spec.template.spec.containers[0].image}\u0026#39; oc -n kube-system get svc traefik # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # traefik LoadBalancer 10.43.148.201 192.168.50.200 80:30854/TCP,443:31067/TCP 3d1h # No other svc will have EXTERNAL-IP. because they are only exposed via Ingress 6.1. DNS nslookup jellyseerr.ak Server: 192.168.50.100 Address: 192.168.50.100#53 Name: jellyseerr.ak Address: 192.168.50.100 nslookup jellyseerr.node1 Server: 192.168.50.100 Address: 192.168.50.100#53 Name: jellyseerr.node1 Address: 192.168.50.200 7. ERROR: Failed to allocate directory watch: Too many open files # Check Limits ssh node1 ulimit -n 1024 cat /proc/$(pgrep -f kubelet)/limits | grep \u0026#34;open files\u0026#34; Max open files 1048576 1048576 files cat /proc/sys/fs/inotify/max_user_watches cat /proc/sys/fs/inotify/max_user_instances 122145 128 # Increase limits: Temporarily sudo sysctl -w fs.inotify.max_user_watches=524288 sudo sysctl -w fs.inotify.max_user_instances=1024 # Restart all sudo systemctl restart k3s oc delete pod -n kubevirt -l kubevirt.io=virt-handler # Increase limits: Permanently (survives reboot) echo \u0026#34;fs.inotify.max_user_watches=524288\u0026#34; | sudo tee -a /etc/sysctl.conf echo \u0026#34;fs.inotify.max_user_instances=1024\u0026#34; | sudo tee -a /etc/sysctl.conf sudo sysctl -p 8. Check version of Helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm search repo prometheus-community/prometheus --versions helm search repo prometheus-community/prometheus --versions | grep \u0026#39;prometheus-community/prometheus \u0026#39; ","date":"15 August, 2025","id":3,"permalink":"/hidden/k3s/","summary":"oc logs pods/prometheus-server-7ffd965767-jhdpc -c prometheus-server --previous","tags":"kubernetes podman","title":"K3s"},{"content":"Introduction Welcome to another installment in our AI series! Today, we\u0026rsquo;re going to set up Open WebUI, a powerful, self-hosted web interface for interacting with various large language models (LLMs). This tool provides a beautiful user experience similar to ChatGPT but on your own terms. We\u0026rsquo;ll be using Podman to containerize Open WebUI, making it a breeze to manage.\nWhy Open WebUI? Open WebUI acts as a central hub for your LLMs. It can connect to local models running on Ollama or LM Studio, as well as remote services like MaaS (Model as a Service). This flexibility allows you to experiment with different models and backends all from a single, consistent interface.\nStep 1: Pulling the Open WebUI Image First, we need to pull the Open WebUI container image from its public registry. Podman makes this simple and efficient.\n# This command fetches the latest main version of the Open WebUI image from GitHub\u0026#39;s container registry. podman pull ghcr.io/open-webui/open-webui:main Step 2: Running the Container Now, let\u0026rsquo;s run the container with all the necessary configurations. We\u0026rsquo;ll map a port, set environment variables, and create a persistent volume for data.\npodman run -d \\ --name open-webui \\ -p 3001:8080 \\ -e OLLAMA_BASE_URL=http://192.168.50.50:11434 \\ # If you have Ollama -e OPENAI_API_KEY=dummykey \\ -v open-webui:/app/backend/data:z \\ --restart=always \\ ghcr.io/open-webui/open-webui:main Step 3: Accessing Open WebUI Once the container is running, you can access the web interface by navigating to your host machine\u0026rsquo;s IP address and the mapped port.\nhttp://192.168.50.200:3001\nInitial Setup The first time you visit the page, you\u0026rsquo;ll be prompted to create a user account. After creating your account, you can log in and access the admin settings to configure your LLM connections.\nStep 4: Configuring Connections In the Open WebUI admin panel, you can add various connections to different LLM services.\nAdd connections: Settings -\u0026gt; Admin Panel -\u0026gt; Conenctions http://192.168.50.200:3001/admin/settings/connections e.g. Notice URL format\nOLLAMA: http://192.168.50.50:11434 # \u0026lt; \u0026mdash; From Ollama CLI OPENAI: http://192.168.50.50:1234/v1 # \u0026lt; \u0026mdash; From LMStudio\n‚ö†Ô∏è WARNING: Remember to use http or https as appropriate for your connection. Always use the correct port and make sure your firewall is configured to allow traffic on these ports.\nYou now have a fully functional Open WebUI instance running on your server, giving you a powerful tool to manage and interact with all your LLMs in one place!\n","date":"8 August, 2025","id":4,"permalink":"/posts/ai/openwebui-on-podman/","summary":"Welcome to another installment in our AI series! Today, we\u0026rsquo;re going to set up Open WebUI, a powerful, self-hosted web interface for interacting with various large language models (LLMs). This tool provides a beautiful user experience similar to ChatGPT but on your own terms. We\u0026rsquo;ll be using Podman to containerize Open WebUI, making it a breeze to manage.","tags":"openwebui podman ai ollama llm","title":"Open WebUI with Podman"},{"content":"Introduction Today, we\u0026rsquo;re going to deploy Dify, an AI application development platform, using Podman and podman-compose. Dify provides a powerful, visual way to build and manage AI applications, and deploying it on your local fedora server gives you full control.\nPrerequisites Before we start, make sure you have Podman and podman-compose installed. If you haven\u0026rsquo;t, you can refer to my previous blog post on how to set up Podman.\nüü¢ NOTE: This guide assumes you are running on a RHEL-based system with SELinux enabled, which is a common setup for fedora environments. The steps for SELinux are crucial for a smooth deployment.\nStep 1: Clone the Dify Repository First, we need to clone the Dify repository from GitHub. This will give us the necessary configuration files to deploy the application.\ngit clone [https://github.com/langgenius/dify.git](https://github.com/langgenius/dify.git) cd ~/dify/docker cp .env.example .env After cloning, we move into the docker directory and copy the example environment file to .env. This file contains the configuration variables for our Dify deployment.\nStep 2: Configure SELinux For systems with SELinux enabled, you\u0026rsquo;ll need to adjust the security context of the Dify volumes. This step prevents permission errors when Podman tries to access the files. By setting the context correctly, we avoid having to use the :z flag in our compose file.\nchcon -Rt container_file_t /home/neo/dify/docker/ ls -Z Step 3: Modify the docker-compose.yaml We need to make a couple of small but important changes to the docker-compose.yaml file to ensure Dify runs smoothly with Podman.\nvim docker-compose.yaml Change 1: Add Default Networks Add a default network to the end of the file. This ensures all services that don\u0026rsquo;t have an explicit network defined can communicate.\n# Go to end of file `shift+g` networks: ssrf_proxy_network: driver: bridge internal: true milvus: driver: bridge opensearch-net: driver: bridge internal: true default: driver: bridge Change 2: Ports Adjust the NGINX ports to avoid conflicts on your host. We\u0026rsquo;ll change the host ports from 80 and 443 to 980 and 9443 respectively. Remember to also change the corresponding variables in the .env file you copied earlier.\n# Search `/NGINX_PORT` ports: - \u0026#39;980:${NGINX_PORT:-80}\u0026#39; - \u0026#39;9443:${NGINX_SSL_PORT:-443}\u0026#39; Might also have to change nginx port in .env file 980 and 9443\nStep 4: Deploy and Verify Now we\u0026rsquo;re ready to deploy Dify using podman-compose.\nStart the containers\npodman-compose up -d podman pod logs pod_docker -f Step 5: Open Ports sudo firewall-cmd --add-port=980/tcp --permanent sudo firewall-cmd --reload Step 6: Test and Access podman pod logs pod_docker -f curl http://localhost:980/apps curl http://192.168.50.200:980/apps That\u0026rsquo;s it! You now have Dify running on your server. You can start building AI applications and experimenting with the platform.\n","date":"8 August, 2025","id":5,"permalink":"/posts/ai/dify-on-podman/","summary":"Today, we\u0026rsquo;re going to deploy Dify, an AI application development platform, using Podman and podman-compose. Dify provides a powerful, visual way to build and manage AI applications, and deploying it on your local fedora server gives you full control.","tags":"dify podman ai langgenius docker-compose","title":"Dify: A Podman Deployment Guide"},{"content":"Part of series on Arr-stack:\nArr-Stack Installation Arr-Stack Configuration Introduction The \u0026ldquo;arr stack\u0026rdquo; is a popular collection of applications used for managing media libraries. By running these applications as rootless containers with Podman, you can enhance security by preventing the containers from having root privileges on the host machine.\nThis guide will walk you through setting up the arr stack, including Jellyfin, Jellyseerr, Prowlarr, Sonarr, Radarr, and Transmission as rootless containers. We will cover directory preparation, mounting an external hard drive, and configuring the services using a podman-compose.yaml file.\nWhat is the \u0026ldquo;arr stack\u0026rdquo;? üóÉÔ∏è ‚ö†Ô∏è Check legal obligations where you live The arr stack is a suite of applications that work together to automate the process of finding, downloading, and organizing movies, TV shows, and other media.\nApplication Purpose Jellyfin A free software media system that lets you control the management and streaming of your media. Jellyseerr A request management and media discovery tool for Jellyfin and other \u0026lsquo;arr\u0026rsquo; apps. Prowlarr An indexer manager for the other \u0026lsquo;arr\u0026rsquo; apps, providing a centralized way to manage your indexers (sources for media). Sonarr Manages and automates the downloading of TV shows. Radarr Manages and automates the downloading of movies. Transmission A lightweight BitTorrent client used for downloading the media files. Prerequisites Before you start, make sure you have Podman and podman-compose installed. For Podman, you can often find it in your distribution\u0026rsquo;s package manager. For podman-compose, it\u0026rsquo;s a Python script that you can install with pip.\n# Example for a Fedora-based system sudo dnf install podman podman-docker # install podman-compose pip install podman-compose Setup Directories and External Drive First, we need to create the necessary directory structure for our applications and media files. This setup ensures that all your data is organized and easily accessible to the containers.\n# Set up a base directory for the arr stack dir=~/arr-stack mkdir -p $dir cd $dir # Create subdirectories for media and application downloads mkdir -p $dir/tvshows mkdir -p $dir/movies mkdir -p $dir/books mkdir -p $dir/transmission/downloads/complete/radarr mkdir -p $dir/transmission/downloads/complete/tv-sonarr mkdir -p $dir/transmission/downloads/incomplete/ mkdir -p $dir/transmission/watch mkdir -p $dir/prowler Mount an External Hard Drive If you have a large media library, it\u0026rsquo;s best to store it on an external hard drive. This section shows you how to mount the drive manually or automatically.\nManual Mount To manually mount your external drive, first find its device name and then mount it with the correct permissions. The uid=1000 and gid=1000 options ensure the drive is owned by your user, which is crucial for rootless containers. The context option is important for SELinux.\n# First, create a mount point mkdir -p $dir/external-drive # List block devices to find your external drive (e.g., /dev/sda1) lsblk # NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS # sda 8:0 0 1.8T 0 disk # ‚îî‚îÄsda1 8:1 0 1.8T 0 part ~/arr-stack/external-drive # Manually mount the drive. Replace /dev/sda1 with your device. sudo mount -t exfat -o users,noexec,uid=1000,gid=1000,context=\u0026#34;system_u:object_r:svirt_sandbox_file_t:s0\u0026#34; /dev/sda1 ~/arr-stack/external-drive Automatic Mount with /etc/fstab To have your drive automatically mount at startup, you need to add an entry to /etc/fstab.\nFind the UUID: The UUID is a unique identifier for your drive. # Find the UUID of your external drive sudo blkid /dev/sda1 Add to /etc/fstab: Append a new line to /etc/fstab using tee -a to avoid overwriting the file. # Path to your UUID and path echo \u0026#39;UUID=ECA3-DE06 /home/neo/arr-stack/external-drive exfat rw,users,noexec,nofail,async,auto,uid=1000,gid=1000,umask=0022,context=system_u:object_r:svirt_sandbox_file_t:s0 0 0\u0026#39; | sudo tee -a /etc/fstab Reload and Mount: Apply the new fstab entry. systemctl daemon-reload sudo mount -a ls -la arr-stack Check permission: Make sure your user (non-root) owns the folder ls -la ~/arr-stack You can verify the mount by running ls -la ~/arr-stack/external-drive.\nNote: The umount command is used to unmount the drive.\nsudo umount ~/arr-stack/external-drive The podman-compose.yaml File This file defines all the services in your arr stack. Each service is a container with specific configurations like image, port mappings, and volume mounts. A crucial aspect here is the volume mapping (- /path/on/host:/path/in/container:z). The :z option is essential for Podman to handle SELinux permissions correctly in a rootless environment.\nLet\u0026rsquo;s briefly explain some key configurations:\nimage: Specifies the container image to use. container_name: Sets a friendly name for the container. environment: Defines environment variables, such as PUID and PGID for user and group IDs, and TZ for the timezone. volumes: Connects host directories to container directories. This is how the applications can access your media files. ports: Maps container ports to host ports, allowing you to access the web UIs. labels: Used for services like Traefik to automatically configure reverse proxying and SSL/TLS. Some services, like Sonarr, Radarr, and Transmission, are run with PUID=0 and PGID=0 inside the container. This is a common practice for these specific images to avoid permission issues when writing to the /downloads directory, which is shared among them. The rootless Podman environment handles this by mapping the container\u0026rsquo;s root user (UID 0) to your host user (UID 1000).\nYou can ignore traefik section\ntee ~/arr-stack/podman-compose.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#34;3\u0026#34; services: # https://docs.linuxserver.io/images/docker-jellyfin/ jellyfin: image: docker.io/jellyfin/jellyfin:latest container_name: jellyfin environment: - TZ=Australia/Sydney volumes: - jellyfin-config:/config:z - jellyfin-cache:/cache:z - ~/arr-stack/external-drive:/media_external-drive:z - ~/arr-stack/movies:/media_movies:z - ~/arr-stack/tvshows:/media_tvshows:z - ~/arr-stack/prowler:/media_prowler:z - ~/arr-stack/transmission:/media_transmission:z ports: - 8096:8096 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.rule=Host(`jellyfin.ak`)\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.jellyfin.loadbalancer.server.port=8096\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.tls=true\u0026#34; # https://docs.jellyseerr.dev/getting-started/docker?docker-methods=docker-compose jellyseerr: image: docker.io/fallenbagel/jellyseerr:latest container_name: jellyseerr environment: - PUID=1000 - PGID=1000 - LOG_LEVEL=debug - TZ=Australia/Sydney ports: - 5055:5055 volumes: - jellyseerr-config:/app/config:z restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.rule=Host(`jellyseerr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.jellyseerr.loadbalancer.server.port=5055\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-prowlarr/#application-setup prowlarr: image: lscr.io/linuxserver/prowlarr:latest container_name: prowlarr environment: - PUID=1000 - PGID=1000 - TZ=Australia/Sydney volumes: - prowlarr-config:/config:z - ~/arr-stack/prowler/downloads:/downloads:z ports: - 9696:9696 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.rule=Host(`prowlarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.prowlarr.loadbalancer.server.port=9696\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-sonarr/ sonarr: image: lscr.io/linuxserver/sonarr:latest container_name: sonarr environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - sonarr-config:/config:z - ~/arr-stack/tvshows:/tvshows:z - ~/arr-stack/transmission/downloads:/downloads:z ports: - 8989:8989 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.sonarr.rule=Host(`sonarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.sonarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.sonarr.loadbalancer.server.port=8989\u0026#34; - \u0026#34;traefik.http.routers.sonarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-radarr/ radarr: image: lscr.io/linuxserver/radarr:latest container_name: radarr environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - radarr-config:/config:z - ~/arr-stack/movies:/movies:z - ~/arr-stack/transmission/downloads:/downloads:z ports: - 7878:7878 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.radarr.rule=Host(`radarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.radarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.radarr.loadbalancer.server.port=7878\u0026#34; - \u0026#34;traefik.http.routers.radarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-transmission/ transmission: image: lscr.io/linuxserver/transmission:latest container_name: transmission environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - transmission-config:/config:z - ~/arr-stack/transmission/downloads:/downloads:z - ~/arr-stack/transmission/watch:/watch:z ports: - 9091:9091 - 51413:51413 - 51413:51413/udp restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.transmission.rule=Host(`transmission.ak`)\u0026#34; - \u0026#34;traefik.http.routers.transmission.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.transmission.loadbalancer.server.port=9091\u0026#34; - \u0026#34;traefik.http.routers.transmission.tls=true\u0026#34; # It always run as UID=1111(flare_bypasser) # https://github.com/yoori/flare-bypasser flare-bypasser: image: ghcr.io/yoori/flare-bypasser:latest container_name: flare-bypasser environment: - PUID=1000 - PGID=1000 - TZ=Australia/Sydney volumes: - flare-bypasser-config:/config:z ports: - 8191:8080 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.rule=Host(`flare-bypasser.ak`)\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.flare-bypasser.loadbalancer.server.port=8191\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.tls=true\u0026#34; volumes: jellyfin-config: jellyfin-cache: sonarr-config: radarr-config: jellyseerr-config: prowlarr-config: transmission-config: flare-bypasser-config: EOL Deploying the Stack Once the podman-compose.yaml file is created, you can deploy the entire stack with a single command.\n# Make sure you\u0026#39;re in the ~/arr-stack directory cd ~/arr-stack # Deploy the services in detached mode podman-compose -f podman-compose.yaml up -d --force-recreate The --force-recreate flag ensures that if any configuration changes have been made, the containers will be rebuilt from scratch.\nTo check if the containers are running and to verify user permissions, you can use the following commands:\npodman ps # To check the user inside a container podman exec -it sonarr whoami This will show you the user that the container is running as, which should be abc (uid 1000) for most services, or root (uid 0) for the PUID=0 services, which is correctly mapped to your user.\nService Address Jellyfin http://192.168.50.200:8096 Jellyserr http://192.168.50.200:5055 Prowlarr http://192.168.50.200:9696 Sonarr http://192.168.50.200:8989 Radarr http://192.168.50.200:7878 Transmission http://192.168.50.200:9091 flare-bypasser http://192.168.50.200:8191 Enable Auto start # Remove old service files - That automatically start containers containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload # Create new service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload Update all cd ~/arr-stack echo \u0026#34;Pulling all images used by running containers...\u0026#34; for img in $(podman ps --format \u0026#34;{{.Image}}\u0026#34; | sort -u); do echo \u0026#34;Pulling: $img\u0026#34; podman pull \u0026#34;$img\u0026#34; done # Remove old service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload # Re-generate and enable new service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for name in \u0026#34;${containers[@]}\u0026#34;; do # This command generates a new systemd service file for the container podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files # This moves the newly generated service file to the correct systemd user directory mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ # Enable and start the new service systemctl --user enable \u0026#34;container-$name.service\u0026#34; systemctl --user start \u0026#34;container-$name.service\u0026#34; done # Reload the systemd manager configuration systemctl --user daemon-reload Remove all cd ~/arr-stack podman-compose down -v podman image prune containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload sudo umount ~/arr-stack/external-drive sudo nano /etc/fstab systemctl daemon-reload rm -rf ~/arr-stack ","date":"8 August, 2025","id":6,"permalink":"/hidden/podman-arr-stack/","summary":"Part of series on Arr-stack:","tags":"arr-stack podman homelab","title":"Arr stack - Installation"},{"content":"1. Introduction Podman is a daemonless container engine for running OCI containers.\nIt can run as root (rootful) or as a non-root user (rootless), each with different privileges and security implications.\nFeature Rootful Podman (Run as root) Rootless Podman (Run as non-root) Privileges Full root privileges on host system (sudo podman ...). Limited to user‚Äôs privileges (podman ...). Security Compromised container could gain root on host. Limited damage ‚Äî container root maps to non-root UID on host. Networking Can bind to privileged ports (\u0026lt;1024) directly. Uses slirp4netns, cannot bind \u0026lt;1024 without extra config. User ID Mapping Container root = Host root. Container root maps to non-root UID. 2. Running Rootless Do not use sudo to start containers.\nEnable lingering for your user:\nFor a rootless container to keep running after you\u0026rsquo;ve logged out, the Podman process itself needs to be managed by a system that persists. By enabling lingering, you allow the user\u0026rsquo;s systemd instance to continue running, which in turn can manage and keep Podman-related services alive.\nsudo loginctl enable-linger \u0026#34;$USER\u0026#34; Enable Podman socket for your user:\nThis command sets up and starts the Podman API socket for the current user.\nsystemctl --user enable --now podman.socket This creates a Podman socket at:\n/run/user/\u0026lt;UID\u0026gt;/podman/podman.sock (e.g., /run/user/1000/podman/podman.sock)\n3. Volume Mounting Default volume paths:\n# Rootful /var/lib/containers/storage/volumes/ # Rootless $HOME/.local/share/containers/storage/volumes/ 4. User Mapping Details Keeep - PUID=0 \u0026amp; - PGID=0 for containers in compose file. They will run as 0 inside container but 1000 on the host. i.e. rootless on host.\nüü¢ Make sure to run PODMAN without sudo Check UID mapping:\ncat /proc/$(pgrep -u \u0026#34;$USER\u0026#34; podman | head -n 1)/uid_map Example output:\nContainer UID Host UID Range Notes 0 1000 1 Container UID 0 is mapped to UID 1000 on host 1 524288 65536 Container UID 1 is mapped to UID 524288 on host 1000 525287 65536 Container UID 1000 is mapped to UID 525287 on host (524,288 + 999 = 525,287) 5. Auto restart: Systemd Integration (Rootless) Rootless service files live in:\n$HOME/.config/systemd/user\nGenerate and enable container services:\nmkdir -p ~/.config/systemd/user/ containers=($(podman ps --format \u0026#39;{{.Names}}\u0026#39;)) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; \\ --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload 6. Update All Containers for name in $(podman ps --format \u0026#39;{{.Names}}\u0026#39;); do image=$(podman inspect --format \u0026#39;{{.ImageName}}\u0026#39; \u0026#34;$name\u0026#34;) podman pull \u0026#34;$image\u0026#34; systemctl --user restart \u0026#34;container-$name.service\u0026#34; echo \u0026#34;$name updated and restarted.\u0026#34; done 7. Stop All Containers for name in $(podman ps --format \u0026#39;{{.Names}}\u0026#39;); do echo \u0026#34;Stopping $name...\u0026#34; podman stop \u0026#34;$name\u0026#34; done 8. List Container IPs for net in $(podman network ls --format \u0026#39;{{.Name}}\u0026#39;); do echo \u0026#34;Network: $net\u0026#34; podman network inspect $net | jq -r \u0026#39; .[0].containers | to_entries[] | \u0026#34;\\(.value.name) \\(.value.interfaces.eth0.subnets[0].ipnet // \u0026#34;\u0026#34;)\u0026#34; \u0026#39; | while read -r name ipcidr; do ip=${ipcidr%%/*} [ -z \u0026#34;$ip\u0026#34; ] \u0026amp;\u0026amp; ip=\u0026#34;(none)\u0026#34; echo -e \u0026#34;$name\\t$ip\u0026#34; done echo \u0026#34;\u0026#34; done 9. Closing Note Rootless Podman is more secure and works seamlessly with systemd for automated container management.\n","date":"8 August, 2025","id":7,"permalink":"/hidden/podman-101/","summary":"","tags":"podman","title":"Podman 101 - Always Run Rootless!"},{"content":"Introduction Setting up a reverse proxy is a crucial step in managing a homelab. It allows you to expose multiple services on your network, all under a single domain, with proper SSL/TLS encryption. Traefik is an excellent, modern, and easy-to-configure reverse proxy that integrates seamlessly with container orchestrators like Podman.\nThis guide walks you through setting up Traefik on Podman, complete with a self-signed wildcard certificate for local development. This setup is perfect for homelab environments where you need secure, accessible services without the hassle of public DNS records and paid certificates.\n1. Prerequisites and System Configuration Before we deploy Traefik, we need to configure our system to allow it to run properly. We\u0026rsquo;ll enable access to privileged ports and configure the Podman socket.\nSystem Setup First, let\u0026rsquo;s allow non-root users to bind to privileged ports (like 80 and 443).\nüö® DANGER: If you are running Pi-Hole as well then you already have minimum port set as 53. In that case do not run below cmd. It will break Pi-hole # Check lowest port. If port \u0026lt; 80 set then no need to run below cmds # cat /etc/sysctl.conf echo \u0026#39;net.ipv4.ip_unprivileged_port_start=80\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p Next, we\u0026rsquo;ll enable the Podman socket for our user. This is essential for Traefik to be able to detect and configure services running in other containers.\nsystemctl --user enable --now podman.socket Finally, we\u0026rsquo;ll open up the necessary ports on the firewall to ensure Traefik can receive traffic.\nsudo firewall-cmd --add-service={http,https} --permanent sudo firewall-cmd --reload 2. Directory Structure and Certificates To keep our configuration organized, we\u0026rsquo;ll create a dedicated directory for Traefik and generate a self-signed certificate. This certificate will be used for all our local services.\nDirectory Structure We\u0026rsquo;ll create a base directory for Traefik and two subdirectories: certs for our SSL certificates and dynamic for dynamic Traefik configurations.\nBASE_DIR=\u0026#34;~/traefik\u0026#34; mkdir -p \u0026#34;$BASE_DIR/certs\u0026#34; mkdir -p \u0026#34;$BASE_DIR/dynamic\u0026#34; cd \u0026#34;$BASE_DIR\u0026#34; Self-Signed Certificate Generation This script will generate a wildcard self-signed certificate for *.ak. This means any subdomain like homeassistant.ak or traefik.ak will be trusted by Traefik.\n# FOR DNS:*.ak domain=\u0026#34;.ak\u0026#34; if [ ! -f certs/local.crt ] \u0026amp;\u0026amp; [ ! -f certs/local.key ]; then echo \u0026#34;Generating wildcard self-signed certificate for *.$domain...\u0026#34; openssl req -x509 -newkey rsa:4096 -nodes -days 825 -sha256 \\ -keyout certs/local.key -out certs/local.crt \\ -subj \u0026#34;/C=PK/ST=Punjab/L=Lahore/O=HomeLab/OU=Development/CN=$domain\u0026#34; \\ -addext \u0026#34;subjectAltName=DNS:$domain,DNS:*.$domain\u0026#34; \\ -addext \u0026#34;basicConstraints=CA:FALSE\u0026#34; \\ -addext \u0026#34;keyUsage=keyEncipherment,dataEncipherment,digitalSignature\u0026#34; \\ -addext \u0026#34;extendedKeyUsage=serverAuth\u0026#34; \\ -addext \u0026#34;authorityKeyIdentifier=keyid,issuer\u0026#34; echo \u0026#34;‚úÖ Certificate generated:\u0026#34; openssl x509 -in certs/local.crt -noout -text | grep -A1 \u0026#34;Subject Alternative Name\u0026#34; else echo \u0026#34;‚ö†Ô∏è Local certificate already exists, skipping generation.\u0026#34; fi ‚ö†Ô∏è WARNING: For browsers to trust this certificate, you\u0026rsquo;ll need to manually import it into your system\u0026rsquo;s trust store.\n3. Traefik Configuration Files Traefik uses two types of configuration files: static and dynamic. The static configuration defines the core settings, while the dynamic configuration sets up routers, services, and middleware.\nStatic Configuration (traefik.yml) This file defines Traefik\u0026rsquo;s entry points (ports 80 and 443), enables the API dashboard, and configures the docker provider to monitor Podman containers. It also enables a file provider for dynamic configurations.\ntee ~/traefik/traefik.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: https scheme: https https: address: \u0026#34;:443\u0026#34; api: dashboard: true insecure: true providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: true file: directory: /etc/traefik/dynamic watch: true EOL Dynamic TLS Configuration (dynamic.yml) This file tells Traefik where to find the self-signed certificates we generated earlier.\ntee ~/traefik/dynamic/dynamic.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; tls: certificates: - certFile: /certs/local.crt keyFile: /certs/local.key EOL Dynamic Routers and Services To expose your services (like Home Assistant), you\u0026rsquo;ll create a separate dynamic configuration file. This file defines a router to match incoming traffic and a service to forward that traffic to the correct backend.\ntee ~/traefik/dynamic/homeassistant.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; http: routers: homeassistant: rule: \u0026#34;Host(`homeassistant.ak`)\u0026#34; service: homeassistant entryPoints: - https tls: {} middlewares: [] priority: 10 services: homeassistant: loadBalancer: servers: - url: \u0026#34;http://192.168.50.202:8123\u0026#34; EOL 4. Deploying with Podman Compose We\u0026rsquo;ll use a podman-compose.yml file to define and run our Traefik container.\npodman-compose.yml This file sets up the Traefik container, maps ports, and mounts the necessary configuration directories and the Podman socket. The labels on the traefik service configure Traefik\u0026rsquo;s own dashboard.\ntee /home/neo/traefik/podman-compose.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#34;3.3\u0026#34; services: traefik: image: docker.io/library/traefik:latest container_name: traefik security_opt: - label=type:container_runtime_t ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; - \u0026#34;8080:8080\u0026#34; volumes: - /run/user/1000/podman/podman.sock:/var/run/docker.sock:z - ./certs:/certs:ro,Z - ./traefik.yml:/etc/traefik/traefik.yml:ro,Z - ./dynamic:/etc/traefik/dynamic:ro,Z labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.traefik.rule=Host(`traefik.ak`)\u0026#34; - \u0026#34;traefik.http.routers.traefik.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.traefik.loadbalancer.server.port=8080\u0026#34; - \u0026#34;traefik.http.routers.traefik.tls=true\u0026#34; EOL Starting Traefik Now, you can start Traefik with the podman compose up command.\npodman compose -f ~/traefik/podman-compose.yml up -d --force-recreate üü¢ NOTE: You can access the Traefik dashboard at http://192.168.50.200:8080\nAuto start containers=(traefik) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload This setup gives you a powerful and flexible reverse proxy for all your homelab services, secured with local certificates. You can easily add new services by simply creating a new configuration file in the dynamic directory. üöÄ\n","date":"8 August, 2025","id":8,"permalink":"/hidden/podman-traefik/","summary":"Setting up a reverse proxy is a crucial step in managing a homelab. It allows you to expose multiple services on your network, all under a single domain, with proper SSL/TLS encryption. Traefik is an excellent, modern, and easy-to-configure reverse proxy that integrates seamlessly with container orchestrators like Podman.","tags":"kubernetes podman traefik","title":"Traefik on Podman with Local Certificates"},{"content":"What Pi-hole Does Pi-hole is a network-level ad blocker that acts as a DNS sinkhole. Here\u0026rsquo;s how to run it securely on your Fedora-based homelab using Podman and Traefik, with a clean and idempotent setup.\nStep-by-Step Setup 1. Prepare System # Create working directory mkdir ~/pihole \u0026amp;\u0026amp; cd ~/pihole 2. Allow Binding to Port 53 (as non-root) echo \u0026#39;net.ipv4.ip_unprivileged_port_start=53\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p # Reloads kernel parameters from the config /etc/sysctl.conf 3. Create podman-compose.yml tee ~/pihole/podman-compose.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#39;3\u0026#39; services: pihole: container_name: pihole image: docker.io/pihole/pihole:latest restart: unless-stopped ports: - \u0026#34;192.168.50.100:53:53/tcp\u0026#34; # \u0026lt;--- Change - \u0026#34;192.168.50.100:53:53/udp\u0026#34; # \u0026lt;--- Change - \u0026#34;192.168.50.100:8099:80/tcp\u0026#34; # \u0026lt;--- Change environment: TZ: \u0026#34;Australia/Sydney\u0026#34; FTLCONF_webserver_api_password: \u0026#34;pihole\u0026#34; # \u0026lt;--- Change volumes: - pihole-etc:/etc/pihole - pihole-dnsmasq:/etc/dnsmasq.d cap_add: - NET_ADMIN dns: - 9.9.9.9 - 45.90.30.0 labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.pihole.rule=Host(`pihole.node1`)\u0026#34; # \u0026lt;--- Change - \u0026#34;traefik.http.routers.pihole.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.pihole.tls=true\u0026#34; - \u0026#34;traefik.http.services.pihole.loadbalancer.server.port=80\u0026#34; volumes: pihole-etc: pihole-dnsmasq: EOL 4. Deploy Pi-hole # Ensure clean state podman compose -f ~/pihole/podman-compose.yml down # Deploy podman compose -f ~/pihole/podman-compose.yml up -d --force-recreate Auto start # Allow the user\u0026#39;s user services (like systemd --user) to run even when not logged in sudo loginctl enable-linger \u0026#34;$USER\u0026#34; # Enable and start the Podman API socket for the current user, required for systemd + Podman integration systemctl --user enable --now podman.socket # Ensure the user systemd service directory exists mkdir -p ~/.config/systemd/user/ # Generate a systemd service unit file for the container containers=(pihole) # \u0026lt;--- Change to your container name for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload 5. Post-deploy Actions # Check HTTP response curl -kI http://192.168.50.100:8099/admin/login # Set or reset password sudo podman exec -it pihole pihole setpassword 6. Open Firewall for DNS sudo firewall-cmd --permanent --add-service=dns sudo firewall-cmd --reload 7. Access Service Address Pi-hole UI https://pihole.node1 (via Traefik) Direct IP http://192.168.50.100:8099/admin 8. DNS: Enter your local DNS entries in pihole So you don\u0026rsquo;t have to add them manually to each devices /etc/hosts file\npodman exec pihole sed -i \u0026#39;s/^ etc_dnsmasq_d *= *.*/ etc_dnsmasq_d=true/\u0026#39; /etc/pihole/pihole.toml # Create custom_hosts.conf in /etc/dnsmasq.d/ with specific entries # Add your hostnames below DOMAIN_SUFFIX=\u0026#34;.node1\u0026#34; IP=\u0026#34;192.168.50.100\u0026#34; HOSTS=( node1 homeassistant cockpit traefik pihole jellyfin jellyseerr prowlarr sonarr radarr transmission flare-bypasser readarr calibre-web calibre wazuh homepage kavita nextcloud hello test test1 test2 argocd k3s ) # address=/node1.localhost/192.168.50.100 for host in \u0026#34;${HOSTS[@]}\u0026#34;; do echo \u0026#34;address=/${host}${DOMAIN_SUFFIX}/${IP}\u0026#34; done | podman exec -i pihole tee /etc/dnsmasq.d/custom_hosts.conf \u0026gt; /dev/null Restart pihole pod\n# Verify the contents of custom_hosts.conf podman exec -it pihole cat /etc/dnsmasq.d/custom_hosts.conf podman exec -it pihole cat /etc/pihole/pihole.toml | grep etc_dnsmasq_d # Restart the pihole container to apply changes podman restart pihole Undo All\n# undo podman exec -it pihole rm /etc/dnsmasq.d/custom_hosts.conf podman exec pihole sed -i \u0026#39;s/^ etc_dnsmasq_d *= *.*/ etc_dnsmasq_d=false/\u0026#39; /etc/pihole/pihole.toml 9. Router Change Primary DNS in router to 192.168.50.100\n10. Summary You now have a self-hosted, containerized Pi-hole setup running under Podman, fronted by Traefik, and configured for secure DNS resolution across your homelab.\n11. Delete podman stop pihole podman rm pihole podman volume rm pihole-etc pihole-dnsmasq rm -rf ~/pihole podman rmi docker.io/pihole/pihole sudo sed -i \u0026#39;/net.ipv4.ip_unprivileged_port_start=53/d\u0026#39; /etc/sysctl.conf sudo sysctl -p sudo firewall-cmd --permanent --remove-service=dns sudo firewall-cmd --reload ","date":"7 August, 2025","id":9,"permalink":"/hidden/podman-pihole/","summary":"Pi-hole is a network-level ad blocker that acts as a DNS sinkhole. Here\u0026rsquo;s how to run it securely on your Fedora-based homelab using Podman and Traefik, with a clean and idempotent setup.","tags":"pihole install fedora","title":"Pihole - Installation on Podman"},{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity‚Äîand the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml Access apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool‚Äîit\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ‚îú‚îÄ‚îÄ apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here ‚îÇ ‚îú‚îÄ‚îÄ guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îÇ ‚îú‚îÄ‚îÄ kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îÇ ‚îî‚îÄ‚îÄ podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îú‚îÄ‚îÄ env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, ‚îÇ ‚îú‚îÄ‚îÄ k3s ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ platform ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ workloads ‚îÇ ‚îî‚îÄ‚îÄ openshift ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îú‚îÄ‚îÄ platform ‚îÇ ‚îî‚îÄ‚îÄ workloads ‚îú‚îÄ‚îÄ ingress.yaml # Ingress to access ArgoCD dashboard ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ root-app.yaml # Root ARGOCD application ‚îî‚îÄ‚îÄ values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ ‚Äì Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ ‚îî‚îÄ‚îÄ \u0026lt;app1\u0026gt;/ ‚îú‚îÄ‚îÄ base/ ‚îî‚îÄ‚îÄ overlays/ ‚îú‚îÄ‚îÄ \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV ‚îî‚îÄ‚îÄ \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ ‚Äì Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n‚îÄ‚îÄ env ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;env1-name\u0026gt; ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ \u0026#39;argocd-application-for-app1\u0026#39;.yaml ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ \u0026#39;argocd-application-for-app2\u0026#39;.yaml ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml ‚Äì The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":10,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity‚Äîand the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute ‚Äî ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , ‚Äî this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":11,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute ‚Äî ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":12,"permalink":"/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure üí° The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n‚úÖ Visual: Prompt with icons, colors, and context-aware sections\n‚úÖ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n‚úÖ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":13,"permalink":"/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You‚Äôve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":14,"permalink":"/posts/networks/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes ‚Äî ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80‚Äôs door anymore ‚Äî it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy‚Äôs new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy‚Äôs map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph‚Äôs Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity ‚Äî just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":15,"permalink":"/posts/networks/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ‚úÖ Works ‚ùå Fails with CSRF token error Safari ‚úÖ Works ‚ùå Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":16,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode ‚Äî K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":17,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":18,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Introduction The \u0026ldquo;Arr stack\u0026rdquo; is a popular set of applications for automating your media library. This blog post will walk you through configuring a basic Arr stack using Podman on your homelab. We\u0026rsquo;ll cover Jellyfin for media streaming, Jellyserr for integrating services, Prowlarr for managing indexers, and Sonarr and Radarr for automating TV and movie downloads. This setup provides a simple yet powerful way to manage your media. üöÄ\nConfiguring the Arr Stack Basic flow about how each service talks to each other\nJellyfin Add media and scan Enable trickplay image extraction Save trickplay images next to media Trickplay Image Interval = 300000 Only generate images from key frames Jellyserr Settings -\u0026gt; Jellyfin. Get API from Jellyfin (Advanced -\u0026gt; API Keys) - See pic js1 Settings -\u0026gt; Services -\u0026gt; Radarr. Get API from Radarr (Settings -\u0026gt; General) Settings -\u0026gt; Services -\u0026gt; Sonarr. Get API from Sonarr (Settings -\u0026gt; General) Make sure flare-bypasser container is up Prowler Add indexers -\u0026gt; YTS etc Settings -\u0026gt; Indexer -\u0026gt; FlareSolverr -\u0026gt; Setup flare-bypasser Settings -\u0026gt; Apps -\u0026gt; Applications -\u0026gt; Radarr Settings -\u0026gt; Apps -\u0026gt; Applications -\u0026gt; Sonarr Sonarr Select location for TvShows Settings -\u0026gt; Indexer -\u0026gt; Verify indexers (from Prowler) Settings -\u0026gt; Download Clients -\u0026gt; Transmission Radarr Select location for Movies Settings -\u0026gt; Indexer -\u0026gt; Verify indexers (from Prowler) Settings -\u0026gt; Download Clients -\u0026gt; Transmission Service Local IP Access Ingress Jellyfin http://192.168.50.200:8096 https://jellyfin.node1 Jellyseerr http://192.168.50.200:5055 https://jellyseerr.node1 Prowlarr http://192.168.50.200:9696 https://prowlarr.node1 Sonarr http://192.168.50.200:8989 https://sonarr.node1 Radarr http://192.168.50.200:7878 https://radarr.node1 Transmission http://192.168.50.200:9091 https://transmission.node1 flare-bypasser http://192.168.50.200:8191 https://flare-bypasser.node1 Jellyfin libraries - Add Movies Jellyfin libraries - Add Shows jellyseerr - Give Jellyfin URL jellyseerr - Sync libraries jellyseerr - Sync with radarr\nGet key from radarr configure radarr in jellyseerr jellyseerr - Sync with sonarr\nGet key from sonarr configure sonarr in jellyseerr jellyseerr - Import users from jellyfin Prowlarr - configure flaresolverr Prowlarr - add radarr Prowlarr - add sonarr Prowlarr - tags sonarr - just get HEVC Summary Congratulations! üéâ You\u0026rsquo;ve now configured a powerful and automated media stack. With this setup, you can add movies and TV shows from your phone or desktop, and the Arr stack will automatically find and download them, ready for you to watch on Jellyfin. This is a great way to streamline your homelab and take control of your media.\n","date":"8 August, 2025","id":19,"permalink":"/hidden/podman-arr-stack-configuration/","summary":"The \u0026ldquo;Arr stack\u0026rdquo; is a popular set of applications for automating your media library. This blog post will walk you through configuring a basic Arr stack using Podman on your homelab. We\u0026rsquo;ll cover Jellyfin for media streaming, Jellyserr for integrating services, Prowlarr for managing indexers, and Sonarr and Radarr for automating TV and movie downloads. This setup provides a simple yet powerful way to manage your media. üöÄ","tags":"arr-stack podman homelab","title":"Arr Stack - Configuration"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic ‚Äî all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM ‚Äî after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ‚úÖ Yes Setup bridge/qemu policies ‚úÖ Yes VM create/operate via libvirt ‚ùå No Use virt-manager GUI ‚ùå No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":20,"permalink":"/homelab/homeassistant-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic ‚Äî all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference ‚Äî opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":21,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub üëâ Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":22,"permalink":"/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog ‚Äî a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\nüîß Tech Walkthroughs ‚Äî Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. üåê Home Lab \u0026amp; Automation ‚Äî Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. üõ°Ô∏è Security \u0026amp; Best Practices ‚Äî Focus security, supply chain integrity, and observability. üì¶ Modern Ops ‚Äî GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you‚Äôre an engineer, architect, or open source enthusiast ‚Äî I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":23,"permalink":"/about/","summary":"Welcome to my blog ‚Äî a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"},{"content":"Introduction Kubevirt VM connect to the cluster CNI (Flannel, Calico, etc), which is perfect for pod-to-pod communication but limits VMs that need full LAN access. We will address that by attaching it to the bridge network on host.\nPart of series on Kubevirt Create a (homeassistant VM) containerdisk Setup Bridge Network for a Kubevirt VM 1. Prerequisites Before diving in, ensure you have:\nA K3s cluster with kubectl or oc access. Linux bridge configured on host (bridge0). jq installed (sudo dnf install jq or brew install jq). Basic knowledge of KubeVirt, CDI, and Kubernetes networking. HLD Home Network\n1.1 Router (192.168.50.1): Provides LAN IPs via DHCP.\n1.2 User Device: Laptop/Phone accessing VM or cluster services.\nKubernetes Host\n2.1 Physical NIC (enp3s0): Connects host to LAN.\n2.2 Linux Bridge (bridge0): Virtual switch connecting VMs to LAN.\n2.3 Flow: NIC ‚Üí Bridge ‚Üí VM ‚Üí LAN\nKubernetes Cluster\n3.1 Multus CNI: Manages VM network attachments.\n3.2 KubeVirt VM (Fedora): Attaches to bridge0 via homenet NAD.\nNetwork Flow\n4.1 VM gets LAN IP via DHCP.\n4.2 VM ‚Üî Linux Bridge ‚Üî Host NIC ‚Üî Router ‚Üî LAN\n4.3 VM can communicate directly with user devices.\nKey Points\n5.1 VM behaves like a normal LAN device.\n5.2 LAN devices can access VM directly.\n5.3 Multus + bridge allows bypassing pod network NAT.\ngraph TD %% Home Network subgraph \u0026#34;Home Network\u0026#34; Router[\u0026#34;Router / DHCP Server\u0026lt;br\u0026gt;IP: 192.168.50.1\u0026#34;] UserDevice[\u0026#34;Laptop / Phone\u0026#34;] end %% Kubernetes Host subgraph \u0026#34;Kubernetes Host\u0026#34; direction LR HostNIC[\u0026#34;Physical NIC: enp3s0\u0026#34;] LinuxBridge[\u0026#34;Linux Bridge: bridge0\u0026#34;] HostNIC --\u0026gt;|Port added to| LinuxBridge end %% Kubernetes Cluster subgraph \u0026#34;Kubernetes Cluster\u0026#34; Multus[\u0026#34;Multus CNI\u0026#34;] KubeVirtVM[\u0026#34;KubeVirt VM - Fedora\u0026#34;] end %% Network Flow Router --\u0026gt;|DHCP Lease / LAN Traffic| HostNIC LinuxBridge --\u0026gt;|Virtual Port| Multus Multus --\u0026gt;|Attaches VM to bridge - NAD homenet| KubeVirtVM KubeVirtVM --\u0026gt;|Traffic to/from LAN| LinuxBridge KubeVirtVM --\u0026gt;|Gets LAN IP| Router KubeVirtVM --\u0026gt;|Communicates with| UserDevice UserDevice --\u0026gt;|Connects to| KubeVirtVM %% Styles style Router fill:#fcf,stroke:#333,stroke-width:2px style UserDevice fill:#cff,stroke:#333,stroke-width:2px style HostNIC fill:#ffc,stroke:#333,stroke-width:2px style LinuxBridge fill:#bbf,stroke:#333,stroke-width:2px style Multus fill:#ff9,stroke:#333,stroke-width:2px style KubeVirtVM fill:#9f9,stroke:#333,stroke-width:2px 2. Why use a Bridge Network for KubeVirt VMs KubeVirt VMs normally connect to the cluster CNI (Flannel, Calico, etc), which is perfect for pod-to-pod communication but limits VMs that need:\nLAN visibility ‚Äì VM gets an IP on the same subnet as your home devices. Direct device access ‚Äì Talk to IoT devices, NAS, printers without NAT. Predictable IPs ‚Äì DHCP or static assignment for consistent network identity. Solution: Multus + bridge CNI. The VM attaches to bridge0 on the host, making it a full LAN citizen.\nProcess flow: graph TB A[\u0026#34;Create Linux Bridge on Host\u0026#34;] --\u0026gt; B[\u0026#34;Install KubeVirt \u0026amp; CDI\u0026#34;] B --\u0026gt; C[\u0026#34;Install Multus\u0026#34;] C --\u0026gt; D[\u0026#34;Create NetworkAttachmentDefinition\u0026#34;] D --\u0026gt; E[\u0026#34;Deploy VM with Network Attachment\u0026#34;] 3. Host Linux Bridge Setup (bridge0) with NetworkManager bridge0 is a virtual switch; the physical interface (enp3s0) connects it to your LAN. VMs attach to bridge0 via Multus.\nCLI Steps # 3.1 Create the bridge nmcli connection add type bridge con-name bridge0 ifname bridge0 stp no # 3.2 Add physical NIC to bridge nmcli connection add type ethernet con-name \u0026#34;Wired Connection\u0026#34; ifname enp3s0 master bridge0 # 3.3 Configure DHCP (or static if desired) nmcli connection modify bridge0 ipv4.method auto nmcli connection modify bridge0 ipv6.method auto # 3.4 Enable autoconnect nmcli connection modify bridge0 connection.autoconnect yes nmcli connection modify \u0026#34;Wired Connection\u0026#34; connection.autoconnect yes # 3.5 Bring up connections nmcli connection up bridge0 nmcli connection up \u0026#34;Wired Connection\u0026#34; # 3.6 Verify setup nmcli connection show nmcli device status ‚úÖ Result: bridge0 is live, VMs attached will get LAN IPs automatically.\nüí° Tip: Use nmcli on headless servers for fully automated bridge setup.\n4. Install KubeVirt Deploy the operator and KubeVirt CR to run VMs on Kubernetes.\nexport KUBEVIRT_VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name) echo $KUBEVIRT_VERSION kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-operator.yaml kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-cr.yaml kubectl get pods -n kubevirt ‚≠êÔ∏è Tip: Wait until all pods in kubevirt namespace are Running before continuing. 5. Install Containerized Data Importer (CDI) CDI handles VM image uploads and DataVolumes.\nexport CDI_VERSION=$(curl -s https://api.github.com/repos/kubevirt/containerized-data-importer/releases/latest | jq -r .tag_name) echo $CDI_VERSION kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml kubectl get pods -n cdi # kubectl -n cdi port-forward svc/cdi-uploadproxy 8443:443 6. Install Multus on K3s Multus enables multiple interfaces per VM/pod‚Äîessential for bridge networking.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: multus namespace: kube-system spec: repo: https://rke2-charts.rancher.io chart: rke2-multus targetNamespace: kube-system valuesContent: |- config: fullnameOverride: multus cni_conf: confDir: /var/lib/rancher/k3s/agent/etc/cni/net.d binDir: /var/lib/rancher/k3s/data/cni/ kubeconfig: /var/lib/rancher/k3s/agent/etc/cni/net.d/multus.d/multus.kubeconfig multusAutoconfigDir: /var/lib/rancher/k3s/agent/etc/cni/net.d manifests: dhcpDaemonSet: true EOF 7. Create DataVolume (Fedora 42) Create a persistent VM disk.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: annotations: cdi.kubevirt.io/storage.bind.immediate.requested: \u0026#34;\u0026#34; name: fedora-dv-42 spec: contentType: kubevirt source: http: url: \u0026#34;https://download.fedoraproject.org/pub/fedora/linux/releases/42/Cloud/x86_64/images/Fedora-Cloud-Base-Generic-42-1.1.x86_64.qcow2\u0026#34; storage: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi EOF 8. Network Attachment Definition Attach VM to bridge0. You can choose DHCP or Static IP.\nDHCP Static IP cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: homenet spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;bridge0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dhcp\u0026#34; } }\u0026#39; EOF cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: homenet spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;bridge0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;static\u0026#34;, \u0026#34;addresses\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;192.168.50.204/24\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;192.168.50.1\u0026#34; } ], \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;192.168.50.1\u0026#34; } ], \u0026#34;dns\u0026#34;: { \u0026#34;nameservers\u0026#34;: [\u0026#34;192.168.50.1\u0026#34;, \u0026#34;1.1.1.1\u0026#34;] } } }\u0026#39; EOF NAD diagram graph TD NAD_Choice{{\u0026#34;Network Attachment Definition\u0026lt;br\u0026gt;(homenet)\u0026#34;}} subgraph \u0026#34;Option 1: Dynamic IP\u0026#34; DHCP[\u0026#34;IPAM: DHCP\u0026#34;] DHCP_Config[(\u0026#34;ipam: { type: dhcp }\u0026#34;)] NAD_Choice --\u0026gt;|Selects| DHCP DHCP --\u0026gt;|Uses CNI Plugin| DHCP_Config end subgraph \u0026#34;Option 2: Static IP\u0026#34; Static[\u0026#34;IPAM: Static\u0026#34;] Static_Config[(\u0026#34;ipam: { type: static, addresses: [...] }\u0026#34;)] NAD_Choice --\u0026gt;|Selects| Static Static --\u0026gt;|Specifies configuration| Static_Config end style NAD_Choice fill:#fff,stroke:#333,stroke-width:2px 9. Create VirtualMachine Deploy Fedora VM with homenet interface.\ncat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: fedora-dv-bridge labels: kubevirt.io/os: linux spec: runStrategy: Always template: metadata: labels: kubevirt.io/domain: vm spec: domain: cpu: cores: 1 devices: disks: - name: disk0 disk: bus: virtio - name: cloudinitdisk cdrom: bus: sata readonly: true interfaces: - name: homenet bridge: {} model: virtio machine: type: q35 resources: requests: memory: 2048M networks: - name: homenet multus: networkName: homenet volumes: - name: disk0 persistentVolumeClaim: claimName: fedora-dv-42 - name: cloudinitdisk cloudInitNoCloud: userData: | #cloud-config hostname: fedora-dv-bridge ssh_pwauth: True password: fedora chpasswd: {expire: False} runcmd: - dnf install -y qemu-guest-agent cockpit - systemctl enable qemu-guest-agent - systemctl enable --now cockpit.socket - systemctl start qemu-guest-agent cockpit EOF üéâ Congratulations! Your KubeVirt VM now has a LAN IP, can SSH in, use Cockpit, and interact with home network devices seamlessly.\n","date":"23 August, 2025","id":0,"permalink":"/posts/kubevirt_bridge/","summary":"Kubevirt VM connect to the cluster CNI (Flannel, Calico, etc), which is perfect for pod-to-pod communication but limits VMs that need full LAN access. We will address that by attaching it to the bridge network on host.","tags":"kubernetes multus kubevirt networking","title":"Setup Bridge Network for a Kubevirt VM"},{"content":"1. Introduction: What is Home Assistant? Home Assistant is a powerful, open-source home automation platform that puts local control and privacy first. It can be run on various devices, from single-board computers like the Raspberry Pi to virtual machines and containers. It\u0026rsquo;s a great tool for anyone looking to automate their home without relying on big tech companies, giving you complete control over your smart devices.\n2. Installation Methods Home Assistant offers a few different ways to get started, but two of the most common are:\nHome Assistant Operating System (HaOS): This is the recommended and most popular installation method. HaOS is a lightweight, embedded operating system designed specifically to run Home Assistant and its ecosystem. It\u0026rsquo;s easy to install on a dedicated device like a Home Assistant Green, a Raspberry Pi, or a virtual machine. This method gives you access to the full Home Assistant experience, including the convenience of add-ons, which are pre-packaged applications that extend its functionality.\nHome Assistant Container: This method is for more advanced users who want to run Home Assistant within a container environment (like Docker or Podman). You\u0026rsquo;re responsible for managing the underlying operating system and the container yourself. While this offers flexibility, it comes with a trade-off: you don\u0026rsquo;t get access to the official add-ons.\n3. HaOS on KubeVirt: The Best of Both Worlds If you\u0026rsquo;re already running a Kubernetes homelab and prefer the flexibility of containerized applications but still want the convenience and features of the full HaOS experience, KubeVirt is the perfect solution. KubeVirt is a Kubernetes add-on that lets you run traditional virtual machines (VMs) alongside your container workloads. This means you can run the full HaOS as a VM right inside your Kubernetes cluster, giving you a powerful, unified platform for both your containers and your home automation.\n4. Install KubeVirt and CDI The first step is to get KubeVirt and its dependencies up and running in your cluster. We will also install the Containerized Data Importer (CDI), which is essential for importing the virtual machine disk image into Kubernetes.\nInstall Kubevirt \u0026amp; CDI Install Kubevirt # Install KuberVirt export KUBEVIRT_VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name) echo $KUBEVIRT_VERSION kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-operator.yaml kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$KUBEVIRT_VERSION/kubevirt-cr.yaml kubectl get pods -n kubevirt # Install Containerized Data Importer (CDI) export CDI_VERSION=$(curl -s https://api.github.com/repos/kubevirt/containerized-data-importer/releases/latest | jq -r .tag_name) echo $CDI_VERSION kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-operator.yaml kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$CDI_VERSION/cdi-cr.yaml kubectl get pods -n cdi # kubectl -n cdi port-forward svc/cdi-uploadproxy 8443:443 ‚≠êÔ∏è NOTE: Steps 5-7 are about creating your own HaOS containerdisks. If you are happy to use my image based on HomeAssistant 16.1 then go to step-8 directly.\nIn case you are interested. Here are the official containerdisk images for various OS\n5. Download the HaOS image # Download the HaOS qcow2 image wget https://github.com/home-assistant/operating-system/releases/download/16.1/haos_ova-16.1.qcow2.xz # unzip xz -dk haos_ova-16.1.qcow2.xz 6. Convert the image Convert the image into a foramt that is understood by Kubevirt\nsudo dnf install podman libguestfs-tools guestfs-tools -y # machine-id gave error # virt-sysprep -a haos_ova-16.1.qcow2 --operations machine-id,bash-history,logfiles,tmp-files,net-hostname,net-hwaddr Image_name=haos_ova-16.1.qcow2 # Make Golden Image by removing unique identifiers and temporary files from the image. virt-sysprep -a $Image_name --operations bash-history,logfiles,tmp-files,net-hostname,net-hwaddr # Conpress image qemu-img convert -c -O qcow2 $Image_name \u0026#34;$Image_name-containerimage.qcow2\u0026#34; 7. Build the image and push to your repo # Create your Containerfile tee Containerfile \u0026gt; /dev/null \u0026lt;\u0026lt;EOL FROM kubevirt/container-disk-v1alpha ADD $Image_name-containerimage.qcow2 /disk/ EOL # Login to your repo podman login quay.io -u arslankhanali -p \u0026lt;\u0026gt; # Make sure `homeassistant` repo exists and it is public # Buld the image podman build -t quay.io/arslankhanali/homeassistant:v1 . # Push the image podman push quay.io/arslankhanali/homeassistant:v1 8. Deploy the VM on Kubevirt I will deploy in the namespace vm\nkubectl create ns vm kubectl config set-context --current --namespace=vm cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: name: haos namespace: vm # \u0026lt;-- Change as per need spec: domain: resources: requests: memory: 2048Mi cpu: 1 limits: memory: 4096Mi cpu: 2 devices: disks: - name: containerdisk disk: bus: virtio rng: {} firmware: bootloader: efi: secureBoot: false # ‚úÖ disable SecureBoot to avoid SMM requirement terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: quay.io/arslankhanali/homeassistant:v1 # \u0026lt;-- Change as per need --- apiVersion: v1 kind: Service metadata: name: haos namespace: vm spec: type: NodePort selector: vmi: haos ports: - name: haos-ui port: 8123 protocol: TCP targetPort: 8123 nodePort: 30003 # \u0026lt;-- Change as per need - Will be random it not set EOF 9. Console and port forward virtctl console haos # Password is ususlly set in cloud init inside vmi yaml 10. Console and port forward virtctl port-forward vmi/haos 8123:8123 curl -kI http://localhost:8123/onboarding.html curl -kI http://192.168.50.200:30003 Delete oc delete -f vm-haos.yaml oc delete vmi haos oc delete svc haos References:\nhttps://github.com/ormergi/vm-image-builder/tree/main?tab=readme-ov-file ","date":"17 August, 2025","id":1,"permalink":"/posts/featured/haos-on-kubevirt/","summary":"Home Assistant is a powerful, open-source home automation platform that puts local control and privacy first. It can be run on various devices, from single-board computers like the Raspberry Pi to virtual machines and containers. It\u0026rsquo;s a great tool for anyone looking to automate their home without relying on big tech companies, giving you complete control over your smart devices.","tags":"kubernetes podman","title":"Home Assistant VM on Kubevirt"},{"content":"0. General logins I used with my K3s email. : neogeo@gmail.com username. : neo password 1 : geo password 2 : Admin@123 1. Login to K3s scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config sed -i \u0026#39;\u0026#39; \u0026#39;s|server: https://127\\.0\\.0\\.1:6443|server: https://192.168.50.200:6443|\u0026#39; ~/k3s-config export KUBECONFIG=~/k3s-config oc get all -A 2. Argo oc get secret -n argocd argocd-initial-admin-secret -o jsonpath=\u0026#39;{.data.password}\u0026#39; | base64 -d rfgyTgvrj3RzWee1 3. Headlamp oc get secret headlamp-admin-secret-token -n headlamp -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Bearer token:\neyJhbGciOiJSUzI1NiIsImtpZCI6Ik9QLUlRd0RDOEdaeXFyZ3g0dnVkT1V3RzNIN0oweS1SODVaVDhza0d4RkEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJoZWFkbGFtcCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJoZWFkbGFtcC1hZG1pbi1zZWNyZXQtdG9rZW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiaGVhZGxhbXAtYWRtaW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2YWIyMmQ2ZC1kZmVmLTQ5NzAtODI3MS1jNWU1NjA3NjYyMTciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6aGVhZGxhbXA6aGVhZGxhbXAtYWRtaW4ifQ.eZXZsTH_foaS2VKJuwz8Bj8U337xX6v7KRJCb9ZcQ9TYhbd2CeW9Q9VPIEv6lAEJYbBf_C-RTxnLsszS_lNHPk0sOnTMt6v6bb7qHKP-2uoYrLcGV0zBg-b1QLN2-fcYJEXFE_D_qbkLrclHxzmG-m4xtf2CFJPz-z4PxvTGYdsP1QONTjcdXqdEOSeFIbFSpvtgQ2NRHMOmIV4ANc8jccb_AEhGiFLnSW_6G1TXkgNFXhnUpwR4o42nV3V1_9W-qLUoSwqy7Owh6lfl3SErxy8_IaHf8JMJykhlEDhZQGzXcCIc3tQPtoVsJFwYWfMWmflh16aUmJiUmKKfDrmnqA 4. AWX oc get secret -n awx-operator awx-demo-admin-password -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d wDhnfcMYlNzrGqDkZVUFmq03tQxcpWLt 5. Gitea neo Admin@123 6. Grafana oc get secret -n grafana grafana -o jsonpath=\u0026#39;{.data.admin-password}\u0026#39; | base64 -d T6zGt8FwF2cKUYa9o88uqCOYxE0DE6a2Gpc3qosm 6. Harbor oc get secrets -n harbor harbor-core -o jsonpath=\u0026#39;{.data.HARBOR_ADMIN_PASSWORD}\u0026#39; | base64 -d Admin@123 7. 8. 9. 10. ","date":"15 August, 2025","id":2,"permalink":"/hidden/k3s-logins/","summary":"Bearer token:","tags":"kubernetes podman","title":"K3s Logins"},{"content":"Notes REMEMBER to run kubectl and kustomize apply commands from the correct Namespace/Project! Helm will fuck up otherwise Resources Keep requests.cpu and requests.mem very low. So not a lot of resource is dedicated to the workload. No point giving 512Mi to a workload that is only using 100Mi. Do not put any limits.cpu and limits.mem. Pods crash saying OOM (out of memory) i.e. prometheus Only set it if you fear that a spike from this workload will overwhelm the node. Replicas Keep them to 1 ERROR: _non_namespaceable_\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;olm\u0026quot;}}] error: no objects passed to apply Remove namespace when resource has its own namespace e.g. olm Always use ClusterIP and expose via Ingress. LoadBalancer and NodePort will clash with host ports service: type: ClusterIP When you run the kustomize build or helm commands, it will download the chart in the directory. You can get the default values.yaml file in it. It is always a good idea to search for terms like enabled replicas resources ingress There should be only one storage class local-path is faster - so keep it default - also you might have to delete longhorn , it would mess up everything Use longhorn when a pod needs RWX storage # Check oc get storageclass # Remove default oc annotate storageclass longhorn storageclass.kubernetes.io/is-default-class- # Make default oc annotate storageclass local-path storageclass.kubernetes.io/is-default-class=true Cert-manager uses a webhook to validate resources like Certificate and Issuer before they are stored in etcd. Remove it in testing/dev env oc patch validatingwebhookconfiguration cert-manager-webhook --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/webhooks/0/failurePolicy\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;Ignore\u0026quot;}]' Homepage Annotations annotations: gethomepage.dev/enabled: \u0026#34;true\u0026#34; gethomepage.dev/name: \u0026lt;\u0026gt; gethomepage.dev/description: \u0026lt;\u0026gt; gethomepage.dev/group: Cluster Management | Developer Tools | Storage | Security | Monitoring | Server gethomepage.dev/icon: \u0026lt;\u0026gt;.png # \u0026lt;-- https://github.com/homarr-labs/dashboard-icons/tree/main/png # e.g. \u0026#34;app.kubernetes.io/name: longhorn\u0026#34; , This provides running status in homepage UI gethomepage.dev/pod-selector: |- app.kubernetes.io/name in ( longhorn ) gethomepage.dev/weight: \u0026#39;0\u0026#39; 1. Create ns=grafana oc create ns $ns oc project $ns # For Kustomize with Helm kustomize build ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ --enable-helm | oc apply -f - # For Kustomize oc apply -k ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ # For a simple Yaml manifest oc apply -f ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ingress.yaml 2. Verify ns=grafana oc get all -A oc get all curl -Ik --resolve $ns.node1:443:192.168.50.200 https://$ns.node1 # after /etc/hosts file is updated curl -Ik https://$ns.node1 # Node: See all requests and limits for cpu and mem oc describe nodes node1 2.1 Logs ns=grafana oc project $ns oc logs pods/ oc describe pods/ 2.2 Use --previous flag for why the pods/container crashed oc logs pods/prometheus-server-7ffd965767-jhdpc -c prometheus-server --previous\n3. Stop oc scale deployment --all --replicas=0 -n grafana oc scale deployment --all --replicas=1 -n grafana 4. Delete all ns=grafana oc project $ns kustomize build ~/Codes/homelab-kubernetes/apps/$ns/overlays/k3s/ --enable-helm | oc delete -f - oc delete --all all -n $ns oc delete --all pods -n $ns --force --grace-period=0 oc delete --all pvc -n $ns oc delete ns $ns 4.1. Delete pods Use --grace-period=0 --force\noc delete --all pods --force --grace-period=0 -n $ns 4.2. Delete: Kube proxy method e.g. Delete PVC\nCreate namespace for it again if deleted delete any webhooks start oc proxy in another terminal Run below: remember to change and oc proxy # Delete Namespace ns=cattle-system curl -X PATCH http://127.0.0.1:8001/api/v1/namespaces/$ns \\ -H \u0026#34;Content-Type: application/json-patch+json\u0026#34; \\ --data \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/finalizers\u0026#34;}]\u0026#39; 4.3. Delete: Remove Finalizer e.g. Delete namespace\nns=grafana oc get ns $ns -o json | jq \u0026#39;.spec.finalizers=[]\u0026#39; | oc replace --raw /api/v1/namespaces/$ns/finalize -f - 5. Exec in container in a pod oc exec -it \u0026lt;pod\u0026gt; -c \u0026lt;container\u0026gt; -- sh 6. Ingresss K3s is bound to 192.168.50.200 IP - and podman uses 192.168.50.100 IP for Traefik.\nkubectl logs -n kube-system deploy/traefik # Get Traefik version kubectl -n kube-system get deploy traefik -o jsonpath=\u0026#39;{.spec.template.spec.containers[0].image}\u0026#39; oc -n kube-system get svc traefik # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # traefik LoadBalancer 10.43.148.201 192.168.50.200 80:30854/TCP,443:31067/TCP 3d1h # No other svc will have EXTERNAL-IP. because they are only exposed via Ingress 6.1. DNS nslookup jellyseerr.ak Server: 192.168.50.100 Address: 192.168.50.100#53 Name: jellyseerr.ak Address: 192.168.50.100 nslookup jellyseerr.node1 Server: 192.168.50.100 Address: 192.168.50.100#53 Name: jellyseerr.node1 Address: 192.168.50.200 7. ERROR: Failed to allocate directory watch: Too many open files # Check Limits ssh node1 ulimit -n 1024 cat /proc/$(pgrep -f kubelet)/limits | grep \u0026#34;open files\u0026#34; Max open files 1048576 1048576 files cat /proc/sys/fs/inotify/max_user_watches cat /proc/sys/fs/inotify/max_user_instances 122145 128 # Increase limits: Temporarily sudo sysctl -w fs.inotify.max_user_watches=524288 sudo sysctl -w fs.inotify.max_user_instances=1024 # Restart all sudo systemctl restart k3s oc delete pod -n kubevirt -l kubevirt.io=virt-handler # Increase limits: Permanently (survives reboot) echo \u0026#34;fs.inotify.max_user_watches=524288\u0026#34; | sudo tee -a /etc/sysctl.conf echo \u0026#34;fs.inotify.max_user_instances=1024\u0026#34; | sudo tee -a /etc/sysctl.conf sudo sysctl -p 8. Check version of Helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm search repo prometheus-community/prometheus --versions helm search repo prometheus-community/prometheus --versions | grep \u0026#39;prometheus-community/prometheus \u0026#39; ","date":"15 August, 2025","id":3,"permalink":"/hidden/k3s/","summary":"oc logs pods/prometheus-server-7ffd965767-jhdpc -c prometheus-server --previous","tags":"kubernetes podman","title":"K3s"},{"content":"Introduction Welcome to another installment in our AI series! Today, we\u0026rsquo;re going to set up Open WebUI, a powerful, self-hosted web interface for interacting with various large language models (LLMs). This tool provides a beautiful user experience similar to ChatGPT but on your own terms. We\u0026rsquo;ll be using Podman to containerize Open WebUI, making it a breeze to manage.\nWhy Open WebUI? Open WebUI acts as a central hub for your LLMs. It can connect to local models running on Ollama or LM Studio, as well as remote services like MaaS (Model as a Service). This flexibility allows you to experiment with different models and backends all from a single, consistent interface.\nStep 1: Pulling the Open WebUI Image First, we need to pull the Open WebUI container image from its public registry. Podman makes this simple and efficient.\n# This command fetches the latest main version of the Open WebUI image from GitHub\u0026#39;s container registry. podman pull ghcr.io/open-webui/open-webui:main Step 2: Running the Container Now, let\u0026rsquo;s run the container with all the necessary configurations. We\u0026rsquo;ll map a port, set environment variables, and create a persistent volume for data.\npodman run -d \\ --name open-webui \\ -p 3001:8080 \\ -e OLLAMA_BASE_URL=http://192.168.50.50:11434 \\ # If you have Ollama -e OPENAI_API_KEY=dummykey \\ -v open-webui:/app/backend/data:z \\ --restart=always \\ ghcr.io/open-webui/open-webui:main Step 3: Accessing Open WebUI Once the container is running, you can access the web interface by navigating to your host machine\u0026rsquo;s IP address and the mapped port.\nhttp://192.168.50.200:3001\nInitial Setup The first time you visit the page, you\u0026rsquo;ll be prompted to create a user account. After creating your account, you can log in and access the admin settings to configure your LLM connections.\nStep 4: Configuring Connections In the Open WebUI admin panel, you can add various connections to different LLM services.\nAdd connections: Settings -\u0026gt; Admin Panel -\u0026gt; Conenctions http://192.168.50.200:3001/admin/settings/connections e.g. Notice URL format\nOLLAMA: http://192.168.50.50:11434 # \u0026lt; \u0026mdash; From Ollama CLI OPENAI: http://192.168.50.50:1234/v1 # \u0026lt; \u0026mdash; From LMStudio\n‚ö†Ô∏è WARNING: Remember to use http or https as appropriate for your connection. Always use the correct port and make sure your firewall is configured to allow traffic on these ports.\nYou now have a fully functional Open WebUI instance running on your server, giving you a powerful tool to manage and interact with all your LLMs in one place!\n","date":"8 August, 2025","id":4,"permalink":"/posts/ai/openwebui-on-podman/","summary":"Welcome to another installment in our AI series! Today, we\u0026rsquo;re going to set up Open WebUI, a powerful, self-hosted web interface for interacting with various large language models (LLMs). This tool provides a beautiful user experience similar to ChatGPT but on your own terms. We\u0026rsquo;ll be using Podman to containerize Open WebUI, making it a breeze to manage.","tags":"openwebui podman ai ollama llm","title":"Open WebUI with Podman"},{"content":"Introduction Today, we\u0026rsquo;re going to deploy Dify, an AI application development platform, using Podman and podman-compose. Dify provides a powerful, visual way to build and manage AI applications, and deploying it on your local fedora server gives you full control.\nPrerequisites Before we start, make sure you have Podman and podman-compose installed. If you haven\u0026rsquo;t, you can refer to my previous blog post on how to set up Podman.\nüü¢ NOTE: This guide assumes you are running on a RHEL-based system with SELinux enabled, which is a common setup for fedora environments. The steps for SELinux are crucial for a smooth deployment.\nStep 1: Clone the Dify Repository First, we need to clone the Dify repository from GitHub. This will give us the necessary configuration files to deploy the application.\ngit clone [https://github.com/langgenius/dify.git](https://github.com/langgenius/dify.git) cd ~/dify/docker cp .env.example .env After cloning, we move into the docker directory and copy the example environment file to .env. This file contains the configuration variables for our Dify deployment.\nStep 2: Configure SELinux For systems with SELinux enabled, you\u0026rsquo;ll need to adjust the security context of the Dify volumes. This step prevents permission errors when Podman tries to access the files. By setting the context correctly, we avoid having to use the :z flag in our compose file.\nchcon -Rt container_file_t /home/neo/dify/docker/ ls -Z Step 3: Modify the docker-compose.yaml We need to make a couple of small but important changes to the docker-compose.yaml file to ensure Dify runs smoothly with Podman.\nvim docker-compose.yaml Change 1: Add Default Networks Add a default network to the end of the file. This ensures all services that don\u0026rsquo;t have an explicit network defined can communicate.\n# Go to end of file `shift+g` networks: ssrf_proxy_network: driver: bridge internal: true milvus: driver: bridge opensearch-net: driver: bridge internal: true default: driver: bridge Change 2: Ports Adjust the NGINX ports to avoid conflicts on your host. We\u0026rsquo;ll change the host ports from 80 and 443 to 980 and 9443 respectively. Remember to also change the corresponding variables in the .env file you copied earlier.\n# Search `/NGINX_PORT` ports: - \u0026#39;980:${NGINX_PORT:-80}\u0026#39; - \u0026#39;9443:${NGINX_SSL_PORT:-443}\u0026#39; Might also have to change nginx port in .env file 980 and 9443\nStep 4: Deploy and Verify Now we\u0026rsquo;re ready to deploy Dify using podman-compose.\nStart the containers\npodman-compose up -d podman pod logs pod_docker -f Step 5: Open Ports sudo firewall-cmd --add-port=980/tcp --permanent sudo firewall-cmd --reload Step 6: Test and Access podman pod logs pod_docker -f curl http://localhost:980/apps curl http://192.168.50.200:980/apps That\u0026rsquo;s it! You now have Dify running on your server. You can start building AI applications and experimenting with the platform.\n","date":"8 August, 2025","id":5,"permalink":"/posts/ai/dify-on-podman/","summary":"Today, we\u0026rsquo;re going to deploy Dify, an AI application development platform, using Podman and podman-compose. Dify provides a powerful, visual way to build and manage AI applications, and deploying it on your local fedora server gives you full control.","tags":"dify podman ai langgenius docker-compose","title":"Dify: A Podman Deployment Guide"},{"content":"Part of series on Arr-stack:\nArr-Stack Installation Arr-Stack Configuration Introduction The \u0026ldquo;arr stack\u0026rdquo; is a popular collection of applications used for managing media libraries. By running these applications as rootless containers with Podman, you can enhance security by preventing the containers from having root privileges on the host machine.\nThis guide will walk you through setting up the arr stack, including Jellyfin, Jellyseerr, Prowlarr, Sonarr, Radarr, and Transmission as rootless containers. We will cover directory preparation, mounting an external hard drive, and configuring the services using a podman-compose.yaml file.\nWhat is the \u0026ldquo;arr stack\u0026rdquo;? üóÉÔ∏è ‚ö†Ô∏è Check legal obligations where you live The arr stack is a suite of applications that work together to automate the process of finding, downloading, and organizing movies, TV shows, and other media.\nApplication Purpose Jellyfin A free software media system that lets you control the management and streaming of your media. Jellyseerr A request management and media discovery tool for Jellyfin and other \u0026lsquo;arr\u0026rsquo; apps. Prowlarr An indexer manager for the other \u0026lsquo;arr\u0026rsquo; apps, providing a centralized way to manage your indexers (sources for media). Sonarr Manages and automates the downloading of TV shows. Radarr Manages and automates the downloading of movies. Transmission A lightweight BitTorrent client used for downloading the media files. Prerequisites Before you start, make sure you have Podman and podman-compose installed. For Podman, you can often find it in your distribution\u0026rsquo;s package manager. For podman-compose, it\u0026rsquo;s a Python script that you can install with pip.\n# Example for a Fedora-based system sudo dnf install podman podman-docker # install podman-compose pip install podman-compose Setup Directories and External Drive First, we need to create the necessary directory structure for our applications and media files. This setup ensures that all your data is organized and easily accessible to the containers.\n# Set up a base directory for the arr stack dir=~/arr-stack mkdir -p $dir cd $dir # Create subdirectories for media and application downloads mkdir -p $dir/tvshows mkdir -p $dir/movies mkdir -p $dir/books mkdir -p $dir/transmission/downloads/complete/radarr mkdir -p $dir/transmission/downloads/complete/tv-sonarr mkdir -p $dir/transmission/downloads/incomplete/ mkdir -p $dir/transmission/watch mkdir -p $dir/prowler Mount an External Hard Drive If you have a large media library, it\u0026rsquo;s best to store it on an external hard drive. This section shows you how to mount the drive manually or automatically.\nManual Mount To manually mount your external drive, first find its device name and then mount it with the correct permissions. The uid=1000 and gid=1000 options ensure the drive is owned by your user, which is crucial for rootless containers. The context option is important for SELinux.\n# First, create a mount point mkdir -p $dir/external-drive # List block devices to find your external drive (e.g., /dev/sda1) lsblk # NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS # sda 8:0 0 1.8T 0 disk # ‚îî‚îÄsda1 8:1 0 1.8T 0 part ~/arr-stack/external-drive # Manually mount the drive. Replace /dev/sda1 with your device. sudo mount -t exfat -o users,noexec,uid=1000,gid=1000,context=\u0026#34;system_u:object_r:svirt_sandbox_file_t:s0\u0026#34; /dev/sda1 ~/arr-stack/external-drive Automatic Mount with /etc/fstab To have your drive automatically mount at startup, you need to add an entry to /etc/fstab.\nFind the UUID: The UUID is a unique identifier for your drive. # Find the UUID of your external drive sudo blkid /dev/sda1 Add to /etc/fstab: Append a new line to /etc/fstab using tee -a to avoid overwriting the file. # Path to your UUID and path echo \u0026#39;UUID=ECA3-DE06 /home/neo/arr-stack/external-drive exfat rw,users,noexec,nofail,async,auto,uid=1000,gid=1000,umask=0022,context=system_u:object_r:svirt_sandbox_file_t:s0 0 0\u0026#39; | sudo tee -a /etc/fstab Reload and Mount: Apply the new fstab entry. systemctl daemon-reload sudo mount -a ls -la arr-stack Check permission: Make sure your user (non-root) owns the folder ls -la ~/arr-stack You can verify the mount by running ls -la ~/arr-stack/external-drive.\nNote: The umount command is used to unmount the drive.\nsudo umount ~/arr-stack/external-drive The podman-compose.yaml File This file defines all the services in your arr stack. Each service is a container with specific configurations like image, port mappings, and volume mounts. A crucial aspect here is the volume mapping (- /path/on/host:/path/in/container:z). The :z option is essential for Podman to handle SELinux permissions correctly in a rootless environment.\nLet\u0026rsquo;s briefly explain some key configurations:\nimage: Specifies the container image to use. container_name: Sets a friendly name for the container. environment: Defines environment variables, such as PUID and PGID for user and group IDs, and TZ for the timezone. volumes: Connects host directories to container directories. This is how the applications can access your media files. ports: Maps container ports to host ports, allowing you to access the web UIs. labels: Used for services like Traefik to automatically configure reverse proxying and SSL/TLS. Some services, like Sonarr, Radarr, and Transmission, are run with PUID=0 and PGID=0 inside the container. This is a common practice for these specific images to avoid permission issues when writing to the /downloads directory, which is shared among them. The rootless Podman environment handles this by mapping the container\u0026rsquo;s root user (UID 0) to your host user (UID 1000).\nYou can ignore traefik section\ntee ~/arr-stack/podman-compose.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#34;3\u0026#34; services: # https://docs.linuxserver.io/images/docker-jellyfin/ jellyfin: image: docker.io/jellyfin/jellyfin:latest container_name: jellyfin environment: - TZ=Australia/Sydney volumes: - jellyfin-config:/config:z - jellyfin-cache:/cache:z - ~/arr-stack/external-drive:/media_external-drive:z - ~/arr-stack/movies:/media_movies:z - ~/arr-stack/tvshows:/media_tvshows:z - ~/arr-stack/prowler:/media_prowler:z - ~/arr-stack/transmission:/media_transmission:z ports: - 8096:8096 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.rule=Host(`jellyfin.ak`)\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.jellyfin.loadbalancer.server.port=8096\u0026#34; - \u0026#34;traefik.http.routers.jellyfin.tls=true\u0026#34; # https://docs.jellyseerr.dev/getting-started/docker?docker-methods=docker-compose jellyseerr: image: docker.io/fallenbagel/jellyseerr:latest container_name: jellyseerr environment: - PUID=1000 - PGID=1000 - LOG_LEVEL=debug - TZ=Australia/Sydney ports: - 5055:5055 volumes: - jellyseerr-config:/app/config:z restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.rule=Host(`jellyseerr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.jellyseerr.loadbalancer.server.port=5055\u0026#34; - \u0026#34;traefik.http.routers.jellyseerr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-prowlarr/#application-setup prowlarr: image: lscr.io/linuxserver/prowlarr:latest container_name: prowlarr environment: - PUID=1000 - PGID=1000 - TZ=Australia/Sydney volumes: - prowlarr-config:/config:z - ~/arr-stack/prowler/downloads:/downloads:z ports: - 9696:9696 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.rule=Host(`prowlarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.prowlarr.loadbalancer.server.port=9696\u0026#34; - \u0026#34;traefik.http.routers.prowlarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-sonarr/ sonarr: image: lscr.io/linuxserver/sonarr:latest container_name: sonarr environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - sonarr-config:/config:z - ~/arr-stack/tvshows:/tvshows:z - ~/arr-stack/transmission/downloads:/downloads:z ports: - 8989:8989 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.sonarr.rule=Host(`sonarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.sonarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.sonarr.loadbalancer.server.port=8989\u0026#34; - \u0026#34;traefik.http.routers.sonarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-radarr/ radarr: image: lscr.io/linuxserver/radarr:latest container_name: radarr environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - radarr-config:/config:z - ~/arr-stack/movies:/movies:z - ~/arr-stack/transmission/downloads:/downloads:z ports: - 7878:7878 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.radarr.rule=Host(`radarr.ak`)\u0026#34; - \u0026#34;traefik.http.routers.radarr.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.radarr.loadbalancer.server.port=7878\u0026#34; - \u0026#34;traefik.http.routers.radarr.tls=true\u0026#34; # https://docs.linuxserver.io/images/docker-transmission/ transmission: image: lscr.io/linuxserver/transmission:latest container_name: transmission environment: - PUID=0 - PGID=0 - TZ=Australia/Sydney volumes: - transmission-config:/config:z - ~/arr-stack/transmission/downloads:/downloads:z - ~/arr-stack/transmission/watch:/watch:z ports: - 9091:9091 - 51413:51413 - 51413:51413/udp restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.transmission.rule=Host(`transmission.ak`)\u0026#34; - \u0026#34;traefik.http.routers.transmission.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.transmission.loadbalancer.server.port=9091\u0026#34; - \u0026#34;traefik.http.routers.transmission.tls=true\u0026#34; # It always run as UID=1111(flare_bypasser) # https://github.com/yoori/flare-bypasser flare-bypasser: image: ghcr.io/yoori/flare-bypasser:latest container_name: flare-bypasser environment: - PUID=1000 - PGID=1000 - TZ=Australia/Sydney volumes: - flare-bypasser-config:/config:z ports: - 8191:8080 restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.rule=Host(`flare-bypasser.ak`)\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.flare-bypasser.loadbalancer.server.port=8191\u0026#34; - \u0026#34;traefik.http.routers.flare-bypasser.tls=true\u0026#34; volumes: jellyfin-config: jellyfin-cache: sonarr-config: radarr-config: jellyseerr-config: prowlarr-config: transmission-config: flare-bypasser-config: EOL Deploying the Stack Once the podman-compose.yaml file is created, you can deploy the entire stack with a single command.\n# Make sure you\u0026#39;re in the ~/arr-stack directory cd ~/arr-stack # Deploy the services in detached mode podman-compose -f podman-compose.yaml up -d --force-recreate The --force-recreate flag ensures that if any configuration changes have been made, the containers will be rebuilt from scratch.\nTo check if the containers are running and to verify user permissions, you can use the following commands:\npodman ps # To check the user inside a container podman exec -it sonarr whoami This will show you the user that the container is running as, which should be abc (uid 1000) for most services, or root (uid 0) for the PUID=0 services, which is correctly mapped to your user.\nService Address Jellyfin http://192.168.50.200:8096 Jellyserr http://192.168.50.200:5055 Prowlarr http://192.168.50.200:9696 Sonarr http://192.168.50.200:8989 Radarr http://192.168.50.200:7878 Transmission http://192.168.50.200:9091 flare-bypasser http://192.168.50.200:8191 Enable Auto start # Remove old service files - That automatically start containers containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload # Create new service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload Update all cd ~/arr-stack echo \u0026#34;Pulling all images used by running containers...\u0026#34; for img in $(podman ps --format \u0026#34;{{.Image}}\u0026#34; | sort -u); do echo \u0026#34;Pulling: $img\u0026#34; podman pull \u0026#34;$img\u0026#34; done # Remove old service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload # Re-generate and enable new service files containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for name in \u0026#34;${containers[@]}\u0026#34;; do # This command generates a new systemd service file for the container podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files # This moves the newly generated service file to the correct systemd user directory mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ # Enable and start the new service systemctl --user enable \u0026#34;container-$name.service\u0026#34; systemctl --user start \u0026#34;container-$name.service\u0026#34; done # Reload the systemd manager configuration systemctl --user daemon-reload Remove all cd ~/arr-stack podman-compose down -v podman image prune containers=(prowlarr transmission flare-bypasser sonarr radarr jellyseerr jellyfin) for container in \u0026#34;${containers[@]}\u0026#34;; do rm ~/.config/systemd/user/container-$container.service done systemctl --user daemon-reload sudo umount ~/arr-stack/external-drive sudo nano /etc/fstab systemctl daemon-reload rm -rf ~/arr-stack ","date":"8 August, 2025","id":6,"permalink":"/hidden/podman-arr-stack/","summary":"Part of series on Arr-stack:","tags":"arr-stack podman homelab","title":"Arr stack - Installation"},{"content":"1. Introduction Podman is a daemonless container engine for running OCI containers.\nIt can run as root (rootful) or as a non-root user (rootless), each with different privileges and security implications.\nFeature Rootful Podman (Run as root) Rootless Podman (Run as non-root) Privileges Full root privileges on host system (sudo podman ...). Limited to user‚Äôs privileges (podman ...). Security Compromised container could gain root on host. Limited damage ‚Äî container root maps to non-root UID on host. Networking Can bind to privileged ports (\u0026lt;1024) directly. Uses slirp4netns, cannot bind \u0026lt;1024 without extra config. User ID Mapping Container root = Host root. Container root maps to non-root UID. 2. Running Rootless Do not use sudo to start containers.\nEnable lingering for your user:\nFor a rootless container to keep running after you\u0026rsquo;ve logged out, the Podman process itself needs to be managed by a system that persists. By enabling lingering, you allow the user\u0026rsquo;s systemd instance to continue running, which in turn can manage and keep Podman-related services alive.\nsudo loginctl enable-linger \u0026#34;$USER\u0026#34; Enable Podman socket for your user:\nThis command sets up and starts the Podman API socket for the current user.\nsystemctl --user enable --now podman.socket This creates a Podman socket at:\n/run/user/\u0026lt;UID\u0026gt;/podman/podman.sock (e.g., /run/user/1000/podman/podman.sock)\n3. Volume Mounting Default volume paths:\n# Rootful /var/lib/containers/storage/volumes/ # Rootless $HOME/.local/share/containers/storage/volumes/ 4. User Mapping Details Keeep - PUID=0 \u0026amp; - PGID=0 for containers in compose file. They will run as 0 inside container but 1000 on the host. i.e. rootless on host.\nüü¢ Make sure to run PODMAN without sudo Check UID mapping:\ncat /proc/$(pgrep -u \u0026#34;$USER\u0026#34; podman | head -n 1)/uid_map Example output:\nContainer UID Host UID Range Notes 0 1000 1 Container UID 0 is mapped to UID 1000 on host 1 524288 65536 Container UID 1 is mapped to UID 524288 on host 1000 525287 65536 Container UID 1000 is mapped to UID 525287 on host (524,288 + 999 = 525,287) 5. Auto restart: Systemd Integration (Rootless) Rootless service files live in:\n$HOME/.config/systemd/user\nGenerate and enable container services:\nmkdir -p ~/.config/systemd/user/ containers=($(podman ps --format \u0026#39;{{.Names}}\u0026#39;)) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; \\ --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload 6. Update All Containers for name in $(podman ps --format \u0026#39;{{.Names}}\u0026#39;); do image=$(podman inspect --format \u0026#39;{{.ImageName}}\u0026#39; \u0026#34;$name\u0026#34;) podman pull \u0026#34;$image\u0026#34; systemctl --user restart \u0026#34;container-$name.service\u0026#34; echo \u0026#34;$name updated and restarted.\u0026#34; done 7. Stop All Containers for name in $(podman ps --format \u0026#39;{{.Names}}\u0026#39;); do echo \u0026#34;Stopping $name...\u0026#34; podman stop \u0026#34;$name\u0026#34; done 8. List Container IPs for net in $(podman network ls --format \u0026#39;{{.Name}}\u0026#39;); do echo \u0026#34;Network: $net\u0026#34; podman network inspect $net | jq -r \u0026#39; .[0].containers | to_entries[] | \u0026#34;\\(.value.name) \\(.value.interfaces.eth0.subnets[0].ipnet // \u0026#34;\u0026#34;)\u0026#34; \u0026#39; | while read -r name ipcidr; do ip=${ipcidr%%/*} [ -z \u0026#34;$ip\u0026#34; ] \u0026amp;\u0026amp; ip=\u0026#34;(none)\u0026#34; echo -e \u0026#34;$name\\t$ip\u0026#34; done echo \u0026#34;\u0026#34; done 9. Closing Note Rootless Podman is more secure and works seamlessly with systemd for automated container management.\n","date":"8 August, 2025","id":7,"permalink":"/hidden/podman-101/","summary":"","tags":"podman","title":"Podman 101 - Always Run Rootless!"},{"content":"Introduction Setting up a reverse proxy is a crucial step in managing a homelab. It allows you to expose multiple services on your network, all under a single domain, with proper SSL/TLS encryption. Traefik is an excellent, modern, and easy-to-configure reverse proxy that integrates seamlessly with container orchestrators like Podman.\nThis guide walks you through setting up Traefik on Podman, complete with a self-signed wildcard certificate for local development. This setup is perfect for homelab environments where you need secure, accessible services without the hassle of public DNS records and paid certificates.\n1. Prerequisites and System Configuration Before we deploy Traefik, we need to configure our system to allow it to run properly. We\u0026rsquo;ll enable access to privileged ports and configure the Podman socket.\nSystem Setup First, let\u0026rsquo;s allow non-root users to bind to privileged ports (like 80 and 443).\nüö® DANGER: If you are running Pi-Hole as well then you already have minimum port set as 53. In that case do not run below cmd. It will break Pi-hole # Check lowest port. If port \u0026lt; 80 set then no need to run below cmds # cat /etc/sysctl.conf echo \u0026#39;net.ipv4.ip_unprivileged_port_start=80\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p Next, we\u0026rsquo;ll enable the Podman socket for our user. This is essential for Traefik to be able to detect and configure services running in other containers.\nsystemctl --user enable --now podman.socket Finally, we\u0026rsquo;ll open up the necessary ports on the firewall to ensure Traefik can receive traffic.\nsudo firewall-cmd --add-service={http,https} --permanent sudo firewall-cmd --reload 2. Directory Structure and Certificates To keep our configuration organized, we\u0026rsquo;ll create a dedicated directory for Traefik and generate a self-signed certificate. This certificate will be used for all our local services.\nDirectory Structure We\u0026rsquo;ll create a base directory for Traefik and two subdirectories: certs for our SSL certificates and dynamic for dynamic Traefik configurations.\nBASE_DIR=\u0026#34;~/traefik\u0026#34; mkdir -p \u0026#34;$BASE_DIR/certs\u0026#34; mkdir -p \u0026#34;$BASE_DIR/dynamic\u0026#34; cd \u0026#34;$BASE_DIR\u0026#34; Self-Signed Certificate Generation This script will generate a wildcard self-signed certificate for *.ak. This means any subdomain like homeassistant.ak or traefik.ak will be trusted by Traefik.\n# FOR DNS:*.ak domain=\u0026#34;.ak\u0026#34; if [ ! -f certs/local.crt ] \u0026amp;\u0026amp; [ ! -f certs/local.key ]; then echo \u0026#34;Generating wildcard self-signed certificate for *.$domain...\u0026#34; openssl req -x509 -newkey rsa:4096 -nodes -days 825 -sha256 \\ -keyout certs/local.key -out certs/local.crt \\ -subj \u0026#34;/C=PK/ST=Punjab/L=Lahore/O=HomeLab/OU=Development/CN=$domain\u0026#34; \\ -addext \u0026#34;subjectAltName=DNS:$domain,DNS:*.$domain\u0026#34; \\ -addext \u0026#34;basicConstraints=CA:FALSE\u0026#34; \\ -addext \u0026#34;keyUsage=keyEncipherment,dataEncipherment,digitalSignature\u0026#34; \\ -addext \u0026#34;extendedKeyUsage=serverAuth\u0026#34; \\ -addext \u0026#34;authorityKeyIdentifier=keyid,issuer\u0026#34; echo \u0026#34;‚úÖ Certificate generated:\u0026#34; openssl x509 -in certs/local.crt -noout -text | grep -A1 \u0026#34;Subject Alternative Name\u0026#34; else echo \u0026#34;‚ö†Ô∏è Local certificate already exists, skipping generation.\u0026#34; fi ‚ö†Ô∏è WARNING: For browsers to trust this certificate, you\u0026rsquo;ll need to manually import it into your system\u0026rsquo;s trust store.\n3. Traefik Configuration Files Traefik uses two types of configuration files: static and dynamic. The static configuration defines the core settings, while the dynamic configuration sets up routers, services, and middleware.\nStatic Configuration (traefik.yml) This file defines Traefik\u0026rsquo;s entry points (ports 80 and 443), enables the API dashboard, and configures the docker provider to monitor Podman containers. It also enables a file provider for dynamic configurations.\ntee ~/traefik/traefik.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: https scheme: https https: address: \u0026#34;:443\u0026#34; api: dashboard: true insecure: true providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: true file: directory: /etc/traefik/dynamic watch: true EOL Dynamic TLS Configuration (dynamic.yml) This file tells Traefik where to find the self-signed certificates we generated earlier.\ntee ~/traefik/dynamic/dynamic.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; tls: certificates: - certFile: /certs/local.crt keyFile: /certs/local.key EOL Dynamic Routers and Services To expose your services (like Home Assistant), you\u0026rsquo;ll create a separate dynamic configuration file. This file defines a router to match incoming traffic and a service to forward that traffic to the correct backend.\ntee ~/traefik/dynamic/homeassistant.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; http: routers: homeassistant: rule: \u0026#34;Host(`homeassistant.ak`)\u0026#34; service: homeassistant entryPoints: - https tls: {} middlewares: [] priority: 10 services: homeassistant: loadBalancer: servers: - url: \u0026#34;http://192.168.50.202:8123\u0026#34; EOL 4. Deploying with Podman Compose We\u0026rsquo;ll use a podman-compose.yml file to define and run our Traefik container.\npodman-compose.yml This file sets up the Traefik container, maps ports, and mounts the necessary configuration directories and the Podman socket. The labels on the traefik service configure Traefik\u0026rsquo;s own dashboard.\ntee /home/neo/traefik/podman-compose.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#34;3.3\u0026#34; services: traefik: image: docker.io/library/traefik:latest container_name: traefik security_opt: - label=type:container_runtime_t ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; - \u0026#34;8080:8080\u0026#34; volumes: - /run/user/1000/podman/podman.sock:/var/run/docker.sock:z - ./certs:/certs:ro,Z - ./traefik.yml:/etc/traefik/traefik.yml:ro,Z - ./dynamic:/etc/traefik/dynamic:ro,Z labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.traefik.rule=Host(`traefik.ak`)\u0026#34; - \u0026#34;traefik.http.routers.traefik.entrypoints=https\u0026#34; - \u0026#34;traefik.http.services.traefik.loadbalancer.server.port=8080\u0026#34; - \u0026#34;traefik.http.routers.traefik.tls=true\u0026#34; EOL Starting Traefik Now, you can start Traefik with the podman compose up command.\npodman compose -f ~/traefik/podman-compose.yml up -d --force-recreate üü¢ NOTE: You can access the Traefik dashboard at http://192.168.50.200:8080\nAuto start containers=(traefik) for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload This setup gives you a powerful and flexible reverse proxy for all your homelab services, secured with local certificates. You can easily add new services by simply creating a new configuration file in the dynamic directory. üöÄ\n","date":"8 August, 2025","id":8,"permalink":"/hidden/podman-traefik/","summary":"Setting up a reverse proxy is a crucial step in managing a homelab. It allows you to expose multiple services on your network, all under a single domain, with proper SSL/TLS encryption. Traefik is an excellent, modern, and easy-to-configure reverse proxy that integrates seamlessly with container orchestrators like Podman.","tags":"kubernetes podman traefik","title":"Traefik on Podman with Local Certificates"},{"content":"What Pi-hole Does Pi-hole is a network-level ad blocker that acts as a DNS sinkhole. Here\u0026rsquo;s how to run it securely on your Fedora-based homelab using Podman and Traefik, with a clean and idempotent setup.\nStep-by-Step Setup 1. Prepare System # Create working directory mkdir ~/pihole \u0026amp;\u0026amp; cd ~/pihole 2. Allow Binding to Port 53 (as non-root) echo \u0026#39;net.ipv4.ip_unprivileged_port_start=53\u0026#39; | sudo tee -a /etc/sysctl.conf sudo sysctl -p # Reloads kernel parameters from the config /etc/sysctl.conf 3. Create podman-compose.yml tee ~/pihole/podman-compose.yml \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; version: \u0026#39;3\u0026#39; services: pihole: container_name: pihole image: docker.io/pihole/pihole:latest restart: unless-stopped ports: - \u0026#34;192.168.50.100:53:53/tcp\u0026#34; # \u0026lt;--- Change - \u0026#34;192.168.50.100:53:53/udp\u0026#34; # \u0026lt;--- Change - \u0026#34;192.168.50.100:8099:80/tcp\u0026#34; # \u0026lt;--- Change environment: TZ: \u0026#34;Australia/Sydney\u0026#34; FTLCONF_webserver_api_password: \u0026#34;pihole\u0026#34; # \u0026lt;--- Change volumes: - pihole-etc:/etc/pihole - pihole-dnsmasq:/etc/dnsmasq.d cap_add: - NET_ADMIN dns: - 9.9.9.9 - 45.90.30.0 labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.pihole.rule=Host(`pihole.node1`)\u0026#34; # \u0026lt;--- Change - \u0026#34;traefik.http.routers.pihole.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.pihole.tls=true\u0026#34; - \u0026#34;traefik.http.services.pihole.loadbalancer.server.port=80\u0026#34; volumes: pihole-etc: pihole-dnsmasq: EOL 4. Deploy Pi-hole # Ensure clean state podman compose -f ~/pihole/podman-compose.yml down # Deploy podman compose -f ~/pihole/podman-compose.yml up -d --force-recreate Auto start # Allow the user\u0026#39;s user services (like systemd --user) to run even when not logged in sudo loginctl enable-linger \u0026#34;$USER\u0026#34; # Enable and start the Podman API socket for the current user, required for systemd + Podman integration systemctl --user enable --now podman.socket # Ensure the user systemd service directory exists mkdir -p ~/.config/systemd/user/ # Generate a systemd service unit file for the container containers=(pihole) # \u0026lt;--- Change to your container name for name in \u0026#34;${containers[@]}\u0026#34;; do podman update --restart=no \u0026#34;$name\u0026#34; podman generate systemd --name \u0026#34;$name\u0026#34; --restart-policy=always --container-prefix=container --files mv \u0026#34;container-$name.service\u0026#34; ~/.config/systemd/user/ systemctl --user enable \u0026#34;container-$name.service\u0026#34; # systemctl --user start \u0026#34;container-$name.service\u0026#34; done systemctl --user daemon-reload 5. Post-deploy Actions # Check HTTP response curl -kI http://192.168.50.100:8099/admin/login # Set or reset password sudo podman exec -it pihole pihole setpassword 6. Open Firewall for DNS sudo firewall-cmd --permanent --add-service=dns sudo firewall-cmd --reload 7. Access Service Address Pi-hole UI https://pihole.node1 (via Traefik) Direct IP http://192.168.50.100:8099/admin 8. DNS: Enter your local DNS entries in pihole So you don\u0026rsquo;t have to add them manually to each devices /etc/hosts file\npodman exec pihole sed -i \u0026#39;s/^ etc_dnsmasq_d *= *.*/ etc_dnsmasq_d=true/\u0026#39; /etc/pihole/pihole.toml # Create custom_hosts.conf in /etc/dnsmasq.d/ with specific entries # Add your hostnames below DOMAIN_SUFFIX=\u0026#34;.node1\u0026#34; IP=\u0026#34;192.168.50.100\u0026#34; HOSTS=( node1 homeassistant cockpit traefik pihole jellyfin jellyseerr prowlarr sonarr radarr transmission flare-bypasser readarr calibre-web calibre wazuh homepage kavita nextcloud hello test test1 test2 argocd k3s ) # address=/node1.localhost/192.168.50.100 for host in \u0026#34;${HOSTS[@]}\u0026#34;; do echo \u0026#34;address=/${host}${DOMAIN_SUFFIX}/${IP}\u0026#34; done | podman exec -i pihole tee /etc/dnsmasq.d/custom_hosts.conf \u0026gt; /dev/null Restart pihole pod\n# Verify the contents of custom_hosts.conf podman exec -it pihole cat /etc/dnsmasq.d/custom_hosts.conf podman exec -it pihole cat /etc/pihole/pihole.toml | grep etc_dnsmasq_d # Restart the pihole container to apply changes podman restart pihole Undo All\n# undo podman exec -it pihole rm /etc/dnsmasq.d/custom_hosts.conf podman exec pihole sed -i \u0026#39;s/^ etc_dnsmasq_d *= *.*/ etc_dnsmasq_d=false/\u0026#39; /etc/pihole/pihole.toml 9. Router Change Primary DNS in router to 192.168.50.100\n10. Summary You now have a self-hosted, containerized Pi-hole setup running under Podman, fronted by Traefik, and configured for secure DNS resolution across your homelab.\n11. Delete podman stop pihole podman rm pihole podman volume rm pihole-etc pihole-dnsmasq rm -rf ~/pihole podman rmi docker.io/pihole/pihole sudo sed -i \u0026#39;/net.ipv4.ip_unprivileged_port_start=53/d\u0026#39; /etc/sysctl.conf sudo sysctl -p sudo firewall-cmd --permanent --remove-service=dns sudo firewall-cmd --reload ","date":"7 August, 2025","id":9,"permalink":"/hidden/podman-pihole/","summary":"Pi-hole is a network-level ad blocker that acts as a DNS sinkhole. Here\u0026rsquo;s how to run it securely on your Fedora-based homelab using Podman and Traefik, with a clean and idempotent setup.","tags":"pihole install fedora","title":"Pihole - Installation on Podman"},{"content":"Mastering Kubernetes Deployments with GitOps Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity‚Äîand the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.\nWhy Read this blog? To live like this What is GitOps? GitOps is a DevOps operating model where Git is the single source of truth for declarative infrastructure and applications. Tools like Argo CD sync the state of your Kubernetes clusters to match Git, automatically and continuously.\nWHAT is the \u0026ldquo;App of Apps Pattern\u0026rdquo;? The App of Apps pattern uses a single Argo CD Application to manage many other Argo CD Applications. It enables modular, scalable, and environment-specific deployment structures.\nImagine one app (root-app.yaml) that deploys:\nPlatform apps like Ingress, Cert-Manager \u0026amp; Operators Workload apps like Podinfo, Guestbook, etc. Each app lives in its own folder, can use Kustomize/Helm, and is deployed declaratively from Git.\nWHY use the \u0026ldquo;App of Apps Pattern\u0026rdquo;? It offers:\nDeclarative control : Everything is defined in Git. Zero-touch provisioning : GitOps installs and configures your entire stack. Environment-specific overlays : Adapt configurations for K3s, OpenShift, Dev, Prod etc. Disaster recovery : Rebuild any where Auditable changes : Every change is a Git commit. No drift : GitOps continuously reconciles desired vs. actual state. Self Healing : Accidently deleted something ? Let GitOps fix it for you. Let\u0026rsquo;s Deploy everything (in seconds) Start the timer\nPrerequisites to Deploy A Kubernetes cluster: This demo is tested on K3s but should work on any cluster CLI tools : kubectl, helm Forked git repo : git clone https://github.com/arslankhanali/GitOps-App-of-Apps-Pattern.git` Now! start the timer\n1. Install argocd on your Kubernetes cluster export KUBECONFIG=~/k3s-config # \u0026lt;-- To access Kubernetes cluster # kubectl get all -A helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml Apply environment-specific ingress for argocd :\n# K3s kubectl apply -f argocd/ingress.yaml # OpenShift kubectl apply -f argocd/route.yaml 2. Set DNS locally Make sure your /etc/hosts file has following entries.\n# sudo vim /etc/hosts \u0026lt;K3s-cluster-IP\u0026gt; k3s.node1 argocd.node1 test.node1 hello.node1 3. Login to Argo dashboard To see apps getting deployed.\nArgocd argocd.node1 # Get Login password for admin user kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 4. Unleash everything This points to k3s right now\nkubectl apply -f root-app.yaml Access apps Kubernetes Dashboard k3s.node1 # Get Bearer Token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d Guestbook test.node1 Podinfo hello.node1 You can now stop the Timer. It tooks me \u0026lt; 1min to deploy everything.\nArgoCD has : Synced the env/{k3s}/ directory. Created child applications in {platform \u0026amp; workloads} folders. Deployed all components declaratively. This pattern allows full cluster rebuilds and updates via Git commits alone. Steps to deploy new app Add application to the apps/ folder. Test the application kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - # or kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace named above should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Push to git git add . \u0026amp;\u0026amp; git commit -m \u0026quot;new app\u0026quot; \u0026amp;\u0026amp; git push Argo should sync automatically Delete All kubectl delete -f root-app.yaml # delete all argocd apps for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo kubectl delete ns guestbook Summary The ArgoCD App of Apps pattern offers a scalable, Git-driven blueprint for managing Kubernetes clusters :\nManage everything declaratively in Git Scale across environments like K3s and OpenShift Rebuild or recover your clusters on demand The App of Apps pattern isn\u0026rsquo;t just a tool‚Äîit\u0026rsquo;s a mindset shift for cloud-native GitOps. Adopt it to bring structure, repeatability, and security to your infrastructure.\nAppendix Repository Structure Overview ‚îú‚îÄ‚îÄ apps # Apps \u0026amp; workload YAMLS, Helm charts or Kustomize can go here ‚îÇ ‚îú‚îÄ‚îÄ guestbook # Sample App from https://github.com/argoproj/argocd-example-apps/tree/master/kustomize-guestbook ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îÇ ‚îú‚îÄ‚îÄ kubernetes-dashboard # Upstream K8s dashboard https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îÇ ‚îî‚îÄ‚îÄ podinfo # Sample App from https://github.com/stefanprodan/podinfo/tree/master/kustomize ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ overlays ‚îú‚îÄ‚îÄ env # ArgoCD Applications - Folders can be Cluster-specific (k3s,openshift) or Env Specific (dev, ‚îÇ ‚îú‚îÄ‚îÄ k3s ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ platform ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ workloads ‚îÇ ‚îî‚îÄ‚îÄ openshift ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îú‚îÄ‚îÄ platform ‚îÇ ‚îî‚îÄ‚îÄ workloads ‚îú‚îÄ‚îÄ ingress.yaml # Ingress to access ArgoCD dashboard ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ root-app.yaml # Root ARGOCD application ‚îî‚îÄ‚îÄ values.yaml # Deploy Argo with insecure access (needed for Ingress) \u0026amp; enable Helm for kustomize 1. apps/ ‚Äì Add your Apps in a folder here I have 3 apps here as an example :\nguestbook : Kustomize based app argocd-kustomize-guestbook kubernetes-dashboard/ : Kustomize calls Helm to install K8s dashboard for K3s. podinfo : Kustomize based app stefanprodan-podinfo You can use YAML manifests, kustomize or Helm charts to add more applications in this folder.\nEach app follows :\napps/ ‚îî‚îÄ‚îÄ \u0026lt;app1\u0026gt;/ ‚îú‚îÄ‚îÄ base/ ‚îî‚îÄ‚îÄ overlays/ ‚îú‚îÄ‚îÄ \u0026lt;env1-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. DEV ‚îî‚îÄ‚îÄ \u0026lt;env2-name\u0026gt;/ # \u0026lt;--- Can Change Name - e.g. PROD 2. env/ ‚Äì Create your ARGOCD APPLICATIONS here for your env \u0026ldquo;ArgoCD Application\u0026rdquo; definitions for different environments. They basically call different overlays in apps.\nenv/k3s/ : Deploys K8s Dashboard and uses Ingress for apps env/openshift/ : No K8s Dashboard and uses Route for apps Each env follows :\n‚îÄ‚îÄ env ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;env1-name\u0026gt; ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ platform # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ \u0026#39;argocd-application-for-app1\u0026#39;.yaml ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ workloads # \u0026lt;--- Can Change Name - Just used to categorise `argocd-application` ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ \u0026#39;argocd-application-for-app2\u0026#39;.yaml ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ \u0026#39;argocd-application-for-app3\u0026#39;.yaml 3. root-app.yaml ‚Äì The Orchestrator Main reason this pattern is called APP OF APPS.\nThis top-level ArgoCD Application points to env/{k3s} and deploys all children ArgoCD Application in it.\n","date":"6 August, 2025","id":10,"permalink":"/posts/featured/argocd-app-of-apps/","summary":"Managing multiple Kubernetes clusters and applications can get complex fast. GitOps helps tame this complexity‚Äîand the App of Apps pattern takes it to the next level with declarative, scalable, and automated infrastructure management.","tags":"gitops kubernetes devops","title":"Mastering Kubernetes Deployments with the GitOps based App of Apps Pattern"},{"content":"Get started with Ansible in under 1 minute ‚Äî ideal for homelab setups and automation testing.\nInstall Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible Run an Ansible Playbook Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;expected that you know\u0026gt; My playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL Run the Playbook # Run with `login password` prompt ansible-playbook --ask-pass -u neo -i 192.168.50.205, ping.yaml # Run with \u0026#39;login password\u0026#39; \u0026amp; \u0026#39;sudo password\u0026#39; prompt ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, ping.yaml Try Ad-hoc Commands Need to use all\n# Ping remote node ansible all -i 192.168.50.205, -u neo -m ping # Run shell command ansible all -i 192.168.50.205, -u neo -m shell -a \u0026#34;uptime\u0026#34; Note the trailing comma , ‚Äî this tells Ansible you\u0026rsquo;re passing a literal list of hosts, not an inventory file.\nThis gets you running fast with Ansible on macOS or RHEL. You can later scale by adding inventories, roles, and vaults.\n","date":"4 August, 2025","id":11,"permalink":"/posts/ansible/ansible-quickstart-1/","summary":"Get started with Ansible in under 1 minute ‚Äî ideal for homelab setups and automation testing.","tags":"ansible","title":"Ansible: Quick Start - 1"},{"content":"ssh is on # Enable SSH daemon sudo systemctl enable sshd.service \u0026amp;\u0026amp; systemctl start sshd.service # Allow SSH in firewall sudo firewall-cmd --permanent --add-service=ssh sudo firewall-cmd --reload Basic playbook Installs DNF packages Set Hostname Disable sleep when idle Changes terminal to ZSH tee playbook.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: VM setup hosts: all gather_facts: true vars: hostname: node2 packages_to_install: - podman - podman-compose - cockpit - cockpit-files - cockpit-machines - cockpit-navigator - cockpit-podman - cockpit-selinux - cockpit-storaged - cockpit-system - zsh - git - curl - python3-pygments local_backup_zsh: \u0026#34;~/Codes/homelab/ansible/files/zshrc\u0026#34; local_backup_p10k: \u0026#34;~/Codes/homelab/ansible/files/p10k\u0026#34; remote_home: \u0026#34;{{ ansible_env.HOME }}\u0026#34; remote_zshrc: \u0026#34;{{ remote_home }}/.zshrc\u0026#34; remote_p10k: \u0026#34;{{ remote_home }}/.p10k.zsh\u0026#34; ohmyzsh_install_script: \u0026#34;{{ remote_home }}/install-oh-my-zsh.sh\u0026#34; tasks: - name: Bootstrap dnf module support (Fedora only) become: true ansible.builtin.command: dnf install -y python3-libdnf5 when: ansible_distribution == \u0026#34;Fedora\u0026#34; args: creates: /usr/lib/python3*/site-packages/libdnf5 - name: Install required packages become: true ansible.builtin.dnf: name: \u0026#34;{{ packages_to_install }}\u0026#34; state: present - name: Enable and start cockpit become: true ansible.builtin.service: name: cockpit.socket enabled: true state: started - name: Change default shell to Zsh become: true ansible.builtin.user: name: \u0026#34;{{ ansible_user_id }}\u0026#34; shell: /bin/zsh - name: Check if Oh My Zsh is installed ansible.builtin.stat: path: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; register: ohmyzsh_installed - name: Download Oh My Zsh installer ansible.builtin.get_url: url: \u0026#34;https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh\u0026#34; dest: \u0026#34;{{ ohmyzsh_install_script }}\u0026#34; mode: \u0026#39;0755\u0026#39; when: not ohmyzsh_installed.stat.exists - name: Run Oh My Zsh installer ansible.builtin.command: \u0026#34;{{ ohmyzsh_install_script }} --unattended\u0026#34; when: not ohmyzsh_installed.stat.exists args: chdir: \u0026#34;{{ remote_home }}\u0026#34; creates: \u0026#34;{{ remote_home }}/.oh-my-zsh\u0026#34; - name: Clone Powerlevel10k ansible.builtin.git: repo: https://github.com/romkatv/powerlevel10k.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/themes/powerlevel10k\u0026#34; depth: 1 - name: Clone zsh-autosuggestions ansible.builtin.git: repo: https://github.com/zsh-users/zsh-autosuggestions.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\u0026#34; depth: 1 - name: Clone zsh-syntax-highlighting ansible.builtin.git: repo: https://github.com/zsh-users/zsh-syntax-highlighting.git dest: \u0026#34;{{ remote_home }}/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting\u0026#34; depth: 1 - name: Copy .zshrc ansible.builtin.copy: src: \u0026#34;{{ local_backup_zsh }}\u0026#34; dest: \u0026#34;{{ remote_zshrc }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Copy .p10k.zsh ansible.builtin.copy: src: \u0026#34;{{ local_backup_p10k }}\u0026#34; dest: \u0026#34;{{ remote_p10k }}\u0026#34; mode: \u0026#39;0644\u0026#39; - name: Set hostname become: true ansible.builtin.hostname: name: \u0026#34;{{ hostname }}\u0026#34; when: hostname is defined - name: Configure /etc/systemd/logind.conf to disable suspend/lid actions become: true ansible.builtin.blockinfile: path: /etc/systemd/logind.conf marker: \u0026#34;# {mark} ANSIBLE MANAGED BLOCK - power settings\u0026#34; block: | [Login] IdleAction=ignore IdleActionSec=0 HandleLidSwitch=ignore HandleLidSwitchExternalPower=ignore HandleSuspendKey=ignore HandleHibernateKey=ignore create: true mode: \u0026#39;0644\u0026#39; - name: Restart systemd-logind become: true ansible.builtin.service: name: systemd-logind state: restarted EOL Configure Networking Check network settings\nsudo ls /etc/NetworkManager/system-connections/ sudo cat /etc/NetworkManager/system-connections/bridge0.nmconnection You can edit the file before copying\ntee network.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Configure Fedora Networking hosts: all gather_facts: true vars: wifi_conn: \u0026#34;ASUS_6E\u0026#34; bridge_conn: \u0026#34;bridge0\u0026#34; eth_conn: \u0026#34;Wired Connection\u0026#34; wifi_iface: \u0026#34;wlp1s0\u0026#34; bridge_iface: \u0026#34;bridge0\u0026#34; eth_iface: \u0026#34;enp3s0\u0026#34; wifi_psk: \u0026#34;eq4akar?qk\u0026#34; tasks: - name: Configure ASUS_6E Wi-Fi connection become: true community.general.nmcli: conn_name: \u0026#34;{{ wifi_conn }}\u0026#34; type: wifi ifname: \u0026#34;{{ wifi_iface }}\u0026#34; state: present autoconnect: yes wifi: ssid: \u0026#34;{{ wifi_conn }}\u0026#34; wifi_sec: key_mgmt: sae psk: \u0026#34;{{ wifi_psk }}\u0026#34; ipv4: method: manual address1: \u0026#34;192.168.50.100/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate ASUS_6E connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ wifi_conn }}\u0026#34; changed_when: false ignore_errors: true # Safe fallback in case it\u0026#39;s already up - name: Configure bridge0 connection with static IP become: true community.general.nmcli: conn_name: \u0026#34;{{ bridge_conn }}\u0026#34; type: bridge ifname: \u0026#34;{{ bridge_iface }}\u0026#34; state: present autoconnect: yes bridge: stp: no ipv4: method: manual address1: \u0026#34;192.168.50.200/24\u0026#34; gateway: \u0026#34;192.168.50.100\u0026#34; dns: - 192.168.50.100 - 9.9.9.9 - 192.168.50.1 ipv6: method: disabled - name: Activate bridge0 connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ bridge_conn }}\u0026#34; changed_when: false ignore_errors: true - name: Attach enp3s0 to bridge0 become: true community.general.nmcli: conn_name: \u0026#34;{{ eth_conn }}\u0026#34; type: ethernet ifname: \u0026#34;{{ eth_iface }}\u0026#34; state: present master: \u0026#34;{{ bridge_conn }}\u0026#34; ethernet: {} bridge_port: {} - name: Activate Wired (bridge slave) connection become: true ansible.builtin.command: \u0026#34;nmcli con up {{ eth_conn }}\u0026#34; changed_when: false ignore_errors: true EOL Run the Playbook # ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.100 ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, playbook.yaml ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.205, network.yaml ","date":"4 August, 2025","id":12,"permalink":"/homelab/ansible-fedora/","summary":"Check network settings","tags":"ansible fedora","title":"Homelab: Initial setup for a Fedora VM"},{"content":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.\nStep 1: Install Zsh and Plugins # Install zsh via Homebrew brew install zsh # Oh My Zsh framework sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # Install plugins git clone https://github.com/zsh-users/zsh-syntax-highlighting.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions.git \\ ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Step 2: Install Powerlevel10k Theme # Install Powerlevel10k theme brew install powerlevel10k # Add theme to .zshrc echo \u0026#39;source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\u0026#39; \u0026gt;\u0026gt;~/.zshrc # Configure p10k configure üí° The p10k configure command launches an interactive wizard to customize your prompt.\nStep 3: Basic ~/.zshrc Configuration Below is a minimal yet powerful .zshrc example. It includes:\nPowerlevel10k theme Plugin setup (autosuggestions, syntax highlighting) Useful aliases and functions History, completion, and path setup cat \u0026gt;\u0026gt; ~/.zshrc \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Powerlevel10k Instant Prompt if [[ -r \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; ]]; then source \u0026#34;\\${XDG_CACHE_HOME:-\\$HOME/.cache}/p10k-instant-prompt-\\${(%):-%n}.zsh\u0026#34; fi # Plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # Oh My Zsh export ZSH=\u0026#34;\\$HOME/.oh-my-zsh\u0026#34; source \\$ZSH/oh-my-zsh.sh plugins=( aliases alias-finder ansible macos argocd colored-man-pages colorize command-not-found common-aliases gh git-commit nmap oc python ssh sudo virtualenv zsh-interactive-cd zsh-navigation-tools dnf podman kubectl ) # Custom Aliases alias ipp=\u0026#34;ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34; # Functions backup() { cp -r \u0026#34;\\$1\u0026#34; \u0026#34;\\$1.backup\u0026#34;; } ip() { ip=\\$(ifconfig en0 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) ip1=\\$(ifconfig en7 | grep \u0026#39;inet \u0026#39; | awk \u0026#39;{print \\$2}\u0026#39;) dns=\\$(awk \u0026#39;/nameserver/ {print \\$2}\u0026#39; /etc/resolv.conf) echo -e \u0026#34;WiFi: \\$ip\\nLAN: \\$ip1\\nDNS:\\n\\$dns\u0026#34; } gp() { git add . git commit -am \u0026#34;git push via gp\u0026#34; git push } ct() { echo \u0026#39;cat \u0026lt;\u0026lt; EOF | oc apply -f-\u0026#39; echo \u0026#39;EOF\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;cat \u0026gt;\u0026gt; text.sh \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; echo \u0026#34;--or--\u0026#34; echo \u0026#34;sudo tee text.sh \u0026gt; /dev/null \u0026lt;\u0026lt;EOL\u0026#34; echo \u0026#39;EOL\u0026#39; } # Alias Finder Plugin Settings zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; autoload yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; longer yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; exact yes zstyle \u0026#39;:omz:plugins:alias-finder\u0026#39; cheaper yes # Path Setup export PATH=\u0026#34;\\$HOME/.local/bin:\\$HOME/.krew/bin:\\$HOME/Codes/0-scripts:\\$PATH\u0026#34; # OpenShift Autocompletion if [ -x \u0026#34;/usr/local/bin/oc\u0026#34; ]; then source \u0026lt;(oc completion zsh) compdef _oc oc fi # Editor and History export EDITOR=\u0026#39;vim\u0026#39; HISTFILE=~/.histfile HISTSIZE=100000 SAVEHIST=100000 alias hist=\u0026#34;fc -ln\u0026#34; # Powerlevel10k Prompt [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh source /opt/homebrew/share/powerlevel10k/powerlevel10k.zsh-theme # Brew Env eval \u0026#34;\\$(/opt/homebrew/bin/brew shellenv)\u0026#34; EOF Step 4: Activate Your New Shell # Change to zsh exec zsh # Reload config source ~/.zshrc Result Your Mac terminal will now be:\n‚úÖ Visual: Prompt with icons, colors, and context-aware sections\n‚úÖ Efficient: Aliases, plugins, autosuggestions, syntax highlighting\n‚úÖ Extensible: Add more plugins or themes as needed\nTo tweak appearance later, just run:\np10k configure Done! Your terminal is now both beautiful and powerful.\n","date":"4 August, 2025","id":13,"permalink":"/homelab/terminal-zsh/","summary":"Tired of the plain old macOS terminal? Here\u0026rsquo;s how to transform it into a powerful, beautiful, and productive environment using Zsh, Oh My Zsh, Powerlevel10k, and a handful of plugins.","tags":"zsh powerlevel10k macos","title":"Homelab: Oh My Zsh - My terminal setup"},{"content":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.\n1. Download and Configure SSH Key For the Red Hat certification lab, the SSH private key is provided in the Lab Environment section.\nRun these commands on your Mac terminal:\n# Move the downloaded key to your SSH folder mv ~/Downloads/rht_classroom.rsa ~/.ssh/ # Secure the key with correct permissions chmod 0600 ~/.ssh/rht_classroom.rsa # Add the key to your ssh-agent ssh-add ~/.ssh/rht_classroom.rsa Test SSH login to remote VM via jump host Replace IPs and ports if different:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 student@172.25.252.1 -p 53009 Note:\nIf you get the error Host key verification failed, remove your known hosts file and retry:\nrm ~/.ssh/known_hosts 2. Setup Squid Proxy on Remote VM SSH into the remote VM and become root or use sudo:\nsudo su dnf install squid -y Add access control to Squid config (adjust IP range if different):\nsudo tee /etc/squid/squid.conf \u0026gt; /dev/null \u0026lt;\u0026lt;EOL acl localnet src 172.25.252.1/24 # Change IP as needed acl Safe_ports port 22 EOL Enable and restart Squid:\nsystemctl enable squid systemctl restart squid 3. Create SSH Tunnel to Forward Proxy Port From your local Mac laptop open a new terminal and run:\nssh -i ~/.ssh/rht_classroom.rsa -J cloud-user@148.62.92.65:22022 \\ -L 3128:localhost:3128 \\ student@172.25.252.1 -p 53009 This forwards local port 3128 to the remote Squid proxy.\n4. Configure Browser Proxy Settings (Firefox Recommended) Tip: Use a secondary browser profile or a different browser to avoid routing all traffic unintentionally.\nOpen Firefox settings Scroll to the Network section at the bottom Select Manual proxy configuration Set: HTTP Proxy: localhost Port: 3128 Check Use this proxy server for all protocols 5. Test Access Visit any URL only accessible from the remote VM, e.g.:\nhttps://console-openshift-console.apps.ocp4.example.com/ You should now be able to access it locally via your browser.\nAs a quick test, visit https://whatismyipaddress.com to confirm your IP corresponds to the remote environment.\nConclusion You‚Äôve successfully tunneled your browser traffic through the remote Squid proxy using SSH, enabling access to URLs only reachable from your lab environment.\nThis method keeps your local and remote network environments cleanly separated while allowing seamless access to remote resources.\n","date":"4 August, 2025","id":14,"permalink":"/posts/networks/squid-rh-lab/","summary":"This blog shows how to access URLs available only on a remote Red Hat lab environment from your local MacOS laptop using SSH tunneling and a Squid proxy.","tags":"squid-proxy redhat","title":"Squid Proxy: Access Remote Red Hat Lab Environment"},{"content":" In a lab far away, Ceph lived across three nodes ‚Äî ceph-node01, ceph-node02, and ceph-node03. Each node was a diligent guardian, managing storage and services on port 8443. But there was a problem: access was restricted, and only one gateway, a single door at IP 192.168.99.61 on port 9000, was open to outsiders. No one could knock on port 80‚Äôs door anymore ‚Äî it was locked tight.\nCeph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.\nThe Challenge The Ceph nodes spoke securely on port 8443. Only port 9000 was reachable from outside. SELinux guarded the system fiercely, preventing rogue processes from binding unusual ports or making unexpected connections. HAProxy to the Rescue HAProxy was installed quietly with:\ndnf -y install haproxy To convince SELinux to trust HAProxy‚Äôs new role, the magic command was cast:\nsetsebool -P haproxy_connect_any=1 With trust secured, HAProxy configured its front door by listening on 192.168.99.61:9000 and redirecting incoming visitors to the three Ceph nodes in a balanced, round-robin dance.\nThe Configuration Story A little script was written to tell HAProxy exactly how to guide visitors:\n#!/bin/bash # frontend_ip=\u0026#34;192.168.99.61\u0026#34; # frontend_port=\u0026#34;9000\u0026#34; # backend_ips=(\u0026#34;192.168.99.61\u0026#34; \u0026#34;192.168.99.62\u0026#34; \u0026#34;192.168.99.63\u0026#34;) # backend_hostnames=(\u0026#34;ceph-node01\u0026#34; \u0026#34;ceph-node02\u0026#34; \u0026#34;ceph-node03\u0026#34;) # backend_port=\u0026#34;8443\u0026#34; cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt; EOF frontend ceph_front bind 192.168.99.61:9000 default_backend ceph_back backend ceph_back balance roundrobin server ceph-node01 192.168.99.61:8443 check server ceph-node02 192.168.99.62:8443 check server ceph-node03 192.168.99.63:8443 check EOF systemctl restart haproxy This script is HAProxy‚Äôs map and guide, balancing load and checking if each Ceph node is ready to receive guests.\nThe Happy Ending Visitors came knocking on https://192.168.99.61:9000, unaware of the careful orchestration behind the scenes. HAProxy gracefully sent each visitor to a Ceph node in turn, ensuring no one node was overwhelmed.\nSELinux nodded approvingly, and the lab stayed secure.\nYou can test this harmony yourself:\ncurl -k https://192.168.99.61:9000 Lessons from Ceph‚Äôs Story Problem Solution Restricted port access Use HAProxy on an allowed port (9000) Multiple backend servers Round-robin load balancing SELinux blocking connections Enable haproxy_connect_any boolean Dynamic backend management Scripted configuration for easy updates In your own labs, think of HAProxy as the wise gatekeeper, balancing requests with fairness, security, and simplicity ‚Äî just like Ceph needed.\nThis story shows how small tweaks and a simple tool can solve network puzzles and keep services running smoothly.\n","date":"4 August, 2025","id":15,"permalink":"/posts/networks/haproxy-ceph-story/","summary":"Ceph needed a wise gatekeeper to direct visitors fairly among the three nodes, so none got overwhelmed. Enter HAProxy, a simple but powerful load balancer, ready to bring harmony.","tags":"haproxy ceph","title":"HAProxy: How Ceph Found L3 Balance"},{"content":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.\nPrerequisites K3s on Fedora Install Helm:\nsudo dnf install helm helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm repo update Deploy the Dashboard To avoid the error Unknown error (200): Http failure during parsing, configure Kong to enable HTTP access. This is needed for Ingress.\nAllow http tee dashboard-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL kong: proxy: http: enabled: true EOL Install the dashboard: helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --namespace kubernetes-dashboard \\ --create-namespace \\ -f dashboard-values.yaml TLS Setup for Ingress If you want to provide your own certificate for Traefik Ingress.\nCreate a self-signed certificate:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \u0026#34;/CN=*node1\u0026#34; Create the secret in the correct namespace:\nkubectl create secret tls dashboard-tls \\ --cert=tls.crt --key=tls.key \\ -n kubernetes-dashboard Create Admin Service Account cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF Ingress Configuration (Traefik) cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: ingressClassName: traefik rules: - host: k3s.node1 # Change as needed http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard-kong-proxy port: number: 80 # Comment below lines If you are happy to use default Traefik certificate tls: - hosts: - k3s.node1 # Change as needed secretName: dashboard-tls Verify Services and Ingress kubectl -n kubernetes-dashboard get ingress kubectl -n kubernetes-dashboard get services Update /etc/hosts:\necho \u0026#34;192.168.50.200 k3s.node1\u0026#34; | sudo tee -a /etc/hosts Test access:\ncurl -k https://192.168.50.200 -H \u0026#34;Host: k3s.node1\u0026#34; curl -Ik https://k3s.node1/ Browser Notes Browser HTTPS HTTP Chrome ‚úÖ Works ‚ùå Fails with CSRF token error Safari ‚úÖ Works ‚ùå Unauthorized (401) Get Token for Login kubectl -n kubernetes-dashboard create token admin-user --duration=1999h Paste the token in the dashboard login screen.\nErrors Login errors that you might see:\nUnauthorized (401).\nTry using https instead of http. Fails with CSRF token error\nDid you allow insecure(http) connection. See Allow http Try incognito mode - Previously saved tokens can lead to errors Summary This guide sets up the dashboard with HTTP enabled behind Traefik, adds an admin user, and exposes it securely with a self-signed TLS cert. Works best with Chrome.\n","date":"4 August, 2025","id":16,"permalink":"/posts/kubernetes/k3s-dashboard/","summary":"This post walks through deploying the Kubernetes Dashboard using Helm, with access via Traefik Ingress.","tags":"k3s","title":"Kubernetes: Deploy Dashboard for K3s"},{"content":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.\nPrerequisites Fedora (Workstation or Server) firewalld active and running SELinux in enforcing mode ‚Äî K3s works fine User with sudo privileges Deploy K3s via ansible This playbook deploys K3s on fedora\nCreate 'deploy-k3s.yaml' tee deploy-k3s.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes EOL ansible-playbook --ask-pass --ask-become-pass -u \u0026lt;ssh-user\u0026gt; -i \u0026lt;IP-of-Server\u0026gt;, deploy-k3s.yaml Step by Step via CLI Configure Firewalld sudo firewall-cmd --permanent --add-port=6443/tcp # API Server port sudo firewall-cmd --permanent --zone=trusted --add-source=10.42.0.0/16 # Pod CIDR sudo firewall-cmd --permanent --zone=trusted --add-source=10.43.0.0/16 # Service CIDR sudo firewall-cmd --reload # Optional: Confirm port is listening ss -tulpn | grep 6443 Install K3s # Create a secure group(kubeconfig) to access kubeconfig sudo groupadd kubeconfig sudo usermod -aG kubeconfig $USER newgrp kubeconfig # Install K3s with kubeconfig permissions curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - Verify kubeconfig permissions:\nls -l /etc/rancher/k3s/k3s.yaml # Expected: -rw-r----- 1 root kubeconfig ... Test K3s Installation kubectl get all -A # Create kubeconfig symlink mkdir -p ~/.kube ln -s /etc/rancher/k3s/k3s.yaml ~/.kube/config Uninstall K3s sudo /usr/local/bin/k3s-uninstall.sh Optional: Install OpenShift CLI (oc) wget https://github.com/cptmorgan-rh/install-oc-tools/blob/master/install-oc-tools.sh chmod +x install-oc-tools.sh sudo ./install-oc-tools.sh --latest Access K3s Remotely (macOS or Another Host) # From your client (e.g., macOS), copy kubeconfig from Fedora host: scp -r \u0026lt;user\u0026gt;@\u0026lt;fedora-host-ip\u0026gt;:~/.kube/config ~/k3s-config Edit the config file:\n# vim ~/k3s-config Change: server: https://127.0.0.1:6443 To: server: https://\u0026lt;fedora-host-ip\u0026gt;:6443 Use it:\nexport KUBECONFIG=~/Codes/k3s-config oc get all -A Summary Step Command/Action Firewall Setup firewall-cmd for 6443 and CIDRs SELinux K3s runs fine in enforcing mode K3s Install curl -sfL https://get.k3s.io Verify Node kubectl get nodes Remote Access scp + IP update + export KUBECONFIG Uninstall k3s-uninstall.sh This setup gives you a clean, minimal Kubernetes environment with K3s on Fedora. Works great for homelabs and lightweight clusters.\n","date":"4 August, 2025","id":17,"permalink":"/posts/kubernetes/k3s-install/","summary":"A minimal and secure K3s setup on a Fedora host with proper firewalld rules and SELinux support.","tags":"k3s fedora","title":"Kubernetes: Install K3s on Fedora"},{"content":"Install ArgoCD on K3s with Traefik Ingress This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.\nSetup Kubernetes: K3s Ingress Controller: Traefik Deployment method: Helm Install ArgoCD via Helm helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd Option 1: Without Ingress Access service locally. Access service locally. See Port Forwarding section.\nhelm install argocd argo/argo-cd --create-namespace --namespace argocd Option 2: With Ingress (Insecure) Ingress is needed to expose the Services out of the cluster By setting the server.insecure flag to true, you\u0026rsquo;re telling the ArgoCD server not to handle TLS itself to avoid common issue known as a \u0026ldquo;redirect loop\u0026rdquo; or ERR_TOO_MANY_REDIRECTS. Instead, it listens for and accepts plain HTTP traffic.\nYour browser sends an HTTPS request to Traefik. Traefik terminates the TLS and forwards an HTTP request to the argocd-server service. The argocd-server accepts this HTTP request on its insecure port (typically port 80), serves the content, and the connection is successful. # Using CLI flag helm install argocd argo/argo-cd --create-namespace --namespace argocd --set configs.params.\u0026#34;server\\.insecure\u0026#34;=true # OR using values.yaml tee argocd-values.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL configs: params: server.insecure: true EOL helm install argocd argo/argo-cd --create-namespace --namespace argocd -f argocd-values.yaml Verify that server.insecure is set:\nkubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure Port Forwarding (Optional Access) # Kubeconfig # Fetch kubeconfig to your local machine scp -r \u0026lt;user\u0026gt;@\u0026lt;K8s-cluster-IP\u0026gt;:~/.kube/config ~/k3s-config export KUBECONFIG=~/k3s-config # Port-forward to localhost kubectl port-forward svc/argocd-server -n argocd 8080:443 # Open in browser http://localhost:8080 Get Default Admin Password # Ignore the `%` sign at the end - It\u0026#39;s not part of the password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Default username: admin\nIngress Setup (Traefik) 1. Make sure you set server.insecure:true If you did not Install argo with \u0026ldquo;server.insecure\u0026rdquo;:\u0026ldquo;true\u0026rdquo; then you can patch the configmap and restart pods.\n# Check current value kubectl get cm argocd-cmd-params-cm -n argocd -o yaml | grep insecure # Change value to true if not already kubectl patch cm argocd-cmd-params-cm -n argocd --type=merge \\ -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;server.insecure\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; # Restart the server for changes to take effect kubectl -n argocd rollout restart deployment argocd-server 2. Create Ingress Resource cat \u0026lt;\u0026lt; EOF | oc apply -f- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd spec: ingressClassName: traefik rules: - host: argocd.node1 #Change to your hostname http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 EOF Apply it:\nkubectl apply -f argocd-ingress.yaml Add local DNS Update your /etc/hosts:\necho \u0026quot;192.168.50.200 argocd.node1\u0026quot; | sudo tee -a /etc/hosts\nor\nsudo vim /etc/hosts Add:\n192.168.50.200 argocd.node1 Now you can access ArgoCD https://argocd.node1\nCleanup helm uninstall argocd --namespace argocd kubectl delete namespace argocd ArgoCD is now set up with Traefik Ingress on your K3s cluster.\n","date":"4 August, 2025","id":18,"permalink":"/posts/kubernetes/argocd-install/","summary":"This post walks through installing ArgoCD on a K3s cluster that uses Traefik as its default Ingress controller.","tags":"argocd k3s","title":"ArgoCD: Installation"},{"content":"Introduction The \u0026ldquo;Arr stack\u0026rdquo; is a popular set of applications for automating your media library. This blog post will walk you through configuring a basic Arr stack using Podman on your homelab. We\u0026rsquo;ll cover Jellyfin for media streaming, Jellyserr for integrating services, Prowlarr for managing indexers, and Sonarr and Radarr for automating TV and movie downloads. This setup provides a simple yet powerful way to manage your media. üöÄ\nConfiguring the Arr Stack Basic flow about how each service talks to each other\nJellyfin Add media and scan Enable trickplay image extraction Save trickplay images next to media Trickplay Image Interval = 300000 Only generate images from key frames Jellyserr Settings -\u0026gt; Jellyfin. Get API from Jellyfin (Advanced -\u0026gt; API Keys) - See pic js1 Settings -\u0026gt; Services -\u0026gt; Radarr. Get API from Radarr (Settings -\u0026gt; General) Settings -\u0026gt; Services -\u0026gt; Sonarr. Get API from Sonarr (Settings -\u0026gt; General) Make sure flare-bypasser container is up Prowler Add indexers -\u0026gt; YTS etc Settings -\u0026gt; Indexer -\u0026gt; FlareSolverr -\u0026gt; Setup flare-bypasser Settings -\u0026gt; Apps -\u0026gt; Applications -\u0026gt; Radarr Settings -\u0026gt; Apps -\u0026gt; Applications -\u0026gt; Sonarr Sonarr Select location for TvShows Settings -\u0026gt; Indexer -\u0026gt; Verify indexers (from Prowler) Settings -\u0026gt; Download Clients -\u0026gt; Transmission Radarr Select location for Movies Settings -\u0026gt; Indexer -\u0026gt; Verify indexers (from Prowler) Settings -\u0026gt; Download Clients -\u0026gt; Transmission Service Local IP Access Ingress Jellyfin http://192.168.50.200:8096 https://jellyfin.node1 Jellyseerr http://192.168.50.200:5055 https://jellyseerr.node1 Prowlarr http://192.168.50.200:9696 https://prowlarr.node1 Sonarr http://192.168.50.200:8989 https://sonarr.node1 Radarr http://192.168.50.200:7878 https://radarr.node1 Transmission http://192.168.50.200:9091 https://transmission.node1 flare-bypasser http://192.168.50.200:8191 https://flare-bypasser.node1 Jellyfin libraries - Add Movies Jellyfin libraries - Add Shows jellyseerr - Give Jellyfin URL jellyseerr - Sync libraries jellyseerr - Sync with radarr\nGet key from radarr configure radarr in jellyseerr jellyseerr - Sync with sonarr\nGet key from sonarr configure sonarr in jellyseerr jellyseerr - Import users from jellyfin Prowlarr - configure flaresolverr Prowlarr - add radarr Prowlarr - add sonarr Prowlarr - tags sonarr - just get HEVC Summary Congratulations! üéâ You\u0026rsquo;ve now configured a powerful and automated media stack. With this setup, you can add movies and TV shows from your phone or desktop, and the Arr stack will automatically find and download them, ready for you to watch on Jellyfin. This is a great way to streamline your homelab and take control of your media.\n","date":"8 August, 2025","id":19,"permalink":"/hidden/podman-arr-stack-configuration/","summary":"The \u0026ldquo;Arr stack\u0026rdquo; is a popular set of applications for automating your media library. This blog post will walk you through configuring a basic Arr stack using Podman on your homelab. We\u0026rsquo;ll cover Jellyfin for media streaming, Jellyserr for integrating services, Prowlarr for managing indexers, and Sonarr and Radarr for automating TV and movie downloads. This setup provides a simple yet powerful way to manage your media. üöÄ","tags":"arr-stack podman homelab","title":"Arr Stack - Configuration"},{"content":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic ‚Äî all running locally on your own hardware.\nHome Assistant OS (HAOS) is the official operating system for running Home Assistant as a virtual appliance. It includes everything needed: supervisor, OS, and the Home Assistant core.\nThis guide shows how to run HAOS inside a KVM virtual machine using libvirt on Fedora without requiring sudo to manage the VM ‚Äî after an initial root configuration.\nWhy run HAOS as a non-root user? Reduces attack surface and limits damage in case of misconfiguration Lets you manage your smart home environment without admin rights Enables easier automation and scripting without sudo prompts Aligns with the principle of least privilege in homelab setups 1. System Preparation Install required packages:\nsudo dnf install -y \\ libvirt \\ qemu-kvm \\ virt-install \\ bridge-utils \\ wget \\ xz \\ python3-libvirt \\ virt-manager Enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd 2. Download and Prepare HAOS Image Find the latest HAOS releases here:\nhttps://github.com/home-assistant/operating-system/releases/\nmkdir haos \u0026amp;\u0026amp; cd haos download_url=\u0026#34;https://github.com/home-assistant/operating-system/releases/download/16.1.rc1/haos_ova-16.1.rc1.qcow2.xz\u0026#34; image_file=\u0026#34;haos_ova-16.1.rc1.qcow2.xz\u0026#34; wget \u0026#34;$download_url\u0026#34; -O \u0026#34;$image_file\u0026#34; xz -dk \u0026#34;$image_file\u0026#34; 3. Create bridge0 Network Interface To enable the VM to access your LAN via bridged networking, create a bridge0 interface using nmcli.\nBridge on WiFi is not supported. Use Ethernet for bridge Change IFACE variable accordingly # Set your physical interface (e.g., enp3s0) IFACE=\u0026#34;enp3s0\u0026#34; # See available devices nmcli device status # Create bridge0 sudo nmcli connection add type bridge ifname bridge0 con-name bridge0 # Set static IP, gateway, and DNS for the bridge sudo nmcli connection modify bridge0 \\ ipv4.method manual \\ ipv4.addresses 192.168.50.200/24 \\ ipv4.gateway 192.168.50.100 \\ ipv4.dns \u0026#34;192.168.50.100 9.9.9.9 192.168.50.1\u0026#34; \\ ipv6.method auto \\ bridge.stp no # Create and attach the physical interface as a bridge port sudo nmcli connection add type ethernet ifname \u0026#34;$IFACE\u0026#34; con-name bridge0-slave \\ master bridge0 # Bring up the connections sudo nmcli connection up bridge0 sudo nmcli connection up bridge0-slave 3.1 Allow bridge0 in QEMU sudo tee /etc/qemu/bridge.conf \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOL\u0026#39; allow bridge0 EOL 4. Grant Non-Root Libvirt Access These steps are required so you can manage VMs without needing sudo.\n4.1 Authorise your user to manage libvirt sudo tee /etc/polkit-1/rules.d/50-libvirt.rules \u0026gt; /dev/null \u0026lt;\u0026lt;EOL polkit.addRule(function(action, subject) { if (action.id == \u0026#34;org.libvirt.unix.manage\u0026#34; \u0026amp;\u0026amp; subject.user == \u0026#34;$USER\u0026#34;) { return polkit.Result.YES; } }); EOL 4.2 Add user to libvirt group sudo usermod -a -G libvirt $USER newgrp libvirt # Apply changes to current shell Verify:\nid -Gn 5. Create the HAOS VM VM_NAME=\u0026#34;haos\u0026#34; VM_MAC=\u0026#34;52:54:00:12:34:60\u0026#34; VM_DISK=\u0026#34;$HOME/haos/${image_file%.xz}\u0026#34; virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --description \u0026#34;Home Assistant OS\u0026#34; \\ --os-variant generic \\ --ram 3072 \\ --vcpus 1 \\ --disk path=\u0026#34;$VM_DISK\u0026#34;,bus=scsi \\ --controller type=scsi,model=virtio-scsi \\ --import \\ --graphics none \\ --boot uefi \\ --network bridge=bridge0,mac=\u0026#34;$VM_MAC\u0026#34; \\ --noautoconsole Enable autostart:\nvirsh autostart haos 6. Managing the VM (as non-root) virsh list virsh --connect qemu:///session list --all virsh --connect qemu:///system list --all Check MAC address:\nvirsh dumpxml haos | grep \u0026#34;mac address\u0026#34; | awk -F\\\u0026#39; \u0026#39;{ print $2 }\u0026#39; Delete the VM:\nvirsh destroy haos virsh undefine haos 7. Backup and Restore Fetch backups to your Mac:\nscp -r \u0026#34;$USER@192.168.50.100:/home/$USER/haos/nfs/*\u0026#34; \\ ~/Codes/homelab/home_assisstant/backups/ 8. Notes Action Needs Sudo? Install packages ‚úÖ Yes Setup bridge/qemu policies ‚úÖ Yes VM create/operate via libvirt ‚ùå No Use virt-manager GUI ‚ùå No After one-time configuration, everything runs user-only.\n9. Related Home Assistant OS Releases Libvirt Non-root Setup Bridge Networking Guide ","date":"4 August, 2025","id":20,"permalink":"/homelab/homeassistant-setup/","summary":"Home Assistant is an open-source platform for smart home automation. It integrates with a wide range of devices and services, and allows powerful automation logic ‚Äî all running locally on your own hardware.","tags":"homeassistant libvirt fedora","title":"HomeLab: Home Assistant VM - Non-root deployment on Fedora"},{"content":"1. Install Ansible # On RHEL sudo dnf install -y ansible-core # On macOS brew install ansible 2. Example Remote Host Field Value Username neo Hostname node2 IP 192.168.50.205 OS Fedora Password \u0026lt;you should know\u0026gt; 3. SSH Setup (Optional) On your laptop\n3.0 SSH setup for remote host # Check for SSH keys ls ~/.ssh # If you dont already have a ssh key pair ssh-keygen -t rsa -b 4096 # Copy your public key to host ssh-copy-id -o StrictHostKeyChecking=no neo@192.168.50.205 3.1 SSH Config tee ~/.ssh/config \u0026gt; /dev/null \u0026lt;\u0026lt;EOL Host node2 User neo EOL 3.2 Local DNS Resolution echo \u0026#34;192.168.50.205 node2\u0026#34; | sudo tee -a /etc/hosts 3.3 Test Login without IP and password\nssh node2 4. Create Your First Playbook tee ping.yaml \u0026gt; /dev/null \u0026lt;\u0026lt;EOL --- - name: Test Connection Playbook hosts: all gather_facts: true max_fail_percentage: 0 tasks: - name: Ping hosts ansible.builtin.ping: EOL 5. Run Playbook with IP # Run with login password prompt ansible-playbook -u neo --ask-pass -i 192.168.50.205, ping.yaml # Run with sudo password prompt as well ansible-playbook -u neo --ask-pass --ask-become-pass -i 192.168.50.205, ping.yaml Note the trailing comma , tells Ansible this is a literal host list.\n6. Create ansible.cfg sudo tee ansible.cfg \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [defaults] inventory = ~/Codes/inventory gathering = explicit private_key_file = ~/.ssh/id_rsa [ssh_connection] EOL 7. Create Inventory file sudo tee inventory \u0026gt; /dev/null \u0026lt;\u0026lt;EOL [nodes] node2 ansible_host=192.168.50.205 ansible_user=neo ansible_become_password=\u0026lt;NOT REAL PASSWORD\u0026gt; [localhost] mac ansible_host=127.0.0.1 ansible_user=arslankhan ansible_connection=local [nodes:vars] ansible_ssh_common_args = -o StrictHostKeyChecking=no -o ControlMaster=auto -o ControlPersist=60s [homelab] node[1:2] EOL Run playbooks # Run playbooks ansible-playbook ping.yaml -l node2 8. Common Commands # View inventory ansible-inventory --inventory inventory --list ansible-inventory --graph # List variables ansible-inventory --host node1 # Syntax check ansible-playbook ping.yaml --syntax-check # List target hosts ansible-playbook -l node1 ping.yaml --list-hosts 9. Using Ansible Vault 9.1 Create and Use Vault ansible-vault create secrets.yaml # Add secrets like: # ansible_ssh_pass: your_password # ansible_become_pass: your_sudo_password echo \u0026#34;your_password\u0026#34; \u0026gt; vault-password-file 9.2 Edit/View Vault ansible-vault edit secrets.yaml ansible-vault view secrets.yaml 10. Run Playbooks with Vault and Inventory # Basic ansible-playbook ping.yaml -l node2 # With vault + vars ansible-playbook ping.yaml \\ --vault-password-file vault-password-file \\ -e @secrets.yaml \\ -l node2 11. Run Locally on macOS # Without root ansible-playbook -l localhost ping.yaml --connection=local # With root ansible-playbook -l localhost ping.yaml --connection=local --ask-become-pass 12. Expect Module for Privileged Access Use when you can\u0026rsquo;t sudo and root login is disabled.\n# Whoami as root ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c whoami\u0026#39; responses=password=\u0026lt;YOUR PASSWORD\u0026gt; timeout=1\u0026#34; Make User Passwordless Sudo (using expect) # Create sudoers file ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;touch /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; # Add permission line ansible node2 \\ -u neo --ask-pass \\ -m expect \\ -a \u0026#34;command=\u0026#39;su root -c \\\u0026#39;echo \\\u0026#34;%neo ALL=(ALL) NOPASSWD: ALL\\\u0026#34; | sudo tee -a /etc/sudoers.d/neo\\\u0026#39;\u0026#39; responses=Password=\u0026lt;YOUR PASSWORD\u0026gt;\u0026#34; 13. Missing sshpass Error Fix (macOS) brew install https://raw.githubusercontent.com/kadwanev/bigboybrew/master/Library/Formula/sshpass.rb This is your personal Ansible quick reference ‚Äî opinionated, minimal, and proven in a homelab context.\n","date":"4 August, 2025","id":21,"permalink":"/posts/ansible/ansible-quickstart-2/","summary":"On your laptop","tags":"ansible","title":"Ansible: Quick Start - 2"},{"content":"Let\u0026rsquo;s Deploy Everything Example Remote Host Field Value Username neo Hostname node1 IP 192.168.50.200 OS Fedora Password \u0026lt;expected that you know\u0026gt; 1. Deploy K3s ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/deploy-k3s.yaml Click to see ansible playbook 'deploy-k3s.yaml' --- - name: Deploy K3s on Fedora VM hosts: all vars: k3s_install_script_url: \u0026#34;https://get.k3s.io\u0026#34; tasks: - name: Ensure firewalld is running ansible.builtin.service: name: firewalld state: started enabled: yes become: yes - name: Add K3s API server ansible.posix.firewalld: port: 6443/tcp permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Pod network ansible.posix.firewalld: zone: trusted source: 10.42.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Add K3s Service network ansible.posix.firewalld: zone: trusted source: 10.43.0.0/16 permanent: yes state: enabled become: yes notify: Reload firewalld - name: Create kubeconfig group ansible.builtin.group: name: kubeconfig state: present become: yes - name: Add user to kubeconfig group ansible.builtin.user: name: \u0026#34;{{ ansible_user }}\u0026#34; groups: kubeconfig append: yes become: yes - name: Install K3s with custom kubeconfig permissions ansible.builtin.shell: | curl -sfL {{ k3s_install_script_url }} | INSTALL_K3S_EXEC=\u0026#34;--write-kubeconfig-mode 640 --write-kubeconfig-group kubeconfig\u0026#34; sh - args: creates: /usr/local/bin/k3s become: true - name: Create .kube directory for the user ansible.builtin.file: path: \u0026#34;{{ ansible_user_dir }}/.kube\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; - name: Symlink K3s kubeconfig to ~/.kube/config ansible.builtin.file: src: /etc/rancher/k3s/k3s.yaml dest: \u0026#34;{{ ansible_user_dir }}/.kube/config\u0026#34; state: link owner: \u0026#34;{{ ansible_user }}\u0026#34; group: \u0026#34;{{ ansible_user }}\u0026#34; become: true handlers: - name: Reload firewalld ansible.builtin.service: name: firewalld state: reloaded become: yes Fetch kubeconfig # Fetch kubeconfig from K8s cluster scp -r neo@192.168.50.200:~/.kube/config ~/k3s-config # MacOS only: Update IP in kubeconfig sed -i \u0026#39;\u0026#39; \u0026#39;s/127.0.0.1/192.168.50.200/g\u0026#39; ~/k3s-config # Login to K8s export KUBECONFIG=~/k3s-config kubectl get all -A # verify access See my previous post on App of Apps\n2. Install ArgoCD helm repo add argo https://argoproj.github.io/argo-helm helm repo update kubectl create namespace argocd helm install argocd argo/argo-cd -n argocd -f argocd/values.yaml # insecure access = true for Ingress through Traefik \u0026amp; enable Helm through Kustomize # Ingress (for K3s) - Expose argocd at https://argocd.node1 kubectl apply -f argocd/ingress.yaml 3. Set Local DNS Edit /etc/hosts:\n192.168.50.200 k3s.node1 argocd.node1 test.node1 hello.node1 4. Give ArgoCD Access to Your Private Git Repo # Generate SSH key (no passphrase) ssh-keygen -t ed25519 -C \u0026#34;argocd@node1\u0026#34; -f argocd_git_key # Copy public key to GitHub deploy keys cat argocd_git_key.pub üëâ Add the key at\nhttps://github.com/arslankhanali/homelab-kubernetes/settings/keys/new\n# Login to ArgoCD argocd login argocd.node1 --insecure --username admin \\ --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) # Add private Git repo argocd repo add git@github.com:arslankhanali/homelab-kubernetes.git \\ --ssh-private-key-path argocd_git_key \\ --name homelab-kubernetes \\ --project default # Clean up keys rm argocd_git_key* 5. Access ArgoCD Dashboard To observe app deployment in real time:\nOpen https://argocd.node1 # Get initial admin password kubectl -n argocd get secret argocd-initial-admin-secret \\ -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 6. Unleash Everything # Trigger App of Apps pattern kubectl apply -f root-app.yaml 7. Access Apps Kubernetes Dashboard # Get bearer token kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d If you get 401 Unauthorized, ensure you\u0026rsquo;re using HTTPS.\nGuestbook Podinfo 7. Delete Everything # Delete all ArgoCD apps kubectl delete -f root-app.yaml for app in $(kubectl get applications -n argocd -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;); do kubectl patch application \u0026#34;$app\u0026#34; -n argocd -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete application \u0026#34;$app\u0026#34; -n argocd --force --grace-period=0 done # Clean up namespaces kubectl delete ns argocd kubectl delete ns kubernetes-dashboard kubectl delete ns podinfo # Delete K3s # ansible-playbook --ask-pass --ask-become-pass -u neo -i 192.168.50.200, ansible/remove-k3s.yaml 8. Deploy new app Add application to the apps/ folder. Test the application kustomize build . kustomize edit fix kustomize build \u0026lt;kustomization-dir\u0026gt; | kubectl apply -f - kubectl apply -k k8s/overlays/dev Create argocd-application in env/{k3s} folder See example: argoproj Application' apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed namespace: argocd # \u0026lt;-- NEVER CHANGE spec: project: default source: repoURL: git@github.com:arslankhanali/homelab-kubernetes.git path: apps/\u0026lt;app-name\u0026gt;/overlays/k3s # \u0026lt;-- CHANGE THIS as needed targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: \u0026lt;app-name\u0026gt; # \u0026lt;-- CHANGE THIS as needed syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # If namespace should be created Reference file you created in step3 in /env/k3s/kustomization.yaml Git push the repository Argo should sync automatically ","date":"7 August, 2025","id":22,"permalink":"/homelab/homelab-kubernetes/","summary":"See my previous post on App of Apps","tags":"kubernetes gitops","title":"Homelab: Kubernetes"},{"content":"About Welcome to my blog ‚Äî a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.\nHere, you\u0026rsquo;ll find:\nüîß Tech Walkthroughs ‚Äî Open source tooling, secure deployment patterns, and container-native workflows using Podman and Linux-based infrastructure. üåê Home Lab \u0026amp; Automation ‚Äî Hands-on experiments with Home Assistant, HomeKit, Fedora servers, and self-hosted services. üõ°Ô∏è Security \u0026amp; Best Practices ‚Äî Focus security, supply chain integrity, and observability. üì¶ Modern Ops ‚Äî GitOps, CI/CD with GitLab \u0026amp; ArgoCD, Helm templating, and cloud-native design thinking. Whether you‚Äôre an engineer, architect, or open source enthusiast ‚Äî I hope this blog helps you build smarter and more secure systems.\n","date":"2 August, 2025","id":23,"permalink":"/about/","summary":"Welcome to my blog ‚Äî a space where I share practical insights, deployment patterns, and lessons from real-world IT architecture and automation projects.","tags":"","title":"About"}]